nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/change_p
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=3000
max_epoch=200
r3f_lambda=1
extr='--warmup-init-lr 1e-07'
2021-01-08 13:42:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:42:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:42:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:42:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:42:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:42:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:42:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:42:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:42:39 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:16182
2021-01-08 13:42:39 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:16182
2021-01-08 13:42:39 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2021-01-08 13:42:39 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:16182
2021-01-08 13:42:39 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2021-01-08 13:42:39 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2021-01-08 13:42:42 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, cv=False, cv_lambda=0.0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:16182', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=False, nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/change_p', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=3000, weight_decay=0.0, zero_sharding='none')
2021-01-08 13:42:43 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2021-01-08 13:42:43 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2021-01-08 13:42:43 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2021-01-08 13:42:43 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2021-01-08 13:42:43 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2021-01-08 13:42:44 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2021-01-08 13:42:44 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-01-08 13:42:44 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2021-01-08 13:42:44 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2021-01-08 13:42:44 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2021-01-08 13:42:44 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-01-08 13:42:44 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-01-08 13:42:44 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-08 13:42:44 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-08 13:42:44 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-08 13:42:44 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-08 13:42:44 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-08 13:42:44 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2021-01-08 13:42:44 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2021-01-08 13:42:44 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2021-01-08 13:42:46 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2021-01-08 13:42:46 | INFO | fairseq.optim.adam | using FusedAdam
2021-01-08 13:42:46 | INFO | fairseq.trainer | loading train data for epoch 1
2021-01-08 13:42:46 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2021-01-08 13:42:46 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2021-01-08 13:42:46 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2021-01-08 13:42:46 | INFO | fairseq.trainer | begin training epoch 1
2021-01-08 13:42:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:42:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:42:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:42:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:42:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:42:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:43:49 | INFO | train_inner | epoch 001:    100 / 561 symm_kl=1.023, self_kl=0, self_cv=0, loss=4.132, nll_loss=0.84, ppl=1.79, wps=17844.4, ups=1.68, wpb=10623.3, bsz=364.6, num_updates=100, lr=1.43e-06, gnorm=2.561, train_wall=60, wall=66
2021-01-08 13:44:49 | INFO | train_inner | epoch 001:    200 / 561 symm_kl=0.95, self_kl=0, self_cv=0, loss=4.035, nll_loss=0.855, ppl=1.81, wps=17775.6, ups=1.68, wpb=10583.4, bsz=369.8, num_updates=200, lr=2.76e-06, gnorm=2.548, train_wall=59, wall=125
2021-01-08 13:45:50 | INFO | train_inner | epoch 001:    300 / 561 symm_kl=0.844, self_kl=0, self_cv=0, loss=3.898, nll_loss=0.883, ppl=1.84, wps=17008.1, ups=1.65, wpb=10335, bsz=373, num_updates=300, lr=4.09e-06, gnorm=2.302, train_wall=61, wall=186
2021-01-08 13:46:51 | INFO | train_inner | epoch 001:    400 / 561 symm_kl=0.773, self_kl=0, self_cv=0, loss=3.795, nll_loss=0.882, ppl=1.84, wps=17222.2, ups=1.63, wpb=10571.8, bsz=388.4, num_updates=400, lr=5.42e-06, gnorm=1.751, train_wall=61, wall=247
2021-01-08 13:47:52 | INFO | train_inner | epoch 001:    500 / 561 symm_kl=0.744, self_kl=0, self_cv=0, loss=3.764, nll_loss=0.892, ppl=1.86, wps=16935.4, ups=1.63, wpb=10411.2, bsz=371.8, num_updates=500, lr=6.75e-06, gnorm=1.65, train_wall=61, wall=309
2021-01-08 13:48:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 13:48:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:48:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:48:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:48:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:48:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:48:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:48:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:48:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:48:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:48:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:48:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:48:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:48:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:48:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:48:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:48:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:48:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:48:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:48:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:48:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:48:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:48:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:48:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:48:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:48:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:48:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:48:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:48:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:48:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:48:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:48:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:48:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:48:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:48:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:48:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:48:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:48:50 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.664 | nll_loss 4.096 | ppl 17.1 | bleu 22.4 | wps 4947 | wpb 7508.5 | bsz 272.7 | num_updates 561
2021-01-08 13:48:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 13:48:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:48:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:48:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 1 @ 561 updates, score 22.4) (writing took 2.1356032006442547 seconds)
2021-01-08 13:48:52 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-01-08 13:48:52 | INFO | train | epoch 001 | symm_kl 0.855 | self_kl 0 | self_cv 0 | loss 3.911 | nll_loss 0.875 | ppl 1.83 | wps 16240.4 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 561 | lr 7.5613e-06 | gnorm 2.106 | train_wall 340 | wall 368
2021-01-08 13:48:52 | INFO | fairseq.trainer | begin training epoch 2
2021-01-08 13:48:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:48:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:48:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:48:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:49:18 | INFO | train_inner | epoch 002:     39 / 561 symm_kl=0.733, self_kl=0, self_cv=0, loss=3.754, nll_loss=0.901, ppl=1.87, wps=12043.1, ups=1.16, wpb=10345.6, bsz=358.4, num_updates=600, lr=8.08e-06, gnorm=1.616, train_wall=61, wall=395
2021-01-08 13:50:20 | INFO | train_inner | epoch 002:    139 / 561 symm_kl=0.71, self_kl=0, self_cv=0, loss=3.729, nll_loss=0.914, ppl=1.88, wps=17162.2, ups=1.63, wpb=10532.9, bsz=366.4, num_updates=700, lr=9.41e-06, gnorm=1.53, train_wall=61, wall=456
2021-01-08 13:51:21 | INFO | train_inner | epoch 002:    239 / 561 symm_kl=0.681, self_kl=0, self_cv=0, loss=3.683, nll_loss=0.913, ppl=1.88, wps=17053.5, ups=1.62, wpb=10499.4, bsz=369.4, num_updates=800, lr=1.074e-05, gnorm=1.47, train_wall=61, wall=518
2021-01-08 13:52:23 | INFO | train_inner | epoch 002:    339 / 561 symm_kl=0.667, self_kl=0, self_cv=0, loss=3.671, nll_loss=0.923, ppl=1.9, wps=17126.2, ups=1.62, wpb=10541.2, bsz=377.1, num_updates=900, lr=1.207e-05, gnorm=1.431, train_wall=61, wall=579
2021-01-08 13:53:25 | INFO | train_inner | epoch 002:    439 / 561 symm_kl=0.662, self_kl=0, self_cv=0, loss=3.676, nll_loss=0.939, ppl=1.92, wps=16961.3, ups=1.62, wpb=10472.1, bsz=362.4, num_updates=1000, lr=1.34e-05, gnorm=1.411, train_wall=62, wall=641
2021-01-08 13:54:26 | INFO | train_inner | epoch 002:    539 / 561 symm_kl=0.65, self_kl=0, self_cv=0, loss=3.667, nll_loss=0.949, ppl=1.93, wps=17099.7, ups=1.64, wpb=10444.3, bsz=375.2, num_updates=1100, lr=1.473e-05, gnorm=1.388, train_wall=61, wall=702
2021-01-08 13:54:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 13:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:54:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:54:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:54:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:54:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:54:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:54:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:54:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:54:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:54:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:54:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:54:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:54:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:54:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:54:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:54:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:54:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 13:54:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 13:54:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 13:55:00 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.536 | nll_loss 3.991 | ppl 15.9 | bleu 22.41 | wps 4725.2 | wpb 7508.5 | bsz 272.7 | num_updates 1122 | best_bleu 22.41
2021-01-08 13:55:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 13:55:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:55:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:55:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:55:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:55:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 2 @ 1122 updates, score 22.41) (writing took 4.731146886944771 seconds)
2021-01-08 13:55:04 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-01-08 13:55:04 | INFO | train | epoch 002 | symm_kl 0.675 | self_kl 0 | self_cv 0 | loss 3.686 | nll_loss 0.926 | ppl 1.9 | wps 15784.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 1122 | lr 1.50226e-05 | gnorm 1.455 | train_wall 343 | wall 741
2021-01-08 13:55:04 | INFO | fairseq.trainer | begin training epoch 3
2021-01-08 13:55:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 13:55:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 13:55:54 | INFO | train_inner | epoch 003:     78 / 561 symm_kl=0.632, self_kl=0, self_cv=0, loss=3.631, nll_loss=0.94, ppl=1.92, wps=11760.3, ups=1.13, wpb=10441.4, bsz=356.7, num_updates=1200, lr=1.606e-05, gnorm=1.366, train_wall=60, wall=791
2021-01-08 13:56:56 | INFO | train_inner | epoch 003:    178 / 561 symm_kl=0.625, self_kl=0, self_cv=0, loss=3.629, nll_loss=0.949, ppl=1.93, wps=16861.5, ups=1.62, wpb=10420.6, bsz=376, num_updates=1300, lr=1.739e-05, gnorm=1.337, train_wall=62, wall=853
2021-01-08 13:57:58 | INFO | train_inner | epoch 003:    278 / 561 symm_kl=0.611, self_kl=0, self_cv=0, loss=3.606, nll_loss=0.945, ppl=1.92, wps=17074.1, ups=1.63, wpb=10478.6, bsz=371.6, num_updates=1400, lr=1.872e-05, gnorm=1.313, train_wall=61, wall=914
2021-01-08 13:58:59 | INFO | train_inner | epoch 003:    378 / 561 symm_kl=0.612, self_kl=0, self_cv=0, loss=3.626, nll_loss=0.969, ppl=1.96, wps=17036.8, ups=1.63, wpb=10472.3, bsz=374.7, num_updates=1500, lr=2.005e-05, gnorm=1.315, train_wall=61, wall=975
2021-01-08 14:00:01 | INFO | train_inner | epoch 003:    478 / 561 symm_kl=0.592, self_kl=0, self_cv=0, loss=3.586, nll_loss=0.955, ppl=1.94, wps=17351.9, ups=1.63, wpb=10650.7, bsz=373.4, num_updates=1600, lr=2.138e-05, gnorm=1.27, train_wall=61, wall=1037
2021-01-08 14:00:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 14:00:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:00:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:00:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:00:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:00:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:00:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:00:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:00:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:00:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:00:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:00:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:00:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:00:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:00:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:00:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:00:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:00:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:00:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:00:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:00:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:00:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:00:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:00:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:00:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:01:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:01:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:01:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:01:12 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.479 | nll_loss 3.936 | ppl 15.31 | bleu 22.6 | wps 4594.3 | wpb 7508.5 | bsz 272.7 | num_updates 1683 | best_bleu 22.6
2021-01-08 14:01:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 14:01:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:01:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:01:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:01:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:01:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 3 @ 1683 updates, score 22.6) (writing took 4.679040282964706 seconds)
2021-01-08 14:01:17 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-01-08 14:01:17 | INFO | train | epoch 003 | symm_kl 0.609 | self_kl 0 | self_cv 0 | loss 3.609 | nll_loss 0.952 | ppl 1.93 | wps 15799.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 1683 | lr 2.24839e-05 | gnorm 1.312 | train_wall 342 | wall 1113
2021-01-08 14:01:17 | INFO | fairseq.trainer | begin training epoch 4
2021-01-08 14:01:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:01:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:01:30 | INFO | train_inner | epoch 004:     17 / 561 symm_kl=0.589, self_kl=0, self_cv=0, loss=3.577, nll_loss=0.952, ppl=1.93, wps=11676, ups=1.12, wpb=10447.8, bsz=352, num_updates=1700, lr=2.271e-05, gnorm=1.291, train_wall=61, wall=1126
2021-01-08 14:02:31 | INFO | train_inner | epoch 004:    117 / 561 symm_kl=0.586, self_kl=0, self_cv=0, loss=3.583, nll_loss=0.961, ppl=1.95, wps=17265.7, ups=1.65, wpb=10469.1, bsz=365.6, num_updates=1800, lr=2.404e-05, gnorm=1.261, train_wall=60, wall=1187
2021-01-08 14:03:33 | INFO | train_inner | epoch 004:    217 / 561 symm_kl=0.582, self_kl=0, self_cv=0, loss=3.585, nll_loss=0.969, ppl=1.96, wps=16603.1, ups=1.62, wpb=10271.1, bsz=367.4, num_updates=1900, lr=2.537e-05, gnorm=1.268, train_wall=62, wall=1249
2021-01-08 14:04:34 | INFO | train_inner | epoch 004:    317 / 561 symm_kl=0.569, self_kl=0, self_cv=0, loss=3.555, nll_loss=0.957, ppl=1.94, wps=17181.7, ups=1.63, wpb=10571.4, bsz=356.9, num_updates=2000, lr=2.67e-05, gnorm=1.238, train_wall=61, wall=1310
2021-01-08 14:05:36 | INFO | train_inner | epoch 004:    417 / 561 symm_kl=0.562, self_kl=0, self_cv=0, loss=3.551, nll_loss=0.966, ppl=1.95, wps=17112.9, ups=1.62, wpb=10532.7, bsz=370.6, num_updates=2100, lr=2.803e-05, gnorm=1.213, train_wall=61, wall=1372
2021-01-08 14:06:37 | INFO | train_inner | epoch 004:    517 / 561 symm_kl=0.553, self_kl=0, self_cv=0, loss=3.54, nll_loss=0.969, ppl=1.96, wps=17272.6, ups=1.63, wpb=10614.4, bsz=387.6, num_updates=2200, lr=2.936e-05, gnorm=1.201, train_wall=61, wall=1433
2021-01-08 14:07:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 14:07:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:07:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:07:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:07:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:07:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:07:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:07:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:07:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:07:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:07:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:07:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:07:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:07:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:07:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:07:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:07:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:07:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:07:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:07:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:07:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:07:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:07:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:07:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:07:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:07:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:07:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:07:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:07:25 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.441 | nll_loss 3.903 | ppl 14.96 | bleu 22.69 | wps 4585.6 | wpb 7508.5 | bsz 272.7 | num_updates 2244 | best_bleu 22.69
2021-01-08 14:07:25 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 14:07:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:07:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:07:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:07:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:07:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 4 @ 2244 updates, score 22.69) (writing took 4.634979151189327 seconds)
2021-01-08 14:07:30 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-01-08 14:07:30 | INFO | train | epoch 004 | symm_kl 0.568 | self_kl 0 | self_cv 0 | loss 3.558 | nll_loss 0.964 | ppl 1.95 | wps 15774.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 2244 | lr 2.99452e-05 | gnorm 1.233 | train_wall 343 | wall 1486
2021-01-08 14:07:30 | INFO | fairseq.trainer | begin training epoch 5
2021-01-08 14:07:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:07:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:08:06 | INFO | train_inner | epoch 005:     56 / 561 symm_kl=0.542, self_kl=0, self_cv=0, loss=3.515, nll_loss=0.958, ppl=1.94, wps=11709.6, ups=1.12, wpb=10433.5, bsz=373.3, num_updates=2300, lr=3.069e-05, gnorm=1.182, train_wall=60, wall=1522
2021-01-08 14:09:08 | INFO | train_inner | epoch 005:    156 / 561 symm_kl=0.545, self_kl=0, self_cv=0, loss=3.53, nll_loss=0.97, ppl=1.96, wps=16976.6, ups=1.62, wpb=10447.6, bsz=372.4, num_updates=2400, lr=3.202e-05, gnorm=1.195, train_wall=61, wall=1584
2021-01-08 14:10:09 | INFO | train_inner | epoch 005:    256 / 561 symm_kl=0.542, self_kl=0, self_cv=0, loss=3.528, nll_loss=0.974, ppl=1.96, wps=17107.6, ups=1.63, wpb=10524.3, bsz=363.8, num_updates=2500, lr=3.335e-05, gnorm=1.176, train_wall=61, wall=1645
2021-01-08 14:11:11 | INFO | train_inner | epoch 005:    356 / 561 symm_kl=0.545, self_kl=0, self_cv=0, loss=3.545, nll_loss=0.987, ppl=1.98, wps=16909.1, ups=1.62, wpb=10415.7, bsz=357.5, num_updates=2600, lr=3.468e-05, gnorm=1.185, train_wall=61, wall=1707
2021-01-08 14:12:12 | INFO | train_inner | epoch 005:    456 / 561 symm_kl=0.526, self_kl=0, self_cv=0, loss=3.501, nll_loss=0.97, ppl=1.96, wps=17168.3, ups=1.63, wpb=10565, bsz=383, num_updates=2700, lr=3.601e-05, gnorm=1.144, train_wall=61, wall=1769
2021-01-08 14:13:14 | INFO | train_inner | epoch 005:    556 / 561 symm_kl=0.525, self_kl=0, self_cv=0, loss=3.502, nll_loss=0.974, ppl=1.96, wps=17044, ups=1.63, wpb=10479.6, bsz=374.7, num_updates=2800, lr=3.734e-05, gnorm=1.159, train_wall=61, wall=1830
2021-01-08 14:13:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 14:13:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:13:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:13:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:13:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:13:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:13:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:13:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:13:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:13:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:13:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:13:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:13:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:13:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:13:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:13:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:13:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:13:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:13:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:13:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:13:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:13:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:13:38 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.408 | nll_loss 3.872 | ppl 14.64 | bleu 22.81 | wps 4585.2 | wpb 7508.5 | bsz 272.7 | num_updates 2805 | best_bleu 22.81
2021-01-08 14:13:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 14:13:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:13:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:13:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:13:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:13:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 5 @ 2805 updates, score 22.81) (writing took 4.684371924027801 seconds)
2021-01-08 14:13:42 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-01-08 14:13:42 | INFO | train | epoch 005 | symm_kl 0.537 | self_kl 0 | self_cv 0 | loss 3.521 | nll_loss 0.973 | ppl 1.96 | wps 15776.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 2805 | lr 3.74065e-05 | gnorm 1.174 | train_wall 343 | wall 1859
2021-01-08 14:13:42 | INFO | fairseq.trainer | begin training epoch 6
2021-01-08 14:13:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:13:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:14:43 | INFO | train_inner | epoch 006:     95 / 561 symm_kl=0.52, self_kl=0, self_cv=0, loss=3.493, nll_loss=0.97, ppl=1.96, wps=11563.5, ups=1.12, wpb=10318.1, bsz=377.4, num_updates=2900, lr=3.867e-05, gnorm=1.159, train_wall=60, wall=1919
2021-01-08 14:15:45 | INFO | train_inner | epoch 006:    195 / 561 symm_kl=0.517, self_kl=0, self_cv=0, loss=3.492, nll_loss=0.974, ppl=1.96, wps=17243.9, ups=1.61, wpb=10679.3, bsz=372.1, num_updates=3000, lr=4e-05, gnorm=1.132, train_wall=62, wall=1981
2021-01-08 14:16:46 | INFO | train_inner | epoch 006:    295 / 561 symm_kl=0.513, self_kl=0, self_cv=0, loss=3.487, nll_loss=0.974, ppl=1.96, wps=17048.5, ups=1.63, wpb=10477.8, bsz=365.4, num_updates=3100, lr=3.93496e-05, gnorm=1.141, train_wall=61, wall=2043
2021-01-08 14:17:48 | INFO | train_inner | epoch 006:    395 / 561 symm_kl=0.517, self_kl=0, self_cv=0, loss=3.512, nll_loss=0.996, ppl=1.99, wps=17139.8, ups=1.63, wpb=10517.4, bsz=358, num_updates=3200, lr=3.87298e-05, gnorm=1.131, train_wall=61, wall=2104
2021-01-08 14:18:49 | INFO | train_inner | epoch 006:    495 / 561 symm_kl=0.499, self_kl=0, self_cv=0, loss=3.46, nll_loss=0.969, ppl=1.96, wps=17125.1, ups=1.63, wpb=10534.9, bsz=372, num_updates=3300, lr=3.81385e-05, gnorm=1.117, train_wall=61, wall=2166
2021-01-08 14:19:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 14:19:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:19:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:19:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:19:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:19:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:19:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:19:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:19:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:19:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:19:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:19:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:19:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:19:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:19:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:19:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:19:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:19:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:19:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:19:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:19:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:19:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:19:50 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.383 | nll_loss 3.846 | ppl 14.38 | bleu 22.77 | wps 4877 | wpb 7508.5 | bsz 272.7 | num_updates 3366 | best_bleu 22.81
2021-01-08 14:19:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 14:19:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:19:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:19:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:19:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:19:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 6 @ 3366 updates, score 22.77) (writing took 2.8314708340913057 seconds)
2021-01-08 14:19:53 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-01-08 14:19:53 | INFO | train | epoch 006 | symm_kl 0.513 | self_kl 0 | self_cv 0 | loss 3.491 | nll_loss 0.98 | ppl 1.97 | wps 15883.8 | ups 1.52 | wpb 10483.4 | bsz 369.6 | num_updates 3366 | lr 3.77627e-05 | gnorm 1.134 | train_wall 343 | wall 2229
2021-01-08 14:19:53 | INFO | fairseq.trainer | begin training epoch 7
2021-01-08 14:19:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:19:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:20:16 | INFO | train_inner | epoch 007:     34 / 561 symm_kl=0.506, self_kl=0, self_cv=0, loss=3.499, nll_loss=1.002, ppl=2, wps=11814, ups=1.15, wpb=10237, bsz=369, num_updates=3400, lr=3.75735e-05, gnorm=1.122, train_wall=60, wall=2252
2021-01-08 14:21:17 | INFO | train_inner | epoch 007:    134 / 561 symm_kl=0.5, self_kl=0, self_cv=0, loss=3.469, nll_loss=0.977, ppl=1.97, wps=17124.8, ups=1.63, wpb=10508.4, bsz=371.6, num_updates=3500, lr=3.70328e-05, gnorm=1.107, train_wall=61, wall=2314
2021-01-08 14:22:18 | INFO | train_inner | epoch 007:    234 / 561 symm_kl=0.497, self_kl=0, self_cv=0, loss=3.47, nll_loss=0.983, ppl=1.98, wps=17027.2, ups=1.64, wpb=10404.4, bsz=363.4, num_updates=3600, lr=3.65148e-05, gnorm=1.103, train_wall=61, wall=2375
2021-01-08 14:23:19 | INFO | train_inner | epoch 007:    334 / 561 symm_kl=0.495, self_kl=0, self_cv=0, loss=3.472, nll_loss=0.988, ppl=1.98, wps=17179.5, ups=1.64, wpb=10456.6, bsz=375.4, num_updates=3700, lr=3.6018e-05, gnorm=1.097, train_wall=61, wall=2436
2021-01-08 14:24:21 | INFO | train_inner | epoch 007:    434 / 561 symm_kl=0.494, self_kl=0, self_cv=0, loss=3.463, nll_loss=0.979, ppl=1.97, wps=17100.8, ups=1.63, wpb=10467.8, bsz=366.5, num_updates=3800, lr=3.55409e-05, gnorm=1.114, train_wall=61, wall=2497
2021-01-08 14:25:22 | INFO | train_inner | epoch 007:    534 / 561 symm_kl=0.485, self_kl=0, self_cv=0, loss=3.453, nll_loss=0.984, ppl=1.98, wps=17416.8, ups=1.63, wpb=10688.9, bsz=373.3, num_updates=3900, lr=3.50823e-05, gnorm=1.066, train_wall=61, wall=2558
2021-01-08 14:25:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 14:25:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:25:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:25:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:25:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:25:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:25:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:25:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:25:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:25:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:25:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:25:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:25:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:25:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:25:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:25:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:25:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:25:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:25:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:25:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:25:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:25:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:25:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:25:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:25:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:25:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:25:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:25:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:25:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:25:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:25:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:25:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:25:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:25:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:25:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:25:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:25:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:25:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:25:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:25:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:26:01 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.368 | nll_loss 3.834 | ppl 14.26 | bleu 22.8 | wps 4171.5 | wpb 7508.5 | bsz 272.7 | num_updates 3927 | best_bleu 22.81
2021-01-08 14:26:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 14:26:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:26:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:26:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:26:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:26:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 7 @ 3927 updates, score 22.8) (writing took 2.8644617963582277 seconds)
2021-01-08 14:26:04 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-01-08 14:26:04 | INFO | train | epoch 007 | symm_kl 0.493 | self_kl 0 | self_cv 0 | loss 3.464 | nll_loss 0.982 | ppl 1.98 | wps 15842.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 3927 | lr 3.49615e-05 | gnorm 1.096 | train_wall 341 | wall 2600
2021-01-08 14:26:04 | INFO | fairseq.trainer | begin training epoch 8
2021-01-08 14:26:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:26:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:26:51 | INFO | train_inner | epoch 008:     73 / 561 symm_kl=0.478, self_kl=0, self_cv=0, loss=3.435, nll_loss=0.974, ppl=1.96, wps=11913.2, ups=1.12, wpb=10590, bsz=375, num_updates=4000, lr=3.4641e-05, gnorm=1.068, train_wall=60, wall=2647
2021-01-08 14:27:52 | INFO | train_inner | epoch 008:    173 / 561 symm_kl=0.488, self_kl=0, self_cv=0, loss=3.452, nll_loss=0.976, ppl=1.97, wps=17143.3, ups=1.63, wpb=10504.7, bsz=358.9, num_updates=4100, lr=3.4216e-05, gnorm=1.087, train_wall=61, wall=2708
2021-01-08 14:28:53 | INFO | train_inner | epoch 008:    273 / 561 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.441, nll_loss=0.979, ppl=1.97, wps=17021.7, ups=1.64, wpb=10367.5, bsz=366.4, num_updates=4200, lr=3.38062e-05, gnorm=1.073, train_wall=61, wall=2769
2021-01-08 14:29:54 | INFO | train_inner | epoch 008:    373 / 561 symm_kl=0.475, self_kl=0, self_cv=0, loss=3.431, nll_loss=0.974, ppl=1.96, wps=16945.5, ups=1.63, wpb=10416.1, bsz=389.5, num_updates=4300, lr=3.34108e-05, gnorm=1.068, train_wall=61, wall=2831
2021-01-08 14:30:56 | INFO | train_inner | epoch 008:    473 / 561 symm_kl=0.469, self_kl=0, self_cv=0, loss=3.425, nll_loss=0.979, ppl=1.97, wps=17340.1, ups=1.63, wpb=10648.7, bsz=379.6, num_updates=4400, lr=3.30289e-05, gnorm=1.045, train_wall=61, wall=2892
2021-01-08 14:31:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 14:31:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:31:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:31:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:31:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:31:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:31:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:31:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:31:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:31:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:31:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:31:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:31:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:31:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:31:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:31:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:31:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:31:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:31:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:31:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:31:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:31:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:31:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:31:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:31:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:31:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:31:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:31:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:31:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:31:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:31:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:31:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:31:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:31:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:31:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:31:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:31:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:31:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:31:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:31:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:32:11 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.359 | nll_loss 3.825 | ppl 14.17 | bleu 22.74 | wps 4630 | wpb 7508.5 | bsz 272.7 | num_updates 4488 | best_bleu 22.81
2021-01-08 14:32:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 14:32:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:32:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:32:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:32:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:32:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 8 @ 4488 updates, score 22.74) (writing took 2.8213074412196875 seconds)
2021-01-08 14:32:13 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-01-08 14:32:13 | INFO | train | epoch 008 | symm_kl 0.479 | self_kl 0 | self_cv 0 | loss 3.442 | nll_loss 0.981 | ppl 1.97 | wps 15912.4 | ups 1.52 | wpb 10483.4 | bsz 369.6 | num_updates 4488 | lr 3.27035e-05 | gnorm 1.072 | train_wall 342 | wall 2970
2021-01-08 14:32:13 | INFO | fairseq.trainer | begin training epoch 9
2021-01-08 14:32:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:32:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:32:24 | INFO | train_inner | epoch 009:     12 / 561 symm_kl=0.48, self_kl=0, self_cv=0, loss=3.459, nll_loss=0.999, ppl=2, wps=11737.9, ups=1.14, wpb=10320.4, bsz=352.8, num_updates=4500, lr=3.26599e-05, gnorm=1.089, train_wall=61, wall=2980
2021-01-08 14:33:25 | INFO | train_inner | epoch 009:    112 / 561 symm_kl=0.469, self_kl=0, self_cv=0, loss=3.415, nll_loss=0.964, ppl=1.95, wps=17262.2, ups=1.64, wpb=10532.6, bsz=374.2, num_updates=4600, lr=3.23029e-05, gnorm=1.048, train_wall=61, wall=3041
2021-01-08 14:34:26 | INFO | train_inner | epoch 009:    212 / 561 symm_kl=0.479, self_kl=0, self_cv=0, loss=3.451, nll_loss=0.99, ppl=1.99, wps=17121.7, ups=1.63, wpb=10528, bsz=345, num_updates=4700, lr=3.19574e-05, gnorm=1.059, train_wall=61, wall=3103
2021-01-08 14:35:28 | INFO | train_inner | epoch 009:    312 / 561 symm_kl=0.462, self_kl=0, self_cv=0, loss=3.413, nll_loss=0.975, ppl=1.97, wps=16998.6, ups=1.62, wpb=10473.1, bsz=377.1, num_updates=4800, lr=3.16228e-05, gnorm=1.032, train_wall=61, wall=3164
2021-01-08 14:36:29 | INFO | train_inner | epoch 009:    412 / 561 symm_kl=0.465, self_kl=0, self_cv=0, loss=3.419, nll_loss=0.977, ppl=1.97, wps=17076.6, ups=1.63, wpb=10483.1, bsz=368.3, num_updates=4900, lr=3.12984e-05, gnorm=1.051, train_wall=61, wall=3226
2021-01-08 14:37:31 | INFO | train_inner | epoch 009:    512 / 561 symm_kl=0.46, self_kl=0, self_cv=0, loss=3.412, nll_loss=0.978, ppl=1.97, wps=17058, ups=1.62, wpb=10514.7, bsz=388.6, num_updates=5000, lr=3.09839e-05, gnorm=1.034, train_wall=61, wall=3287
2021-01-08 14:38:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 14:38:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:38:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:38:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:38:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:38:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:38:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:38:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:38:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:38:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:38:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:38:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:38:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:38:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:38:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:38:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:38:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:38:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:38:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:38:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:38:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:38:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:38:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:38:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:38:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:38:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:38:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:38:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:38:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:38:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:38:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:38:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:38:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:38:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:38:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:38:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:38:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:38:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:38:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:38:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:38:22 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.347 | nll_loss 3.814 | ppl 14.06 | bleu 22.81 | wps 4679 | wpb 7508.5 | bsz 272.7 | num_updates 5049 | best_bleu 22.81
2021-01-08 14:38:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 14:38:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:38:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:38:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:38:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:38:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 9 @ 5049 updates, score 22.81) (writing took 4.622058000415564 seconds)
2021-01-08 14:38:26 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-01-08 14:38:26 | INFO | train | epoch 009 | symm_kl 0.467 | self_kl 0 | self_cv 0 | loss 3.425 | nll_loss 0.979 | ppl 1.97 | wps 15773.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 5049 | lr 3.08332e-05 | gnorm 1.047 | train_wall 343 | wall 3342
2021-01-08 14:38:26 | INFO | fairseq.trainer | begin training epoch 10
2021-01-08 14:38:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:38:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:39:00 | INFO | train_inner | epoch 010:     51 / 561 symm_kl=0.468, self_kl=0, self_cv=0, loss=3.437, nll_loss=0.992, ppl=1.99, wps=11620.9, ups=1.12, wpb=10353, bsz=356, num_updates=5100, lr=3.06786e-05, gnorm=1.051, train_wall=61, wall=3376
2021-01-08 14:40:02 | INFO | train_inner | epoch 010:    151 / 561 symm_kl=0.46, self_kl=0, self_cv=0, loss=3.415, nll_loss=0.979, ppl=1.97, wps=17175.5, ups=1.62, wpb=10577.3, bsz=365.6, num_updates=5200, lr=3.03822e-05, gnorm=1.023, train_wall=61, wall=3438
2021-01-08 14:41:03 | INFO | train_inner | epoch 010:    251 / 561 symm_kl=0.464, self_kl=0, self_cv=0, loss=3.419, nll_loss=0.978, ppl=1.97, wps=17025.6, ups=1.62, wpb=10501.8, bsz=363.6, num_updates=5300, lr=3.00942e-05, gnorm=1.041, train_wall=61, wall=3500
2021-01-08 14:42:05 | INFO | train_inner | epoch 010:    351 / 561 symm_kl=0.459, self_kl=0, self_cv=0, loss=3.415, nll_loss=0.982, ppl=1.98, wps=16927.7, ups=1.62, wpb=10450.1, bsz=375.6, num_updates=5400, lr=2.98142e-05, gnorm=1.031, train_wall=62, wall=3561
2021-01-08 14:43:07 | INFO | train_inner | epoch 010:    451 / 561 symm_kl=0.45, self_kl=0, self_cv=0, loss=3.391, nll_loss=0.972, ppl=1.96, wps=16958.9, ups=1.62, wpb=10472.1, bsz=373.6, num_updates=5500, lr=2.9542e-05, gnorm=1.022, train_wall=62, wall=3623
2021-01-08 14:44:08 | INFO | train_inner | epoch 010:    551 / 561 symm_kl=0.454, self_kl=0, self_cv=0, loss=3.404, nll_loss=0.979, ppl=1.97, wps=17143.4, ups=1.63, wpb=10516.7, bsz=381.2, num_updates=5600, lr=2.9277e-05, gnorm=1.031, train_wall=61, wall=3684
2021-01-08 14:44:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 14:44:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:44:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:44:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:44:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:44:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:44:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:44:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:44:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:44:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:44:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:44:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:44:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:44:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:44:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:44:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:44:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:44:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:44:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:44:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:44:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:44:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:44:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:44:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:44:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:44:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:44:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:44:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:44:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:44:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:44:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:44:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:44:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:44:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:44:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:44:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:44:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:44:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:44:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:44:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:44:35 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.338 | nll_loss 3.811 | ppl 14.04 | bleu 22.61 | wps 4614.7 | wpb 7508.5 | bsz 272.7 | num_updates 5610 | best_bleu 22.81
2021-01-08 14:44:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 14:44:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:44:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:44:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:44:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:44:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 10 @ 5610 updates, score 22.61) (writing took 2.8679433166980743 seconds)
2021-01-08 14:44:38 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-01-08 14:44:38 | INFO | train | epoch 010 | symm_kl 0.458 | self_kl 0 | self_cv 0 | loss 3.41 | nll_loss 0.978 | ppl 1.97 | wps 15830.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 5610 | lr 2.92509e-05 | gnorm 1.032 | train_wall 344 | wall 3714
2021-01-08 14:44:38 | INFO | fairseq.trainer | begin training epoch 11
2021-01-08 14:44:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:44:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:45:35 | INFO | train_inner | epoch 011:     90 / 561 symm_kl=0.457, self_kl=0, self_cv=0, loss=3.407, nll_loss=0.976, ppl=1.97, wps=11937.6, ups=1.15, wpb=10358.6, bsz=351.1, num_updates=5700, lr=2.90191e-05, gnorm=1.038, train_wall=60, wall=3771
2021-01-08 14:46:37 | INFO | train_inner | epoch 011:    190 / 561 symm_kl=0.445, self_kl=0, self_cv=0, loss=3.378, nll_loss=0.963, ppl=1.95, wps=17059.1, ups=1.61, wpb=10564, bsz=383.6, num_updates=5800, lr=2.87678e-05, gnorm=1.003, train_wall=62, wall=3833
2021-01-08 14:47:38 | INFO | train_inner | epoch 011:    290 / 561 symm_kl=0.456, self_kl=0, self_cv=0, loss=3.416, nll_loss=0.989, ppl=1.98, wps=16891.6, ups=1.62, wpb=10400.1, bsz=355.4, num_updates=5900, lr=2.8523e-05, gnorm=1.029, train_wall=61, wall=3895
2021-01-08 14:48:40 | INFO | train_inner | epoch 011:    390 / 561 symm_kl=0.455, self_kl=0, self_cv=0, loss=3.418, nll_loss=0.993, ppl=1.99, wps=16815.4, ups=1.62, wpb=10394.2, bsz=372.6, num_updates=6000, lr=2.82843e-05, gnorm=1.024, train_wall=62, wall=3956
2021-01-08 14:49:42 | INFO | train_inner | epoch 011:    490 / 561 symm_kl=0.443, self_kl=0, self_cv=0, loss=3.378, nll_loss=0.966, ppl=1.95, wps=17289, ups=1.62, wpb=10652.7, bsz=380.5, num_updates=6100, lr=2.80515e-05, gnorm=1.007, train_wall=61, wall=4018
2021-01-08 14:50:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 14:50:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:50:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:50:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:50:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:50:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:50:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:50:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:50:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:50:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:50:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:50:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:50:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:50:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:50:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:50:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:50:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:50:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:50:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:50:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:50:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:50:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:50:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:50:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:50:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:50:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:50:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:50:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:50:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:50:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:50:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:50:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:50:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:50:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:50:46 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.325 | nll_loss 3.8 | ppl 13.93 | bleu 22.59 | wps 4619.9 | wpb 7508.5 | bsz 272.7 | num_updates 6171 | best_bleu 22.81
2021-01-08 14:50:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 14:50:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:50:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:50:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:50:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:50:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 11 @ 6171 updates, score 22.59) (writing took 2.9407966397702694 seconds)
2021-01-08 14:50:49 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-01-08 14:50:49 | INFO | train | epoch 011 | symm_kl 0.45 | self_kl 0 | self_cv 0 | loss 3.398 | nll_loss 0.977 | ppl 1.97 | wps 15824.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 6171 | lr 2.78896e-05 | gnorm 1.018 | train_wall 344 | wall 4086
2021-01-08 14:50:49 | INFO | fairseq.trainer | begin training epoch 12
2021-01-08 14:50:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:50:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:51:10 | INFO | train_inner | epoch 012:     29 / 561 symm_kl=0.446, self_kl=0, self_cv=0, loss=3.388, nll_loss=0.973, ppl=1.96, wps=11911.8, ups=1.14, wpb=10473.3, bsz=364.9, num_updates=6200, lr=2.78243e-05, gnorm=1.01, train_wall=61, wall=4106
2021-01-08 14:52:11 | INFO | train_inner | epoch 012:    129 / 561 symm_kl=0.444, self_kl=0, self_cv=0, loss=3.383, nll_loss=0.972, ppl=1.96, wps=17027, ups=1.64, wpb=10400.1, bsz=369, num_updates=6300, lr=2.76026e-05, gnorm=1.01, train_wall=61, wall=4167
2021-01-08 14:53:12 | INFO | train_inner | epoch 012:    229 / 561 symm_kl=0.449, self_kl=0, self_cv=0, loss=3.402, nll_loss=0.984, ppl=1.98, wps=17004, ups=1.63, wpb=10444.8, bsz=372, num_updates=6400, lr=2.73861e-05, gnorm=1.013, train_wall=61, wall=4228
2021-01-08 14:54:14 | INFO | train_inner | epoch 012:    329 / 561 symm_kl=0.437, self_kl=0, self_cv=0, loss=3.366, nll_loss=0.963, ppl=1.95, wps=17321.2, ups=1.63, wpb=10631.9, bsz=382.4, num_updates=6500, lr=2.71746e-05, gnorm=0.988, train_wall=61, wall=4290
2021-01-08 14:55:15 | INFO | train_inner | epoch 012:    429 / 561 symm_kl=0.45, self_kl=0, self_cv=0, loss=3.409, nll_loss=0.992, ppl=1.99, wps=17200.6, ups=1.63, wpb=10531, bsz=361.8, num_updates=6600, lr=2.6968e-05, gnorm=1.012, train_wall=61, wall=4351
2021-01-08 14:56:16 | INFO | train_inner | epoch 012:    529 / 561 symm_kl=0.439, self_kl=0, self_cv=0, loss=3.376, nll_loss=0.973, ppl=1.96, wps=17115.4, ups=1.63, wpb=10493.3, bsz=369.8, num_updates=6700, lr=2.6766e-05, gnorm=1.002, train_wall=61, wall=4412
2021-01-08 14:56:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 14:56:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:56:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:56:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:56:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:56:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:56:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:56:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:56:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:56:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:56:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:56:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:56:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:56:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:56:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:56:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:56:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:56:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:56:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:56:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:56:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:56:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:56:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:56:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:56:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:56:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:56:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:56:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:56:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:56:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:56:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:56:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:56:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:56:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:56:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:56:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:56:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:56:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 14:56:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 14:56:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 14:56:57 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.315 | nll_loss 3.791 | ppl 13.84 | bleu 22.75 | wps 4639.9 | wpb 7508.5 | bsz 272.7 | num_updates 6732 | best_bleu 22.81
2021-01-08 14:56:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 14:56:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:56:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:56:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:56:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:57:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 12 @ 6732 updates, score 22.75) (writing took 2.9452269840985537 seconds)
2021-01-08 14:57:00 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-01-08 14:57:00 | INFO | train | epoch 012 | symm_kl 0.443 | self_kl 0 | self_cv 0 | loss 3.387 | nll_loss 0.976 | ppl 1.97 | wps 15882.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 6732 | lr 2.67023e-05 | gnorm 1.006 | train_wall 342 | wall 4456
2021-01-08 14:57:00 | INFO | fairseq.trainer | begin training epoch 13
2021-01-08 14:57:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 14:57:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 14:57:44 | INFO | train_inner | epoch 013:     68 / 561 symm_kl=0.444, self_kl=0, self_cv=0, loss=3.383, nll_loss=0.971, ppl=1.96, wps=11847.9, ups=1.14, wpb=10372.9, bsz=360.1, num_updates=6800, lr=2.65684e-05, gnorm=1.016, train_wall=60, wall=4500
2021-01-08 14:58:45 | INFO | train_inner | epoch 013:    168 / 561 symm_kl=0.443, self_kl=0, self_cv=0, loss=3.387, nll_loss=0.974, ppl=1.96, wps=17231.8, ups=1.63, wpb=10571.4, bsz=349, num_updates=6900, lr=2.63752e-05, gnorm=0.996, train_wall=61, wall=4561
2021-01-08 14:59:47 | INFO | train_inner | epoch 013:    268 / 561 symm_kl=0.437, self_kl=0, self_cv=0, loss=3.377, nll_loss=0.976, ppl=1.97, wps=17037.3, ups=1.62, wpb=10544.3, bsz=369.9, num_updates=7000, lr=2.61861e-05, gnorm=0.985, train_wall=62, wall=4623
2021-01-08 15:00:48 | INFO | train_inner | epoch 013:    368 / 561 symm_kl=0.429, self_kl=0, self_cv=0, loss=3.355, nll_loss=0.964, ppl=1.95, wps=17052.1, ups=1.63, wpb=10490.1, bsz=389.7, num_updates=7100, lr=2.60011e-05, gnorm=0.989, train_wall=61, wall=4685
2021-01-08 15:01:50 | INFO | train_inner | epoch 013:    468 / 561 symm_kl=0.435, self_kl=0, self_cv=0, loss=3.375, nll_loss=0.975, ppl=1.97, wps=17110.1, ups=1.63, wpb=10510.3, bsz=369.8, num_updates=7200, lr=2.58199e-05, gnorm=0.984, train_wall=61, wall=4746
2021-01-08 15:02:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 15:02:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:02:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:02:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:02:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:02:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:02:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:02:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:02:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:02:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:02:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:02:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:02:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:02:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:02:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:02:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:02:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:02:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:02:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:02:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:02:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:02:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:02:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:02:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:02:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:02:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:02:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:02:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:02:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:02:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:02:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:02:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:02:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:02:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:03:08 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.31 | nll_loss 3.787 | ppl 13.8 | bleu 22.72 | wps 4729.5 | wpb 7508.5 | bsz 272.7 | num_updates 7293 | best_bleu 22.81
2021-01-08 15:03:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 15:03:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:03:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:03:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:03:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:03:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 13 @ 7293 updates, score 22.72) (writing took 2.8214061241596937 seconds)
2021-01-08 15:03:11 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2021-01-08 15:03:11 | INFO | train | epoch 013 | symm_kl 0.437 | self_kl 0 | self_cv 0 | loss 3.376 | nll_loss 0.974 | ppl 1.96 | wps 15858.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 7293 | lr 2.56547e-05 | gnorm 0.994 | train_wall 343 | wall 4827
2021-01-08 15:03:11 | INFO | fairseq.trainer | begin training epoch 14
2021-01-08 15:03:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:03:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:03:18 | INFO | train_inner | epoch 014:      7 / 561 symm_kl=0.436, self_kl=0, self_cv=0, loss=3.381, nll_loss=0.983, ppl=1.98, wps=11723.1, ups=1.14, wpb=10327.2, bsz=373.9, num_updates=7300, lr=2.56424e-05, gnorm=1.001, train_wall=61, wall=4834
2021-01-08 15:04:19 | INFO | train_inner | epoch 014:    107 / 561 symm_kl=0.438, self_kl=0, self_cv=0, loss=3.379, nll_loss=0.973, ppl=1.96, wps=17334.7, ups=1.64, wpb=10590, bsz=367, num_updates=7400, lr=2.54686e-05, gnorm=0.984, train_wall=61, wall=4895
2021-01-08 15:05:21 | INFO | train_inner | epoch 014:    207 / 561 symm_kl=0.429, self_kl=0, self_cv=0, loss=3.355, nll_loss=0.962, ppl=1.95, wps=17031.3, ups=1.61, wpb=10574.3, bsz=374, num_updates=7500, lr=2.52982e-05, gnorm=0.981, train_wall=62, wall=4957
2021-01-08 15:06:22 | INFO | train_inner | epoch 014:    307 / 561 symm_kl=0.434, self_kl=0, self_cv=0, loss=3.372, nll_loss=0.975, ppl=1.97, wps=17012, ups=1.64, wpb=10386.9, bsz=357.6, num_updates=7600, lr=2.51312e-05, gnorm=1.006, train_wall=61, wall=5018
2021-01-08 15:07:24 | INFO | train_inner | epoch 014:    407 / 561 symm_kl=0.438, self_kl=0, self_cv=0, loss=3.386, nll_loss=0.984, ppl=1.98, wps=16777.4, ups=1.62, wpb=10338.9, bsz=367.2, num_updates=7700, lr=2.49675e-05, gnorm=1, train_wall=61, wall=5080
2021-01-08 15:08:26 | INFO | train_inner | epoch 014:    507 / 561 symm_kl=0.429, self_kl=0, self_cv=0, loss=3.361, nll_loss=0.97, ppl=1.96, wps=17183.7, ups=1.62, wpb=10594.9, bsz=368.6, num_updates=7800, lr=2.48069e-05, gnorm=0.972, train_wall=61, wall=5142
2021-01-08 15:08:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 15:09:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:09:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:09:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:09:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:09:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:09:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:09:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:09:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:09:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:09:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:09:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:09:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:09:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:09:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:09:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:09:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:09:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:09:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:09:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:09:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:09:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:09:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:09:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:09:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:09:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:09:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:09:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:09:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:09:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:09:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:09:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:09:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:09:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:09:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:09:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:09:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:09:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:09:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:09:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:09:19 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.304 | nll_loss 3.784 | ppl 13.78 | bleu 22.82 | wps 4764.4 | wpb 7508.5 | bsz 272.7 | num_updates 7854 | best_bleu 22.82
2021-01-08 15:09:19 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 15:09:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:09:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:09:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:09:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:09:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 14 @ 7854 updates, score 22.82) (writing took 4.689290706068277 seconds)
2021-01-08 15:09:24 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2021-01-08 15:09:24 | INFO | train | epoch 014 | symm_kl 0.433 | self_kl 0 | self_cv 0 | loss 3.369 | nll_loss 0.973 | ppl 1.96 | wps 15759.2 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 7854 | lr 2.47215e-05 | gnorm 0.988 | train_wall 344 | wall 5200
2021-01-08 15:09:24 | INFO | fairseq.trainer | begin training epoch 15
2021-01-08 15:09:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:09:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:09:55 | INFO | train_inner | epoch 015:     46 / 561 symm_kl=0.431, self_kl=0, self_cv=0, loss=3.372, nll_loss=0.979, ppl=1.97, wps=11644.1, ups=1.12, wpb=10380.4, bsz=382.1, num_updates=7900, lr=2.46494e-05, gnorm=0.995, train_wall=61, wall=5231
2021-01-08 15:10:56 | INFO | train_inner | epoch 015:    146 / 561 symm_kl=0.43, self_kl=0, self_cv=0, loss=3.368, nll_loss=0.975, ppl=1.97, wps=17070.8, ups=1.63, wpb=10471, bsz=365.8, num_updates=8000, lr=2.44949e-05, gnorm=0.978, train_wall=61, wall=5292
2021-01-08 15:11:58 | INFO | train_inner | epoch 015:    246 / 561 symm_kl=0.424, self_kl=0, self_cv=0, loss=3.342, nll_loss=0.956, ppl=1.94, wps=17023.5, ups=1.62, wpb=10485.3, bsz=382.9, num_updates=8100, lr=2.43432e-05, gnorm=0.974, train_wall=61, wall=5354
2021-01-08 15:12:59 | INFO | train_inner | epoch 015:    346 / 561 symm_kl=0.427, self_kl=0, self_cv=0, loss=3.358, nll_loss=0.97, ppl=1.96, wps=17147.4, ups=1.63, wpb=10526.2, bsz=362.5, num_updates=8200, lr=2.41943e-05, gnorm=0.98, train_wall=61, wall=5415
2021-01-08 15:14:01 | INFO | train_inner | epoch 015:    446 / 561 symm_kl=0.425, self_kl=0, self_cv=0, loss=3.357, nll_loss=0.973, ppl=1.96, wps=17111.2, ups=1.63, wpb=10527.6, bsz=374.3, num_updates=8300, lr=2.40481e-05, gnorm=0.958, train_wall=61, wall=5477
2021-01-08 15:15:02 | INFO | train_inner | epoch 015:    546 / 561 symm_kl=0.432, self_kl=0, self_cv=0, loss=3.378, nll_loss=0.986, ppl=1.98, wps=17097.8, ups=1.63, wpb=10468.7, bsz=364.2, num_updates=8400, lr=2.39046e-05, gnorm=0.982, train_wall=61, wall=5538
2021-01-08 15:15:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 15:15:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:15:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:15:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:15:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:15:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:15:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:15:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:15:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:15:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:15:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:15:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:15:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:15:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:15:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:15:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:15:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:15:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:15:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:15:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:15:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:15:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:15:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:15:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:15:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:15:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:15:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:15:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:15:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:15:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:15:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:15:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:15:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:15:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:15:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:15:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:15:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:15:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:15:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:15:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:15:33 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.306 | nll_loss 3.785 | ppl 13.78 | bleu 22.73 | wps 4199.2 | wpb 7508.5 | bsz 272.7 | num_updates 8415 | best_bleu 22.82
2021-01-08 15:15:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 15:15:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:15:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:15:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:15:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:15:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 15 @ 8415 updates, score 22.73) (writing took 2.8670711517333984 seconds)
2021-01-08 15:15:36 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2021-01-08 15:15:36 | INFO | train | epoch 015 | symm_kl 0.428 | self_kl 0 | self_cv 0 | loss 3.36 | nll_loss 0.971 | ppl 1.96 | wps 15788.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 8415 | lr 2.38833e-05 | gnorm 0.977 | train_wall 343 | wall 5572
2021-01-08 15:15:36 | INFO | fairseq.trainer | begin training epoch 16
2021-01-08 15:15:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:15:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:16:31 | INFO | train_inner | epoch 016:     85 / 561 symm_kl=0.421, self_kl=0, self_cv=0, loss=3.342, nll_loss=0.961, ppl=1.95, wps=11619.7, ups=1.12, wpb=10408.1, bsz=377.6, num_updates=8500, lr=2.37635e-05, gnorm=0.972, train_wall=61, wall=5628
2021-01-08 15:17:33 | INFO | train_inner | epoch 016:    185 / 561 symm_kl=0.426, self_kl=0, self_cv=0, loss=3.357, nll_loss=0.971, ppl=1.96, wps=16955.7, ups=1.63, wpb=10433.6, bsz=372.2, num_updates=8600, lr=2.3625e-05, gnorm=0.976, train_wall=61, wall=5689
2021-01-08 15:18:34 | INFO | train_inner | epoch 016:    285 / 561 symm_kl=0.426, self_kl=0, self_cv=0, loss=3.357, nll_loss=0.97, ppl=1.96, wps=17151.1, ups=1.63, wpb=10533.1, bsz=379.8, num_updates=8700, lr=2.34888e-05, gnorm=0.979, train_wall=61, wall=5750
2021-01-08 15:19:36 | INFO | train_inner | epoch 016:    385 / 561 symm_kl=0.416, self_kl=0, self_cv=0, loss=3.33, nll_loss=0.956, ppl=1.94, wps=17098.8, ups=1.62, wpb=10551.2, bsz=375.2, num_updates=8800, lr=2.3355e-05, gnorm=0.951, train_wall=62, wall=5812
2021-01-08 15:20:38 | INFO | train_inner | epoch 016:    485 / 561 symm_kl=0.425, self_kl=0, self_cv=0, loss=3.357, nll_loss=0.971, ppl=1.96, wps=16959.5, ups=1.62, wpb=10494.4, bsz=360.2, num_updates=8900, lr=2.32234e-05, gnorm=0.969, train_wall=62, wall=5874
2021-01-08 15:21:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 15:21:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:21:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:21:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:21:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:21:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:21:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:21:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:21:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:21:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:21:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:21:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:21:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:21:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:21:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:21:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:21:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:21:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:21:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:21:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:21:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:21:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:21:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:21:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:21:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:21:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:21:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:21:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:21:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:21:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:21:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:21:46 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.299 | nll_loss 3.78 | ppl 13.73 | bleu 22.67 | wps 4583.6 | wpb 7508.5 | bsz 272.7 | num_updates 8976 | best_bleu 22.82
2021-01-08 15:21:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 15:21:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:21:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:21:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:21:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:21:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 16 @ 8976 updates, score 22.67) (writing took 2.830917976796627 seconds)
2021-01-08 15:21:49 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2021-01-08 15:21:49 | INFO | train | epoch 016 | symm_kl 0.424 | self_kl 0 | self_cv 0 | loss 3.353 | nll_loss 0.97 | ppl 1.96 | wps 15786.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 8976 | lr 2.31249e-05 | gnorm 0.972 | train_wall 345 | wall 5945
2021-01-08 15:21:49 | INFO | fairseq.trainer | begin training epoch 17
2021-01-08 15:21:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:21:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:22:06 | INFO | train_inner | epoch 017:     24 / 561 symm_kl=0.423, self_kl=0, self_cv=0, loss=3.36, nll_loss=0.98, ppl=1.97, wps=11804.3, ups=1.13, wpb=10459.4, bsz=358.3, num_updates=9000, lr=2.3094e-05, gnorm=0.973, train_wall=61, wall=5963
2021-01-08 15:23:08 | INFO | train_inner | epoch 017:    124 / 561 symm_kl=0.422, self_kl=0, self_cv=0, loss=3.348, nll_loss=0.967, ppl=1.95, wps=16928.3, ups=1.61, wpb=10489.1, bsz=374.8, num_updates=9100, lr=2.29668e-05, gnorm=0.972, train_wall=62, wall=6025
2021-01-08 15:24:11 | INFO | train_inner | epoch 017:    224 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.317, nll_loss=0.947, ppl=1.93, wps=17006.7, ups=1.61, wpb=10592.8, bsz=369.3, num_updates=9200, lr=2.28416e-05, gnorm=0.96, train_wall=62, wall=6087
2021-01-08 15:25:12 | INFO | train_inner | epoch 017:    324 / 561 symm_kl=0.421, self_kl=0, self_cv=0, loss=3.341, nll_loss=0.96, ppl=1.95, wps=17010.8, ups=1.62, wpb=10507.6, bsz=365.5, num_updates=9300, lr=2.27185e-05, gnorm=0.967, train_wall=62, wall=6149
2021-01-08 15:26:14 | INFO | train_inner | epoch 017:    424 / 561 symm_kl=0.421, self_kl=0, self_cv=0, loss=3.356, nll_loss=0.977, ppl=1.97, wps=16791.4, ups=1.61, wpb=10397.2, bsz=380.2, num_updates=9400, lr=2.25973e-05, gnorm=0.973, train_wall=62, wall=6211
2021-01-08 15:27:16 | INFO | train_inner | epoch 017:    524 / 561 symm_kl=0.426, self_kl=0, self_cv=0, loss=3.376, nll_loss=0.992, ppl=1.99, wps=16906.5, ups=1.61, wpb=10491.6, bsz=362.4, num_updates=9500, lr=2.24781e-05, gnorm=0.972, train_wall=62, wall=6273
2021-01-08 15:27:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 15:27:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:27:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:27:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:27:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:27:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:27:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:27:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:27:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:27:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:27:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:27:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:27:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:27:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:27:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:27:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:27:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:27:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:27:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:27:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:27:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:27:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:27:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:27:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:27:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:27:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:27:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:27:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:27:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:27:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:27:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:27:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:27:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:27:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:27:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:27:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:27:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:27:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:27:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:27:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:28:00 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.293 | nll_loss 3.777 | ppl 13.71 | bleu 22.73 | wps 4612.3 | wpb 7508.5 | bsz 272.7 | num_updates 9537 | best_bleu 22.82
2021-01-08 15:28:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 15:28:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:28:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:28:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:28:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:28:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 17 @ 9537 updates, score 22.73) (writing took 2.9231839179992676 seconds)
2021-01-08 15:28:03 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2021-01-08 15:28:03 | INFO | train | epoch 017 | symm_kl 0.42 | self_kl 0 | self_cv 0 | loss 3.348 | nll_loss 0.969 | ppl 1.96 | wps 15717 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 9537 | lr 2.24344e-05 | gnorm 0.968 | train_wall 346 | wall 6319
2021-01-08 15:28:03 | INFO | fairseq.trainer | begin training epoch 18
2021-01-08 15:28:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:28:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:28:44 | INFO | train_inner | epoch 018:     63 / 561 symm_kl=0.42, self_kl=0, self_cv=0, loss=3.347, nll_loss=0.969, ppl=1.96, wps=11831.8, ups=1.14, wpb=10417.2, bsz=358.2, num_updates=9600, lr=2.23607e-05, gnorm=0.966, train_wall=61, wall=6361
2021-01-08 15:29:47 | INFO | train_inner | epoch 018:    163 / 561 symm_kl=0.417, self_kl=0, self_cv=0, loss=3.341, nll_loss=0.967, ppl=1.95, wps=16883.1, ups=1.61, wpb=10492.8, bsz=370, num_updates=9700, lr=2.22451e-05, gnorm=0.961, train_wall=62, wall=6423
2021-01-08 15:30:49 | INFO | train_inner | epoch 018:    263 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.334, nll_loss=0.961, ppl=1.95, wps=16873.3, ups=1.61, wpb=10456, bsz=363.1, num_updates=9800, lr=2.21313e-05, gnorm=0.956, train_wall=62, wall=6485
2021-01-08 15:31:50 | INFO | train_inner | epoch 018:    363 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.34, nll_loss=0.971, ppl=1.96, wps=16926.6, ups=1.63, wpb=10380.2, bsz=372.9, num_updates=9900, lr=2.20193e-05, gnorm=0.967, train_wall=61, wall=6546
2021-01-08 15:32:52 | INFO | train_inner | epoch 018:    463 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.337, nll_loss=0.97, ppl=1.96, wps=17198.2, ups=1.62, wpb=10622, bsz=379.6, num_updates=10000, lr=2.19089e-05, gnorm=0.94, train_wall=62, wall=6608
2021-01-08 15:33:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 15:33:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:33:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:33:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:33:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:33:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:33:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:33:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:33:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:33:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:33:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:33:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:33:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:33:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:33:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:33:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:33:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:33:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:33:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:33:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:33:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:33:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:34:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:34:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:34:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:34:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:34:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:34:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:34:13 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.292 | nll_loss 3.779 | ppl 13.73 | bleu 22.71 | wps 4604.8 | wpb 7508.5 | bsz 272.7 | num_updates 10098 | best_bleu 22.82
2021-01-08 15:34:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 15:34:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:34:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:34:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:34:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:34:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 18 @ 10098 updates, score 22.71) (writing took 2.8703788239508867 seconds)
2021-01-08 15:34:16 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2021-01-08 15:34:16 | INFO | train | epoch 018 | symm_kl 0.416 | self_kl 0 | self_cv 0 | loss 3.34 | nll_loss 0.968 | ppl 1.96 | wps 15779 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 10098 | lr 2.18023e-05 | gnorm 0.957 | train_wall 345 | wall 6692
2021-01-08 15:34:16 | INFO | fairseq.trainer | begin training epoch 19
2021-01-08 15:34:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:34:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:34:20 | INFO | train_inner | epoch 019:      2 / 561 symm_kl=0.419, self_kl=0, self_cv=0, loss=3.355, nll_loss=0.98, ppl=1.97, wps=11776.2, ups=1.13, wpb=10417.9, bsz=365.4, num_updates=10100, lr=2.18002e-05, gnorm=0.962, train_wall=61, wall=6696
2021-01-08 15:35:21 | INFO | train_inner | epoch 019:    102 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.329, nll_loss=0.959, ppl=1.94, wps=17014.2, ups=1.63, wpb=10422.9, bsz=373.4, num_updates=10200, lr=2.1693e-05, gnorm=0.954, train_wall=61, wall=6758
2021-01-08 15:36:24 | INFO | train_inner | epoch 019:    202 / 561 symm_kl=0.407, self_kl=0, self_cv=0, loss=3.318, nll_loss=0.958, ppl=1.94, wps=17020.2, ups=1.6, wpb=10632.5, bsz=380, num_updates=10300, lr=2.15875e-05, gnorm=0.926, train_wall=62, wall=6820
2021-01-08 15:37:25 | INFO | train_inner | epoch 019:    302 / 561 symm_kl=0.415, self_kl=0, self_cv=0, loss=3.338, nll_loss=0.967, ppl=1.95, wps=17089.4, ups=1.63, wpb=10486.3, bsz=361.5, num_updates=10400, lr=2.14834e-05, gnorm=0.962, train_wall=61, wall=6881
2021-01-08 15:38:27 | INFO | train_inner | epoch 019:    402 / 561 symm_kl=0.418, self_kl=0, self_cv=0, loss=3.351, nll_loss=0.977, ppl=1.97, wps=16955.2, ups=1.62, wpb=10482.8, bsz=369.7, num_updates=10500, lr=2.13809e-05, gnorm=0.96, train_wall=62, wall=6943
2021-01-08 15:39:29 | INFO | train_inner | epoch 019:    502 / 561 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.326, nll_loss=0.962, ppl=1.95, wps=16899.5, ups=1.62, wpb=10451.7, bsz=365.4, num_updates=10600, lr=2.12798e-05, gnorm=0.943, train_wall=62, wall=7005
2021-01-08 15:40:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 15:40:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:40:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:40:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:40:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:40:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:40:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:40:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:40:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:40:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:40:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:40:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:40:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:40:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:40:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:40:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:40:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:40:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:40:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:40:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:40:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:40:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:40:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:40:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:40:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:40:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:40:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:40:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:40:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:40:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:40:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:40:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:40:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:40:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:40:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:40:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:40:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:40:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:40:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:40:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:40:26 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.285 | nll_loss 3.771 | ppl 13.65 | bleu 22.86 | wps 4654.1 | wpb 7508.5 | bsz 272.7 | num_updates 10659 | best_bleu 22.86
2021-01-08 15:40:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 15:40:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:40:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:40:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:40:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:40:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 19 @ 10659 updates, score 22.86) (writing took 4.686824439093471 seconds)
2021-01-08 15:40:30 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2021-01-08 15:40:30 | INFO | train | epoch 019 | symm_kl 0.413 | self_kl 0 | self_cv 0 | loss 3.335 | nll_loss 0.967 | ppl 1.95 | wps 15693.3 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 10659 | lr 2.12208e-05 | gnorm 0.953 | train_wall 345 | wall 7067
2021-01-08 15:40:30 | INFO | fairseq.trainer | begin training epoch 20
2021-01-08 15:40:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:40:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:40:58 | INFO | train_inner | epoch 020:     41 / 561 symm_kl=0.412, self_kl=0, self_cv=0, loss=3.333, nll_loss=0.969, ppl=1.96, wps=11738.1, ups=1.12, wpb=10477.3, bsz=372.4, num_updates=10700, lr=2.11801e-05, gnorm=0.959, train_wall=61, wall=7094
2021-01-08 15:42:00 | INFO | train_inner | epoch 020:    141 / 561 symm_kl=0.413, self_kl=0, self_cv=0, loss=3.34, nll_loss=0.97, ppl=1.96, wps=17059.4, ups=1.62, wpb=10538.1, bsz=365.8, num_updates=10800, lr=2.10819e-05, gnorm=0.949, train_wall=62, wall=7156
2021-01-08 15:43:02 | INFO | train_inner | epoch 020:    241 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.323, nll_loss=0.962, ppl=1.95, wps=16951, ups=1.61, wpb=10496, bsz=377, num_updates=10900, lr=2.09849e-05, gnorm=0.939, train_wall=62, wall=7218
2021-01-08 15:44:03 | INFO | train_inner | epoch 020:    341 / 561 symm_kl=0.414, self_kl=0, self_cv=0, loss=3.334, nll_loss=0.964, ppl=1.95, wps=17130.8, ups=1.63, wpb=10521.2, bsz=361.4, num_updates=11000, lr=2.08893e-05, gnorm=0.949, train_wall=61, wall=7280
2021-01-08 15:45:05 | INFO | train_inner | epoch 020:    441 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.332, nll_loss=0.97, ppl=1.96, wps=16907.7, ups=1.62, wpb=10445.5, bsz=372.4, num_updates=11100, lr=2.0795e-05, gnorm=0.954, train_wall=62, wall=7341
2021-01-08 15:46:07 | INFO | train_inner | epoch 020:    541 / 561 symm_kl=0.411, self_kl=0, self_cv=0, loss=3.337, nll_loss=0.972, ppl=1.96, wps=16892.4, ups=1.62, wpb=10410.2, bsz=367.4, num_updates=11200, lr=2.0702e-05, gnorm=0.957, train_wall=61, wall=7403
2021-01-08 15:46:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 15:46:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:46:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:46:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:46:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:46:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:46:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:46:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:46:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:46:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:46:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:46:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:46:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:46:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:46:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:46:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:46:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:46:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:46:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:46:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:46:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:46:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:46:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:46:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:46:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:46:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:46:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:46:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:46:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:46:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:46:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:46:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:46:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:46:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:46:40 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.289 | nll_loss 3.773 | ppl 13.67 | bleu 22.88 | wps 4576.1 | wpb 7508.5 | bsz 272.7 | num_updates 11220 | best_bleu 22.88
2021-01-08 15:46:40 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 15:46:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:46:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:46:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:46:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:46:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 20 @ 11220 updates, score 22.88) (writing took 4.776103787124157 seconds)
2021-01-08 15:46:45 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2021-01-08 15:46:45 | INFO | train | epoch 020 | symm_kl 0.41 | self_kl 0 | self_cv 0 | loss 3.331 | nll_loss 0.966 | ppl 1.95 | wps 15714.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 11220 | lr 2.06835e-05 | gnorm 0.948 | train_wall 344 | wall 7441
2021-01-08 15:46:45 | INFO | fairseq.trainer | begin training epoch 21
2021-01-08 15:46:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:46:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:47:36 | INFO | train_inner | epoch 021:     80 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.333, nll_loss=0.971, ppl=1.96, wps=11525.2, ups=1.12, wpb=10315.2, bsz=376.1, num_updates=11300, lr=2.06102e-05, gnorm=0.96, train_wall=60, wall=7492
2021-01-08 15:48:38 | INFO | train_inner | epoch 021:    180 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.326, nll_loss=0.965, ppl=1.95, wps=16957.8, ups=1.61, wpb=10526.7, bsz=359.6, num_updates=11400, lr=2.05196e-05, gnorm=0.944, train_wall=62, wall=7555
2021-01-08 15:49:41 | INFO | train_inner | epoch 021:    280 / 561 symm_kl=0.405, self_kl=0, self_cv=0, loss=3.313, nll_loss=0.955, ppl=1.94, wps=16883.6, ups=1.61, wpb=10514.2, bsz=377.1, num_updates=11500, lr=2.04302e-05, gnorm=0.941, train_wall=62, wall=7617
2021-01-08 15:50:42 | INFO | train_inner | epoch 021:    380 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.306, nll_loss=0.952, ppl=1.93, wps=17137, ups=1.62, wpb=10592.6, bsz=375.9, num_updates=11600, lr=2.03419e-05, gnorm=0.935, train_wall=62, wall=7679
2021-01-08 15:51:44 | INFO | train_inner | epoch 021:    480 / 561 symm_kl=0.409, self_kl=0, self_cv=0, loss=3.333, nll_loss=0.971, ppl=1.96, wps=16961.6, ups=1.62, wpb=10501.6, bsz=373.3, num_updates=11700, lr=2.02548e-05, gnorm=0.934, train_wall=62, wall=7741
2021-01-08 15:52:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 15:52:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:52:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:52:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:52:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:52:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:52:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:52:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:52:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:52:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:52:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:52:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:52:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:52:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:52:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:52:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:52:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:52:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:52:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:52:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:52:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:52:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:52:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:52:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:52:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:52:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:52:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:52:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:52:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:52:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:52:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:52:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:52:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:52:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:52:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:52:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:52:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:52:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:52:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:52:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:52:55 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.283 | nll_loss 3.769 | ppl 13.64 | bleu 22.81 | wps 4631.1 | wpb 7508.5 | bsz 272.7 | num_updates 11781 | best_bleu 22.88
2021-01-08 15:52:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 15:52:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:52:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:52:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:52:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:52:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 21 @ 11781 updates, score 22.81) (writing took 2.8777775205671787 seconds)
2021-01-08 15:52:58 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2021-01-08 15:52:58 | INFO | train | epoch 021 | symm_kl 0.408 | self_kl 0 | self_cv 0 | loss 3.326 | nll_loss 0.965 | ppl 1.95 | wps 15770 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 11781 | lr 2.0185e-05 | gnorm 0.946 | train_wall 345 | wall 7814
2021-01-08 15:52:58 | INFO | fairseq.trainer | begin training epoch 22
2021-01-08 15:52:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:53:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:53:12 | INFO | train_inner | epoch 022:     19 / 561 symm_kl=0.412, self_kl=0, self_cv=0, loss=3.341, nll_loss=0.975, ppl=1.97, wps=11910.5, ups=1.14, wpb=10469.7, bsz=353, num_updates=11800, lr=2.01688e-05, gnorm=0.956, train_wall=61, wall=7828
2021-01-08 15:54:14 | INFO | train_inner | epoch 022:    119 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.309, nll_loss=0.954, ppl=1.94, wps=17271.7, ups=1.63, wpb=10589.6, bsz=376.8, num_updates=11900, lr=2.00839e-05, gnorm=0.93, train_wall=61, wall=7890
2021-01-08 15:55:15 | INFO | train_inner | epoch 022:    219 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.304, nll_loss=0.955, ppl=1.94, wps=16935.3, ups=1.61, wpb=10493.2, bsz=374.8, num_updates=12000, lr=2e-05, gnorm=0.922, train_wall=62, wall=7952
2021-01-08 15:56:17 | INFO | train_inner | epoch 022:    319 / 561 symm_kl=0.411, self_kl=0, self_cv=0, loss=3.34, nll_loss=0.977, ppl=1.97, wps=16876.8, ups=1.62, wpb=10407.4, bsz=371.2, num_updates=12100, lr=1.99172e-05, gnorm=0.942, train_wall=61, wall=8013
2021-01-08 15:57:18 | INFO | train_inner | epoch 022:    419 / 561 symm_kl=0.404, self_kl=0, self_cv=0, loss=3.306, nll_loss=0.949, ppl=1.93, wps=17176.8, ups=1.63, wpb=10536.2, bsz=364.9, num_updates=12200, lr=1.98354e-05, gnorm=0.936, train_wall=61, wall=8075
2021-01-08 15:58:20 | INFO | train_inner | epoch 022:    519 / 561 symm_kl=0.408, self_kl=0, self_cv=0, loss=3.337, nll_loss=0.977, ppl=1.97, wps=17031.4, ups=1.63, wpb=10446.6, bsz=366.8, num_updates=12300, lr=1.97546e-05, gnorm=0.945, train_wall=61, wall=8136
2021-01-08 15:58:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 15:58:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:58:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:58:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:58:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:58:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:58:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:58:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:58:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:58:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:58:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:58:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:58:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:58:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:58:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:58:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:58:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:58:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:58:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:58:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:58:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:58:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:58:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:58:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:58:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:58:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:58:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:58:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:58:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:58:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:58:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:58:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:58:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:58:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:58:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:58:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:58:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:58:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 15:58:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 15:58:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 15:59:06 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.282 | nll_loss 3.766 | ppl 13.6 | bleu 22.98 | wps 4632.1 | wpb 7508.5 | bsz 272.7 | num_updates 12342 | best_bleu 22.98
2021-01-08 15:59:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 15:59:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:59:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:59:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:59:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:59:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 22 @ 12342 updates, score 22.98) (writing took 4.882069563493133 seconds)
2021-01-08 15:59:11 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2021-01-08 15:59:11 | INFO | train | epoch 022 | symm_kl 0.405 | self_kl 0 | self_cv 0 | loss 3.321 | nll_loss 0.964 | ppl 1.95 | wps 15739.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 12342 | lr 1.9721e-05 | gnorm 0.937 | train_wall 344 | wall 8188
2021-01-08 15:59:11 | INFO | fairseq.trainer | begin training epoch 23
2021-01-08 15:59:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 15:59:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 15:59:49 | INFO | train_inner | epoch 023:     58 / 561 symm_kl=0.414, self_kl=0, self_cv=0, loss=3.349, nll_loss=0.981, ppl=1.97, wps=11639.7, ups=1.12, wpb=10402.8, bsz=349, num_updates=12400, lr=1.96748e-05, gnorm=0.963, train_wall=60, wall=8225
2021-01-08 16:00:51 | INFO | train_inner | epoch 023:    158 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.311, nll_loss=0.962, ppl=1.95, wps=16982.1, ups=1.61, wpb=10522.5, bsz=379.2, num_updates=12500, lr=1.95959e-05, gnorm=0.926, train_wall=62, wall=8287
2021-01-08 16:01:53 | INFO | train_inner | epoch 023:    258 / 561 symm_kl=0.41, self_kl=0, self_cv=0, loss=3.337, nll_loss=0.973, ppl=1.96, wps=16974.6, ups=1.63, wpb=10426.7, bsz=354.1, num_updates=12600, lr=1.9518e-05, gnorm=0.96, train_wall=61, wall=8349
2021-01-08 16:02:54 | INFO | train_inner | epoch 023:    358 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.307, nll_loss=0.958, ppl=1.94, wps=17170.7, ups=1.62, wpb=10593.4, bsz=371.4, num_updates=12700, lr=1.9441e-05, gnorm=0.921, train_wall=62, wall=8410
2021-01-08 16:03:56 | INFO | train_inner | epoch 023:    458 / 561 symm_kl=0.401, self_kl=0, self_cv=0, loss=3.314, nll_loss=0.963, ppl=1.95, wps=16994.3, ups=1.63, wpb=10408.4, bsz=380.2, num_updates=12800, lr=1.93649e-05, gnorm=0.927, train_wall=61, wall=8472
2021-01-08 16:04:57 | INFO | train_inner | epoch 023:    558 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.301, nll_loss=0.953, ppl=1.94, wps=16989.7, ups=1.61, wpb=10521.5, bsz=381, num_updates=12900, lr=1.92897e-05, gnorm=0.93, train_wall=62, wall=8534
2021-01-08 16:04:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 16:05:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:05:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:05:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:05:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:05:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:05:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:05:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:05:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:05:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:05:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:05:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:05:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:05:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:05:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:05:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:05:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:05:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:05:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:05:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:05:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:05:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:05:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:05:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:05:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:05:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:05:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:05:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:05:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:05:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:05:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:05:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:05:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:05:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:05:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:05:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:05:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:05:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:05:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:05:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:05:20 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.278 | nll_loss 3.765 | ppl 13.6 | bleu 22.68 | wps 4662.5 | wpb 7508.5 | bsz 272.7 | num_updates 12903 | best_bleu 22.98
2021-01-08 16:05:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 16:05:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:05:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:05:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:05:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:05:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 23 @ 12903 updates, score 22.68) (writing took 2.926200309768319 seconds)
2021-01-08 16:05:23 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2021-01-08 16:05:23 | INFO | train | epoch 023 | symm_kl 0.403 | self_kl 0 | self_cv 0 | loss 3.317 | nll_loss 0.963 | ppl 1.95 | wps 15833.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 12903 | lr 1.92875e-05 | gnorm 0.936 | train_wall 344 | wall 8559
2021-01-08 16:05:23 | INFO | fairseq.trainer | begin training epoch 24
2021-01-08 16:05:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:05:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:06:25 | INFO | train_inner | epoch 024:     97 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.308, nll_loss=0.958, ppl=1.94, wps=12035, ups=1.14, wpb=10536, bsz=373.3, num_updates=13000, lr=1.92154e-05, gnorm=0.923, train_wall=61, wall=8621
2021-01-08 16:07:26 | INFO | train_inner | epoch 024:    197 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.312, nll_loss=0.957, ppl=1.94, wps=16954.2, ups=1.63, wpb=10426.6, bsz=361.6, num_updates=13100, lr=1.91419e-05, gnorm=0.928, train_wall=61, wall=8683
2021-01-08 16:08:28 | INFO | train_inner | epoch 024:    297 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.307, nll_loss=0.957, ppl=1.94, wps=17105, ups=1.62, wpb=10536, bsz=369, num_updates=13200, lr=1.90693e-05, gnorm=0.92, train_wall=61, wall=8744
2021-01-08 16:09:29 | INFO | train_inner | epoch 024:    397 / 561 symm_kl=0.402, self_kl=0, self_cv=0, loss=3.322, nll_loss=0.97, ppl=1.96, wps=16922, ups=1.64, wpb=10307.2, bsz=367.5, num_updates=13300, lr=1.89974e-05, gnorm=0.933, train_wall=61, wall=8805
2021-01-08 16:10:31 | INFO | train_inner | epoch 024:    497 / 561 symm_kl=0.403, self_kl=0, self_cv=0, loss=3.328, nll_loss=0.976, ppl=1.97, wps=16999.5, ups=1.62, wpb=10469.8, bsz=376.6, num_updates=13400, lr=1.89264e-05, gnorm=0.931, train_wall=61, wall=8867
2021-01-08 16:11:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 16:11:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:11:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:11:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:11:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:11:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:11:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:11:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:11:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:11:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:11:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:11:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:11:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:11:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:11:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:11:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:11:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:11:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:11:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:11:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:11:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:11:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:11:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:11:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:11:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:11:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:11:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:11:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:11:31 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.276 | nll_loss 3.765 | ppl 13.59 | bleu 22.7 | wps 4624.6 | wpb 7508.5 | bsz 272.7 | num_updates 13464 | best_bleu 22.98
2021-01-08 16:11:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 16:11:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:11:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:11:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:11:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:11:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 24 @ 13464 updates, score 22.7) (writing took 3.0187267437577248 seconds)
2021-01-08 16:11:34 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2021-01-08 16:11:34 | INFO | train | epoch 024 | symm_kl 0.4 | self_kl 0 | self_cv 0 | loss 3.312 | nll_loss 0.961 | ppl 1.95 | wps 15845.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 13464 | lr 1.88814e-05 | gnorm 0.925 | train_wall 343 | wall 8930
2021-01-08 16:11:34 | INFO | fairseq.trainer | begin training epoch 25
2021-01-08 16:11:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:11:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:11:59 | INFO | train_inner | epoch 025:     36 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.295, nll_loss=0.948, ppl=1.93, wps=11990.7, ups=1.13, wpb=10574, bsz=365.8, num_updates=13500, lr=1.88562e-05, gnorm=0.915, train_wall=61, wall=8955
2021-01-08 16:13:00 | INFO | train_inner | epoch 025:    136 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.3, nll_loss=0.95, ppl=1.93, wps=17185.1, ups=1.63, wpb=10570.8, bsz=359.8, num_updates=13600, lr=1.87867e-05, gnorm=0.92, train_wall=61, wall=9017
2021-01-08 16:14:02 | INFO | train_inner | epoch 025:    236 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.312, nll_loss=0.963, ppl=1.95, wps=16874.2, ups=1.63, wpb=10377.5, bsz=373.6, num_updates=13700, lr=1.8718e-05, gnorm=0.926, train_wall=61, wall=9078
2021-01-08 16:15:03 | INFO | train_inner | epoch 025:    336 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.311, nll_loss=0.963, ppl=1.95, wps=17069.3, ups=1.62, wpb=10508.8, bsz=363, num_updates=13800, lr=1.86501e-05, gnorm=0.926, train_wall=61, wall=9140
2021-01-08 16:16:05 | INFO | train_inner | epoch 025:    436 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.313, nll_loss=0.967, ppl=1.95, wps=17262, ups=1.62, wpb=10632, bsz=381.2, num_updates=13900, lr=1.85829e-05, gnorm=0.919, train_wall=61, wall=9201
2021-01-08 16:17:06 | INFO | train_inner | epoch 025:    536 / 561 symm_kl=0.396, self_kl=0, self_cv=0, loss=3.306, nll_loss=0.962, ppl=1.95, wps=16888.5, ups=1.63, wpb=10384.2, bsz=378.7, num_updates=14000, lr=1.85164e-05, gnorm=0.932, train_wall=61, wall=9263
2021-01-08 16:17:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 16:17:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:17:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:17:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:17:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:17:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:17:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:17:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:17:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:17:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:17:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:17:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:17:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:17:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:17:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:17:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:17:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:17:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:17:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:17:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:17:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:17:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:17:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:17:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:17:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:17:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:17:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:17:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:17:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:17:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:17:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:17:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:17:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:17:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:17:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:17:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:17:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:17:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:17:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:17:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:17:43 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.276 | nll_loss 3.767 | ppl 13.61 | bleu 22.69 | wps 4655.6 | wpb 7508.5 | bsz 272.7 | num_updates 14025 | best_bleu 22.98
2021-01-08 16:17:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 16:17:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:17:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:17:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:17:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:17:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 25 @ 14025 updates, score 22.69) (writing took 3.0610741022974253 seconds)
2021-01-08 16:17:46 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2021-01-08 16:17:46 | INFO | train | epoch 025 | symm_kl 0.398 | self_kl 0 | self_cv 0 | loss 3.308 | nll_loss 0.961 | ppl 1.95 | wps 15822.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 14025 | lr 1.84999e-05 | gnorm 0.926 | train_wall 344 | wall 9302
2021-01-08 16:17:46 | INFO | fairseq.trainer | begin training epoch 26
2021-01-08 16:17:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:17:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:18:34 | INFO | train_inner | epoch 026:     75 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.281, nll_loss=0.945, ppl=1.93, wps=11866.4, ups=1.14, wpb=10395, bsz=387.4, num_updates=14100, lr=1.84506e-05, gnorm=0.923, train_wall=61, wall=9350
2021-01-08 16:19:35 | INFO | train_inner | epoch 026:    175 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.307, nll_loss=0.955, ppl=1.94, wps=17050.3, ups=1.64, wpb=10422.9, bsz=353.8, num_updates=14200, lr=1.83855e-05, gnorm=0.938, train_wall=61, wall=9411
2021-01-08 16:20:37 | INFO | train_inner | epoch 026:    275 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.291, nll_loss=0.953, ppl=1.94, wps=17352.5, ups=1.63, wpb=10670.9, bsz=391.1, num_updates=14300, lr=1.83211e-05, gnorm=0.902, train_wall=61, wall=9473
2021-01-08 16:21:38 | INFO | train_inner | epoch 026:    375 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.317, nll_loss=0.967, ppl=1.96, wps=17103.3, ups=1.62, wpb=10536.3, bsz=368.1, num_updates=14400, lr=1.82574e-05, gnorm=0.926, train_wall=61, wall=9534
2021-01-08 16:22:40 | INFO | train_inner | epoch 026:    475 / 561 symm_kl=0.399, self_kl=0, self_cv=0, loss=3.313, nll_loss=0.965, ppl=1.95, wps=17103.6, ups=1.62, wpb=10526, bsz=359.1, num_updates=14500, lr=1.81944e-05, gnorm=0.921, train_wall=61, wall=9596
2021-01-08 16:23:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 16:23:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:23:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:23:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:23:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:23:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:23:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:23:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:23:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:23:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:23:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:23:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:23:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:23:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:23:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:23:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:23:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:23:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:23:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:23:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:23:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:23:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:23:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:23:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:23:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:23:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:23:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:23:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:23:55 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.275 | nll_loss 3.766 | ppl 13.6 | bleu 22.75 | wps 4250.1 | wpb 7508.5 | bsz 272.7 | num_updates 14586 | best_bleu 22.98
2021-01-08 16:23:55 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 16:23:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:23:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:23:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:23:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:23:58 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 26 @ 14586 updates, score 22.75) (writing took 3.0598806757479906 seconds)
2021-01-08 16:23:58 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2021-01-08 16:23:58 | INFO | train | epoch 026 | symm_kl 0.396 | self_kl 0 | self_cv 0 | loss 3.305 | nll_loss 0.96 | ppl 1.95 | wps 15796 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 14586 | lr 1.81406e-05 | gnorm 0.923 | train_wall 343 | wall 9674
2021-01-08 16:23:58 | INFO | fairseq.trainer | begin training epoch 27
2021-01-08 16:23:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:24:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:24:10 | INFO | train_inner | epoch 027:     14 / 561 symm_kl=0.4, self_kl=0, self_cv=0, loss=3.322, nll_loss=0.975, ppl=1.97, wps=11444, ups=1.11, wpb=10277.2, bsz=352.9, num_updates=14600, lr=1.81319e-05, gnorm=0.934, train_wall=61, wall=9686
2021-01-08 16:25:11 | INFO | train_inner | epoch 027:    114 / 561 symm_kl=0.398, self_kl=0, self_cv=0, loss=3.311, nll_loss=0.964, ppl=1.95, wps=17166.4, ups=1.63, wpb=10512.4, bsz=368.1, num_updates=14700, lr=1.80702e-05, gnorm=0.925, train_wall=61, wall=9747
2021-01-08 16:26:13 | INFO | train_inner | epoch 027:    214 / 561 symm_kl=0.397, self_kl=0, self_cv=0, loss=3.305, nll_loss=0.959, ppl=1.94, wps=17002.8, ups=1.61, wpb=10544.2, bsz=362.5, num_updates=14800, lr=1.8009e-05, gnorm=0.925, train_wall=62, wall=9809
2021-01-08 16:27:15 | INFO | train_inner | epoch 027:    314 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.295, nll_loss=0.957, ppl=1.94, wps=17014, ups=1.62, wpb=10494.3, bsz=379.4, num_updates=14900, lr=1.79485e-05, gnorm=0.907, train_wall=61, wall=9871
2021-01-08 16:28:17 | INFO | train_inner | epoch 027:    414 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.952, ppl=1.93, wps=17034, ups=1.61, wpb=10560.1, bsz=377.6, num_updates=15000, lr=1.78885e-05, gnorm=0.905, train_wall=62, wall=9933
2021-01-08 16:29:18 | INFO | train_inner | epoch 027:    514 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.301, nll_loss=0.959, ppl=1.94, wps=16975.1, ups=1.62, wpb=10447, bsz=358.8, num_updates=15100, lr=1.78292e-05, gnorm=0.924, train_wall=61, wall=9994
2021-01-08 16:29:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 16:29:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:29:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:29:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:29:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:29:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:29:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:29:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:29:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:29:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:29:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:29:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:29:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:29:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:29:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:29:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:29:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:29:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:29:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:29:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:29:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:29:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:29:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:29:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:29:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:29:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:29:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:29:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:29:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:29:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:29:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:30:08 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.274 | nll_loss 3.762 | ppl 13.57 | bleu 22.82 | wps 4625.7 | wpb 7508.5 | bsz 272.7 | num_updates 15147 | best_bleu 22.98
2021-01-08 16:30:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 16:30:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:30:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:30:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:30:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:30:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 27 @ 15147 updates, score 22.82) (writing took 3.058590540662408 seconds)
2021-01-08 16:30:11 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2021-01-08 16:30:11 | INFO | train | epoch 027 | symm_kl 0.394 | self_kl 0 | self_cv 0 | loss 3.302 | nll_loss 0.96 | ppl 1.94 | wps 15764.4 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 15147 | lr 1.78015e-05 | gnorm 0.92 | train_wall 345 | wall 10047
2021-01-08 16:30:11 | INFO | fairseq.trainer | begin training epoch 28
2021-01-08 16:30:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:30:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:30:46 | INFO | train_inner | epoch 028:     53 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.946, ppl=1.93, wps=11701.5, ups=1.14, wpb=10301.5, bsz=382.4, num_updates=15200, lr=1.77705e-05, gnorm=0.928, train_wall=61, wall=10082
2021-01-08 16:31:48 | INFO | train_inner | epoch 028:    153 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.304, nll_loss=0.96, ppl=1.94, wps=16883.5, ups=1.61, wpb=10482.5, bsz=354, num_updates=15300, lr=1.77123e-05, gnorm=0.923, train_wall=62, wall=10144
2021-01-08 16:32:50 | INFO | train_inner | epoch 028:    253 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.289, nll_loss=0.951, ppl=1.93, wps=16958, ups=1.61, wpb=10553.1, bsz=365.7, num_updates=15400, lr=1.76547e-05, gnorm=0.912, train_wall=62, wall=10207
2021-01-08 16:33:52 | INFO | train_inner | epoch 028:    353 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.3, nll_loss=0.955, ppl=1.94, wps=17023.2, ups=1.62, wpb=10526.2, bsz=364.1, num_updates=15500, lr=1.75977e-05, gnorm=0.91, train_wall=62, wall=10268
2021-01-08 16:34:54 | INFO | train_inner | epoch 028:    453 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.313, nll_loss=0.975, ppl=1.97, wps=16986.8, ups=1.63, wpb=10436.7, bsz=372.7, num_updates=15600, lr=1.75412e-05, gnorm=0.919, train_wall=61, wall=10330
2021-01-08 16:35:55 | INFO | train_inner | epoch 028:    553 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.312, nll_loss=0.971, ppl=1.96, wps=17119.8, ups=1.62, wpb=10548.9, bsz=382.4, num_updates=15700, lr=1.74852e-05, gnorm=0.915, train_wall=61, wall=10392
2021-01-08 16:36:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 16:36:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:36:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:36:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:36:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:36:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:36:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:36:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:36:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:36:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:36:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:36:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:36:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:36:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:36:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:36:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:36:23 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.267 | nll_loss 3.758 | ppl 13.53 | bleu 22.81 | wps 4084.1 | wpb 7508.5 | bsz 272.7 | num_updates 15708 | best_bleu 22.98
2021-01-08 16:36:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 16:36:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:36:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:36:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:36:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:36:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 28 @ 15708 updates, score 22.81) (writing took 3.130166430026293 seconds)
2021-01-08 16:36:26 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2021-01-08 16:36:26 | INFO | train | epoch 028 | symm_kl 0.393 | self_kl 0 | self_cv 0 | loss 3.298 | nll_loss 0.958 | ppl 1.94 | wps 15668.6 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 15708 | lr 1.74808e-05 | gnorm 0.916 | train_wall 345 | wall 10423
2021-01-08 16:36:26 | INFO | fairseq.trainer | begin training epoch 29
2021-01-08 16:36:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:36:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:37:25 | INFO | train_inner | epoch 029:     92 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.287, nll_loss=0.951, ppl=1.93, wps=11653.3, ups=1.11, wpb=10500.3, bsz=369.8, num_updates=15800, lr=1.74298e-05, gnorm=0.917, train_wall=61, wall=10482
2021-01-08 16:38:27 | INFO | train_inner | epoch 029:    192 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.292, nll_loss=0.955, ppl=1.94, wps=17128, ups=1.63, wpb=10535.7, bsz=369.3, num_updates=15900, lr=1.73749e-05, gnorm=0.921, train_wall=61, wall=10543
2021-01-08 16:39:29 | INFO | train_inner | epoch 029:    292 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.304, nll_loss=0.962, ppl=1.95, wps=17122.5, ups=1.62, wpb=10552.2, bsz=366, num_updates=16000, lr=1.73205e-05, gnorm=0.912, train_wall=61, wall=10605
2021-01-08 16:40:30 | INFO | train_inner | epoch 029:    392 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.286, nll_loss=0.954, ppl=1.94, wps=16923.7, ups=1.63, wpb=10396.4, bsz=373.3, num_updates=16100, lr=1.72666e-05, gnorm=0.924, train_wall=61, wall=10666
2021-01-08 16:41:32 | INFO | train_inner | epoch 029:    492 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.298, nll_loss=0.959, ppl=1.94, wps=16714, ups=1.61, wpb=10367.4, bsz=371.4, num_updates=16200, lr=1.72133e-05, gnorm=0.932, train_wall=62, wall=10728
2021-01-08 16:42:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 16:42:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:42:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:42:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:42:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:42:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:42:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:42:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:42:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:42:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:42:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:42:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:42:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:42:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:42:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:42:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:42:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:42:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:42:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:42:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:42:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:42:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:42:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:42:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:42:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:42:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:42:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:42:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:42:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:42:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:42:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:42:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:42:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:42:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:42:36 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.267 | nll_loss 3.76 | ppl 13.55 | bleu 22.7 | wps 4652.9 | wpb 7508.5 | bsz 272.7 | num_updates 16269 | best_bleu 22.98
2021-01-08 16:42:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 16:42:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:42:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:42:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:42:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:42:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 29 @ 16269 updates, score 22.7) (writing took 3.088254164904356 seconds)
2021-01-08 16:42:39 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2021-01-08 16:42:39 | INFO | train | epoch 029 | symm_kl 0.391 | self_kl 0 | self_cv 0 | loss 3.295 | nll_loss 0.958 | ppl 1.94 | wps 15789.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 16269 | lr 1.71767e-05 | gnorm 0.92 | train_wall 344 | wall 10795
2021-01-08 16:42:39 | INFO | fairseq.trainer | begin training epoch 30
2021-01-08 16:42:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:42:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:43:01 | INFO | train_inner | epoch 030:     31 / 561 symm_kl=0.39, self_kl=0, self_cv=0, loss=3.294, nll_loss=0.959, ppl=1.94, wps=11807, ups=1.13, wpb=10450.9, bsz=375.8, num_updates=16300, lr=1.71604e-05, gnorm=0.91, train_wall=61, wall=10817
2021-01-08 16:44:02 | INFO | train_inner | epoch 030:    131 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.952, ppl=1.93, wps=17241.2, ups=1.63, wpb=10585.6, bsz=366, num_updates=16400, lr=1.7108e-05, gnorm=0.909, train_wall=61, wall=10878
2021-01-08 16:45:03 | INFO | train_inner | epoch 030:    231 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.29, nll_loss=0.957, ppl=1.94, wps=16965.7, ups=1.63, wpb=10434.4, bsz=362.6, num_updates=16500, lr=1.70561e-05, gnorm=0.913, train_wall=61, wall=10940
2021-01-08 16:46:05 | INFO | train_inner | epoch 030:    331 / 561 symm_kl=0.395, self_kl=0, self_cv=0, loss=3.305, nll_loss=0.962, ppl=1.95, wps=16901.4, ups=1.63, wpb=10393.5, bsz=365.7, num_updates=16600, lr=1.70046e-05, gnorm=0.925, train_wall=61, wall=11001
2021-01-08 16:47:07 | INFO | train_inner | epoch 030:    431 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.953, ppl=1.94, wps=17168.6, ups=1.62, wpb=10602.1, bsz=380.2, num_updates=16700, lr=1.69536e-05, gnorm=0.895, train_wall=62, wall=11063
2021-01-08 16:48:08 | INFO | train_inner | epoch 030:    531 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.296, nll_loss=0.962, ppl=1.95, wps=17064, ups=1.62, wpb=10519.5, bsz=368.1, num_updates=16800, lr=1.69031e-05, gnorm=0.911, train_wall=61, wall=11125
2021-01-08 16:48:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 16:48:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:48:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:48:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:48:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:48:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:48:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:48:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:48:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:48:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:48:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:48:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:48:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:48:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:48:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:48:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:48:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:48:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:48:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:48:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:48:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:48:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:48:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:48:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:48:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:48:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:48:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:48:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:48:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:48:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:48:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:48:47 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.269 | nll_loss 3.76 | ppl 13.55 | bleu 22.85 | wps 4686.1 | wpb 7508.5 | bsz 272.7 | num_updates 16830 | best_bleu 22.98
2021-01-08 16:48:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 16:48:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:48:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:48:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:48:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:48:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 30 @ 16830 updates, score 22.85) (writing took 3.165429847314954 seconds)
2021-01-08 16:48:51 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2021-01-08 16:48:51 | INFO | train | epoch 030 | symm_kl 0.39 | self_kl 0 | self_cv 0 | loss 3.293 | nll_loss 0.957 | ppl 1.94 | wps 15817.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 16830 | lr 1.6888e-05 | gnorm 0.912 | train_wall 344 | wall 11167
2021-01-08 16:48:51 | INFO | fairseq.trainer | begin training epoch 31
2021-01-08 16:48:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:48:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:49:36 | INFO | train_inner | epoch 031:     70 / 561 symm_kl=0.393, self_kl=0, self_cv=0, loss=3.3, nll_loss=0.96, ppl=1.94, wps=11851.7, ups=1.14, wpb=10409.6, bsz=363.3, num_updates=16900, lr=1.6853e-05, gnorm=0.928, train_wall=61, wall=11212
2021-01-08 16:50:38 | INFO | train_inner | epoch 031:    170 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.281, nll_loss=0.951, ppl=1.93, wps=17088.9, ups=1.61, wpb=10584.5, bsz=379.7, num_updates=17000, lr=1.68034e-05, gnorm=0.89, train_wall=62, wall=11274
2021-01-08 16:51:40 | INFO | train_inner | epoch 031:    270 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.952, ppl=1.93, wps=16892.5, ups=1.61, wpb=10481.8, bsz=374.6, num_updates=17100, lr=1.67542e-05, gnorm=0.906, train_wall=62, wall=11336
2021-01-08 16:52:42 | INFO | train_inner | epoch 031:    370 / 561 symm_kl=0.394, self_kl=0, self_cv=0, loss=3.308, nll_loss=0.967, ppl=1.95, wps=16772.8, ups=1.62, wpb=10374.8, bsz=340, num_updates=17200, lr=1.67054e-05, gnorm=0.928, train_wall=62, wall=11398
2021-01-08 16:53:44 | INFO | train_inner | epoch 031:    470 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.279, nll_loss=0.95, ppl=1.93, wps=17066.8, ups=1.62, wpb=10527.8, bsz=384.8, num_updates=17300, lr=1.6657e-05, gnorm=0.912, train_wall=61, wall=11460
2021-01-08 16:54:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 16:54:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:54:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:54:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:54:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:54:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:54:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:54:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:54:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:54:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:54:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:54:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:54:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:54:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:54:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:54:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:54:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:54:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:54:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:54:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 16:54:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 16:54:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 16:55:01 | INFO | valid | epoch 031 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.268 | nll_loss 3.76 | ppl 13.55 | bleu 22.8 | wps 4687.9 | wpb 7508.5 | bsz 272.7 | num_updates 17391 | best_bleu 22.98
2021-01-08 16:55:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 16:55:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:55:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:55:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:55:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:55:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 31 @ 17391 updates, score 22.8) (writing took 3.0536402724683285 seconds)
2021-01-08 16:55:05 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2021-01-08 16:55:05 | INFO | train | epoch 031 | symm_kl 0.388 | self_kl 0 | self_cv 0 | loss 3.29 | nll_loss 0.956 | ppl 1.94 | wps 15726.7 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 17391 | lr 1.66134e-05 | gnorm 0.911 | train_wall 346 | wall 11541
2021-01-08 16:55:05 | INFO | fairseq.trainer | begin training epoch 32
2021-01-08 16:55:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 16:55:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 16:55:13 | INFO | train_inner | epoch 032:      9 / 561 symm_kl=0.388, self_kl=0, self_cv=0, loss=3.295, nll_loss=0.963, ppl=1.95, wps=11603.3, ups=1.12, wpb=10367.8, bsz=370, num_updates=17400, lr=1.66091e-05, gnorm=0.92, train_wall=62, wall=11549
2021-01-08 16:56:14 | INFO | train_inner | epoch 032:    109 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.294, nll_loss=0.957, ppl=1.94, wps=17099.8, ups=1.64, wpb=10453, bsz=358.9, num_updates=17500, lr=1.65616e-05, gnorm=0.912, train_wall=61, wall=11610
2021-01-08 16:57:17 | INFO | train_inner | epoch 032:    209 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.94, ppl=1.92, wps=17093.2, ups=1.6, wpb=10691.6, bsz=379.4, num_updates=17600, lr=1.65145e-05, gnorm=0.884, train_wall=62, wall=11673
2021-01-08 16:58:18 | INFO | train_inner | epoch 032:    309 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.951, ppl=1.93, wps=17167.2, ups=1.63, wpb=10541.6, bsz=365.9, num_updates=17700, lr=1.64677e-05, gnorm=0.908, train_wall=61, wall=11734
2021-01-08 16:59:20 | INFO | train_inner | epoch 032:    409 / 561 symm_kl=0.392, self_kl=0, self_cv=0, loss=3.301, nll_loss=0.962, ppl=1.95, wps=16958.4, ups=1.63, wpb=10411, bsz=347.3, num_updates=17800, lr=1.64214e-05, gnorm=0.926, train_wall=61, wall=11796
2021-01-08 17:00:21 | INFO | train_inner | epoch 032:    509 / 561 symm_kl=0.386, self_kl=0, self_cv=0, loss=3.288, nll_loss=0.958, ppl=1.94, wps=16844.7, ups=1.62, wpb=10412.4, bsz=390.5, num_updates=17900, lr=1.63755e-05, gnorm=0.908, train_wall=62, wall=11858
2021-01-08 17:00:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 17:00:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:00:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:00:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:00:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:00:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:00:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:00:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:00:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:00:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:00:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:00:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:00:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:00:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:00:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:00:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:00:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:00:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:00:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:00:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:00:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:00:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:00:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:00:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:00:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:01:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:01:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:01:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:01:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:01:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:01:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:01:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:01:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:01:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:01:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:01:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:01:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:01:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:01:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:01:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:01:15 | INFO | valid | epoch 032 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.269 | nll_loss 3.761 | ppl 13.56 | bleu 22.75 | wps 4590.7 | wpb 7508.5 | bsz 272.7 | num_updates 17952 | best_bleu 22.98
2021-01-08 17:01:15 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 17:01:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:01:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:01:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:01:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:01:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 32 @ 17952 updates, score 22.75) (writing took 3.1032405644655228 seconds)
2021-01-08 17:01:18 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2021-01-08 17:01:18 | INFO | train | epoch 032 | symm_kl 0.387 | self_kl 0 | self_cv 0 | loss 3.287 | nll_loss 0.955 | ppl 1.94 | wps 15762.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 17952 | lr 1.63517e-05 | gnorm 0.907 | train_wall 345 | wall 11914
2021-01-08 17:01:18 | INFO | fairseq.trainer | begin training epoch 33
2021-01-08 17:01:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:01:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:01:50 | INFO | train_inner | epoch 033:     48 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.291, nll_loss=0.964, ppl=1.95, wps=11858.3, ups=1.13, wpb=10480.2, bsz=382.2, num_updates=18000, lr=1.63299e-05, gnorm=0.894, train_wall=61, wall=11946
2021-01-08 17:02:52 | INFO | train_inner | epoch 033:    148 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.267, nll_loss=0.945, ppl=1.92, wps=16853.9, ups=1.61, wpb=10476.8, bsz=378.3, num_updates=18100, lr=1.62848e-05, gnorm=0.891, train_wall=62, wall=12008
2021-01-08 17:03:54 | INFO | train_inner | epoch 033:    248 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.951, ppl=1.93, wps=17004.8, ups=1.61, wpb=10542.1, bsz=374.1, num_updates=18200, lr=1.624e-05, gnorm=0.905, train_wall=62, wall=12070
2021-01-08 17:04:56 | INFO | train_inner | epoch 033:    348 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.29, nll_loss=0.958, ppl=1.94, wps=16861, ups=1.62, wpb=10437.8, bsz=375, num_updates=18300, lr=1.61955e-05, gnorm=0.912, train_wall=62, wall=12132
2021-01-08 17:05:58 | INFO | train_inner | epoch 033:    448 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.295, nll_loss=0.958, ppl=1.94, wps=16891.3, ups=1.61, wpb=10483.6, bsz=349.7, num_updates=18400, lr=1.61515e-05, gnorm=0.915, train_wall=62, wall=12194
2021-01-08 17:07:00 | INFO | train_inner | epoch 033:    548 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.959, ppl=1.94, wps=16974.1, ups=1.62, wpb=10480.7, bsz=370.4, num_updates=18500, lr=1.61077e-05, gnorm=0.901, train_wall=62, wall=12256
2021-01-08 17:07:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 17:07:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:07:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:07:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:07:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:07:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:07:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:07:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:07:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:07:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:07:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:07:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:07:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:07:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:07:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:07:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:07:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:07:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:07:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:07:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:07:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:07:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:07:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:07:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:07:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:07:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:07:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:07:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:07:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:07:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:07:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:07:28 | INFO | valid | epoch 033 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.265 | nll_loss 3.755 | ppl 13.51 | bleu 22.71 | wps 4692.9 | wpb 7508.5 | bsz 272.7 | num_updates 18513 | best_bleu 22.98
2021-01-08 17:07:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 17:07:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:07:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:07:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:07:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:07:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 33 @ 18513 updates, score 22.71) (writing took 3.082505164667964 seconds)
2021-01-08 17:07:31 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2021-01-08 17:07:31 | INFO | train | epoch 033 | symm_kl 0.385 | self_kl 0 | self_cv 0 | loss 3.284 | nll_loss 0.955 | ppl 1.94 | wps 15751 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 18513 | lr 1.61021e-05 | gnorm 0.905 | train_wall 346 | wall 12287
2021-01-08 17:07:31 | INFO | fairseq.trainer | begin training epoch 34
2021-01-08 17:07:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:07:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:08:27 | INFO | train_inner | epoch 034:     87 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.953, ppl=1.94, wps=11987.9, ups=1.14, wpb=10500.6, bsz=376.6, num_updates=18600, lr=1.60644e-05, gnorm=0.91, train_wall=61, wall=12343
2021-01-08 17:09:29 | INFO | train_inner | epoch 034:    187 / 561 symm_kl=0.391, self_kl=0, self_cv=0, loss=3.304, nll_loss=0.967, ppl=1.95, wps=17024.6, ups=1.63, wpb=10453.2, bsz=372.1, num_updates=18700, lr=1.60214e-05, gnorm=0.904, train_wall=61, wall=12405
2021-01-08 17:10:31 | INFO | train_inner | epoch 034:    287 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.954, ppl=1.94, wps=16860.8, ups=1.61, wpb=10473.3, bsz=368.2, num_updates=18800, lr=1.59787e-05, gnorm=0.902, train_wall=62, wall=12467
2021-01-08 17:11:33 | INFO | train_inner | epoch 034:    387 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.267, nll_loss=0.946, ppl=1.93, wps=16821.8, ups=1.62, wpb=10398.1, bsz=377.4, num_updates=18900, lr=1.59364e-05, gnorm=0.89, train_wall=62, wall=12529
2021-01-08 17:12:34 | INFO | train_inner | epoch 034:    487 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.933, ppl=1.91, wps=17258.7, ups=1.62, wpb=10664.6, bsz=372.5, num_updates=19000, lr=1.58944e-05, gnorm=0.883, train_wall=62, wall=12591
2021-01-08 17:13:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 17:13:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:13:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:13:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:13:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:13:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:13:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:13:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:13:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:13:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:13:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:13:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:13:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:13:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:13:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:13:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:13:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:13:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:13:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:13:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:13:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:13:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:13:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:13:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:13:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:13:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:13:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:13:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:13:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:13:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:13:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:13:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:13:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:13:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:13:41 | INFO | valid | epoch 034 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.264 | nll_loss 3.757 | ppl 13.52 | bleu 22.67 | wps 4579.2 | wpb 7508.5 | bsz 272.7 | num_updates 19074 | best_bleu 22.98
2021-01-08 17:13:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 17:13:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:13:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:13:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 34 @ 19074 updates, score 22.67) (writing took 3.2556658629328012 seconds)
2021-01-08 17:13:44 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2021-01-08 17:13:44 | INFO | train | epoch 034 | symm_kl 0.384 | self_kl 0 | self_cv 0 | loss 3.281 | nll_loss 0.954 | ppl 1.94 | wps 15758.1 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 19074 | lr 1.58635e-05 | gnorm 0.901 | train_wall 345 | wall 12660
2021-01-08 17:13:44 | INFO | fairseq.trainer | begin training epoch 35
2021-01-08 17:13:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:13:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:13:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:13:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:14:03 | INFO | train_inner | epoch 035:     26 / 561 symm_kl=0.387, self_kl=0, self_cv=0, loss=3.294, nll_loss=0.963, ppl=1.95, wps=11722.6, ups=1.13, wpb=10418.3, bsz=347, num_updates=19100, lr=1.58527e-05, gnorm=0.917, train_wall=61, wall=12679
2021-01-08 17:15:04 | INFO | train_inner | epoch 035:    126 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.948, ppl=1.93, wps=17099.7, ups=1.64, wpb=10451.9, bsz=365.3, num_updates=19200, lr=1.58114e-05, gnorm=0.897, train_wall=61, wall=12741
2021-01-08 17:16:06 | INFO | train_inner | epoch 035:    226 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.267, nll_loss=0.944, ppl=1.92, wps=17025.1, ups=1.62, wpb=10479.3, bsz=382.2, num_updates=19300, lr=1.57704e-05, gnorm=0.896, train_wall=61, wall=12802
2021-01-08 17:17:07 | INFO | train_inner | epoch 035:    326 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.956, ppl=1.94, wps=17148.7, ups=1.63, wpb=10548.4, bsz=368.7, num_updates=19400, lr=1.57297e-05, gnorm=0.896, train_wall=61, wall=12864
2021-01-08 17:18:09 | INFO | train_inner | epoch 035:    426 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.958, ppl=1.94, wps=16987.5, ups=1.61, wpb=10546.4, bsz=379.4, num_updates=19500, lr=1.56893e-05, gnorm=0.891, train_wall=62, wall=12926
2021-01-08 17:19:11 | INFO | train_inner | epoch 035:    526 / 561 symm_kl=0.389, self_kl=0, self_cv=0, loss=3.299, nll_loss=0.966, ppl=1.95, wps=16997.7, ups=1.63, wpb=10412.2, bsz=355, num_updates=19600, lr=1.56492e-05, gnorm=0.913, train_wall=61, wall=12987
2021-01-08 17:19:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 17:19:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:19:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:19:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:19:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:19:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:19:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:19:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:19:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:19:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:19:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:19:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:19:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:19:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:19:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:19:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:19:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:19:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:19:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:19:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:19:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:19:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:19:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:19:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:19:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:19:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:19:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:19:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:19:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:19:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:19:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:19:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:19:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:19:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:19:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:19:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:19:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:19:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:19:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:19:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:19:53 | INFO | valid | epoch 035 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.263 | nll_loss 3.756 | ppl 13.51 | bleu 22.72 | wps 4563.6 | wpb 7508.5 | bsz 272.7 | num_updates 19635 | best_bleu 22.98
2021-01-08 17:19:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 17:19:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:19:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:19:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:19:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:19:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 35 @ 19635 updates, score 22.72) (writing took 3.148638017475605 seconds)
2021-01-08 17:19:56 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2021-01-08 17:19:56 | INFO | train | epoch 035 | symm_kl 0.382 | self_kl 0 | self_cv 0 | loss 3.279 | nll_loss 0.954 | ppl 1.94 | wps 15806.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 19635 | lr 1.56353e-05 | gnorm 0.898 | train_wall 344 | wall 13033
2021-01-08 17:19:56 | INFO | fairseq.trainer | begin training epoch 36
2021-01-08 17:19:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:19:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:20:39 | INFO | train_inner | epoch 036:     65 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.951, ppl=1.93, wps=11725.8, ups=1.13, wpb=10344.6, bsz=362.5, num_updates=19700, lr=1.56094e-05, gnorm=0.903, train_wall=61, wall=13075
2021-01-08 17:21:40 | INFO | train_inner | epoch 036:    165 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.954, ppl=1.94, wps=16973.2, ups=1.63, wpb=10399, bsz=361.7, num_updates=19800, lr=1.557e-05, gnorm=0.908, train_wall=61, wall=13136
2021-01-08 17:22:42 | INFO | train_inner | epoch 036:    265 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.955, ppl=1.94, wps=17049, ups=1.62, wpb=10532.1, bsz=367.1, num_updates=19900, lr=1.55308e-05, gnorm=0.913, train_wall=62, wall=13198
2021-01-08 17:23:44 | INFO | train_inner | epoch 036:    365 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.951, ppl=1.93, wps=17137.7, ups=1.62, wpb=10583.1, bsz=386.2, num_updates=20000, lr=1.54919e-05, gnorm=0.882, train_wall=62, wall=13260
2021-01-08 17:24:45 | INFO | train_inner | epoch 036:    465 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.27, nll_loss=0.95, ppl=1.93, wps=17335.8, ups=1.62, wpb=10695.3, bsz=376, num_updates=20100, lr=1.54533e-05, gnorm=0.884, train_wall=62, wall=13322
2021-01-08 17:25:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 17:25:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:25:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:25:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:25:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:25:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:25:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:25:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:25:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:25:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:25:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:25:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:25:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:25:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:25:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:25:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:25:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:25:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:25:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:25:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:25:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:25:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:25:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:25:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:25:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:25:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:25:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:25:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:26:06 | INFO | valid | epoch 036 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.262 | nll_loss 3.756 | ppl 13.51 | bleu 22.75 | wps 4294.6 | wpb 7508.5 | bsz 272.7 | num_updates 20196 | best_bleu 22.98
2021-01-08 17:26:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 17:26:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:26:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:26:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:26:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:26:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 36 @ 20196 updates, score 22.75) (writing took 3.084133652970195 seconds)
2021-01-08 17:26:10 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2021-01-08 17:26:10 | INFO | train | epoch 036 | symm_kl 0.381 | self_kl 0 | self_cv 0 | loss 3.277 | nll_loss 0.953 | ppl 1.94 | wps 15759 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 20196 | lr 1.54166e-05 | gnorm 0.9 | train_wall 344 | wall 13406
2021-01-08 17:26:10 | INFO | fairseq.trainer | begin training epoch 37
2021-01-08 17:26:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:26:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:26:15 | INFO | train_inner | epoch 037:      4 / 561 symm_kl=0.382, self_kl=0, self_cv=0, loss=3.282, nll_loss=0.959, ppl=1.94, wps=11469.3, ups=1.12, wpb=10275, bsz=363.2, num_updates=20200, lr=1.5415e-05, gnorm=0.913, train_wall=61, wall=13411
2021-01-08 17:27:16 | INFO | train_inner | epoch 037:    104 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.278, nll_loss=0.952, ppl=1.93, wps=17214.3, ups=1.65, wpb=10439.9, bsz=356.5, num_updates=20300, lr=1.5377e-05, gnorm=0.904, train_wall=60, wall=13472
2021-01-08 17:28:18 | INFO | train_inner | epoch 037:    204 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.957, ppl=1.94, wps=16860.4, ups=1.61, wpb=10457.5, bsz=360.4, num_updates=20400, lr=1.53393e-05, gnorm=0.908, train_wall=62, wall=13534
2021-01-08 17:29:20 | INFO | train_inner | epoch 037:    304 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.947, ppl=1.93, wps=16982.8, ups=1.62, wpb=10499.2, bsz=371, num_updates=20500, lr=1.53018e-05, gnorm=0.893, train_wall=62, wall=13596
2021-01-08 17:30:21 | INFO | train_inner | epoch 037:    404 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.941, ppl=1.92, wps=17091.9, ups=1.62, wpb=10539.4, bsz=381.9, num_updates=20600, lr=1.52647e-05, gnorm=0.894, train_wall=61, wall=13657
2021-01-08 17:31:23 | INFO | train_inner | epoch 037:    504 / 561 symm_kl=0.385, self_kl=0, self_cv=0, loss=3.293, nll_loss=0.964, ppl=1.95, wps=17159.3, ups=1.63, wpb=10546.3, bsz=368.3, num_updates=20700, lr=1.52277e-05, gnorm=0.902, train_wall=61, wall=13719
2021-01-08 17:31:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 17:31:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:31:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:31:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:32:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:32:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:32:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:32:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:32:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:32:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:32:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:32:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:32:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:32:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:32:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:32:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:32:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:32:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:32:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:32:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:32:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:32:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:32:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:32:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:32:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:32:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:32:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:32:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:32:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:32:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:32:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:32:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:32:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:32:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:32:18 | INFO | valid | epoch 037 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.264 | nll_loss 3.754 | ppl 13.49 | bleu 22.71 | wps 4640.2 | wpb 7508.5 | bsz 272.7 | num_updates 20757 | best_bleu 22.98
2021-01-08 17:32:18 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 17:32:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:32:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:32:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:32:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:32:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 37 @ 20757 updates, score 22.71) (writing took 3.052403375506401 seconds)
2021-01-08 17:32:21 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2021-01-08 17:32:21 | INFO | train | epoch 037 | symm_kl 0.38 | self_kl 0 | self_cv 0 | loss 3.274 | nll_loss 0.952 | ppl 1.93 | wps 15816.3 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 20757 | lr 1.52068e-05 | gnorm 0.9 | train_wall 344 | wall 13778
2021-01-08 17:32:21 | INFO | fairseq.trainer | begin training epoch 38
2021-01-08 17:32:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:32:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:32:50 | INFO | train_inner | epoch 038:     43 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.269, nll_loss=0.951, ppl=1.93, wps=11757.7, ups=1.14, wpb=10305.5, bsz=371.7, num_updates=20800, lr=1.51911e-05, gnorm=0.904, train_wall=60, wall=13807
2021-01-08 17:33:52 | INFO | train_inner | epoch 038:    143 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.945, ppl=1.93, wps=16719.5, ups=1.61, wpb=10382.8, bsz=381.5, num_updates=20900, lr=1.51547e-05, gnorm=0.907, train_wall=62, wall=13869
2021-01-08 17:34:54 | INFO | train_inner | epoch 038:    243 / 561 symm_kl=0.383, self_kl=0, self_cv=0, loss=3.285, nll_loss=0.959, ppl=1.94, wps=17255.4, ups=1.63, wpb=10607.4, bsz=373.3, num_updates=21000, lr=1.51186e-05, gnorm=0.901, train_wall=61, wall=13930
2021-01-08 17:35:55 | INFO | train_inner | epoch 038:    343 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.948, ppl=1.93, wps=17110.9, ups=1.62, wpb=10531.3, bsz=375.9, num_updates=21100, lr=1.50827e-05, gnorm=0.889, train_wall=61, wall=13992
2021-01-08 17:36:57 | INFO | train_inner | epoch 038:    443 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.282, nll_loss=0.959, ppl=1.94, wps=17138.1, ups=1.62, wpb=10578.1, bsz=360.2, num_updates=21200, lr=1.50471e-05, gnorm=0.892, train_wall=62, wall=14053
2021-01-08 17:37:59 | INFO | train_inner | epoch 038:    543 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.943, ppl=1.92, wps=16931.9, ups=1.62, wpb=10474.3, bsz=365.5, num_updates=21300, lr=1.50117e-05, gnorm=0.897, train_wall=62, wall=14115
2021-01-08 17:38:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 17:38:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:38:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:38:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:38:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:38:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:38:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:38:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:38:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:38:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:38:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:38:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:38:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:38:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:38:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:38:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:38:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:38:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:38:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:38:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:38:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:38:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:38:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:38:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:38:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:38:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:38:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:38:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:38:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:38:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:38:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:38:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:38:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:38:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:38:31 | INFO | valid | epoch 038 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.26 | nll_loss 3.753 | ppl 13.48 | bleu 22.82 | wps 4658 | wpb 7508.5 | bsz 272.7 | num_updates 21318 | best_bleu 22.98
2021-01-08 17:38:31 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 17:38:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:38:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:38:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:38:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:38:34 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 38 @ 21318 updates, score 22.82) (writing took 3.0575069338083267 seconds)
2021-01-08 17:38:34 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2021-01-08 17:38:34 | INFO | train | epoch 038 | symm_kl 0.379 | self_kl 0 | self_cv 0 | loss 3.272 | nll_loss 0.951 | ppl 1.93 | wps 15784.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 21318 | lr 1.50054e-05 | gnorm 0.899 | train_wall 344 | wall 14150
2021-01-08 17:38:34 | INFO | fairseq.trainer | begin training epoch 39
2021-01-08 17:38:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:38:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:39:27 | INFO | train_inner | epoch 039:     82 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.947, ppl=1.93, wps=11812.1, ups=1.14, wpb=10369.8, bsz=363, num_updates=21400, lr=1.49766e-05, gnorm=0.898, train_wall=61, wall=14203
2021-01-08 17:40:28 | INFO | train_inner | epoch 039:    182 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.258, nll_loss=0.944, ppl=1.92, wps=17235.7, ups=1.62, wpb=10627.7, bsz=378.5, num_updates=21500, lr=1.49417e-05, gnorm=0.877, train_wall=61, wall=14265
2021-01-08 17:41:30 | INFO | train_inner | epoch 039:    282 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.944, ppl=1.92, wps=16959.4, ups=1.62, wpb=10491.2, bsz=374.5, num_updates=21600, lr=1.49071e-05, gnorm=0.892, train_wall=62, wall=14327
2021-01-08 17:42:32 | INFO | train_inner | epoch 039:    382 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.959, ppl=1.94, wps=17171.3, ups=1.62, wpb=10585.4, bsz=374.2, num_updates=21700, lr=1.48727e-05, gnorm=0.881, train_wall=61, wall=14388
2021-01-08 17:43:34 | INFO | train_inner | epoch 039:    482 / 561 symm_kl=0.379, self_kl=0, self_cv=0, loss=3.281, nll_loss=0.96, ppl=1.95, wps=16783.4, ups=1.62, wpb=10386.4, bsz=370.4, num_updates=21800, lr=1.48386e-05, gnorm=0.899, train_wall=62, wall=14450
2021-01-08 17:44:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 17:44:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:44:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:44:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:44:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:44:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:44:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:44:43 | INFO | valid | epoch 039 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.26 | nll_loss 3.754 | ppl 13.49 | bleu 22.8 | wps 4610.1 | wpb 7508.5 | bsz 272.7 | num_updates 21879 | best_bleu 22.98
2021-01-08 17:44:43 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 17:44:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:44:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:44:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 39 @ 21879 updates, score 22.8) (writing took 3.067896019667387 seconds)
2021-01-08 17:44:46 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2021-01-08 17:44:46 | INFO | train | epoch 039 | symm_kl 0.378 | self_kl 0 | self_cv 0 | loss 3.27 | nll_loss 0.951 | ppl 1.93 | wps 15795.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 21879 | lr 1.48118e-05 | gnorm 0.892 | train_wall 344 | wall 14523
2021-01-08 17:44:46 | INFO | fairseq.trainer | begin training epoch 40
2021-01-08 17:44:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:44:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:44:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:45:02 | INFO | train_inner | epoch 040:     21 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.952, ppl=1.93, wps=11773.1, ups=1.13, wpb=10399.8, bsz=356.1, num_updates=21900, lr=1.48047e-05, gnorm=0.906, train_wall=61, wall=14538
2021-01-08 17:46:03 | INFO | train_inner | epoch 040:    121 / 561 symm_kl=0.384, self_kl=0, self_cv=0, loss=3.284, nll_loss=0.955, ppl=1.94, wps=16969.2, ups=1.63, wpb=10379.6, bsz=338.5, num_updates=22000, lr=1.4771e-05, gnorm=0.904, train_wall=61, wall=14600
2021-01-08 17:47:05 | INFO | train_inner | epoch 040:    221 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.949, ppl=1.93, wps=16966, ups=1.62, wpb=10463.9, bsz=383.7, num_updates=22100, lr=1.47375e-05, gnorm=0.886, train_wall=61, wall=14661
2021-01-08 17:48:07 | INFO | train_inner | epoch 040:    321 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.948, ppl=1.93, wps=17209.6, ups=1.62, wpb=10604.3, bsz=372.6, num_updates=22200, lr=1.47043e-05, gnorm=0.891, train_wall=61, wall=14723
2021-01-08 17:49:08 | INFO | train_inner | epoch 040:    421 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.947, ppl=1.93, wps=17134.9, ups=1.63, wpb=10543, bsz=368, num_updates=22300, lr=1.46713e-05, gnorm=0.886, train_wall=61, wall=14784
2021-01-08 17:50:10 | INFO | train_inner | epoch 040:    521 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.275, nll_loss=0.958, ppl=1.94, wps=16940.9, ups=1.61, wpb=10508.8, bsz=389.1, num_updates=22400, lr=1.46385e-05, gnorm=0.89, train_wall=62, wall=14846
2021-01-08 17:50:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 17:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:50:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:50:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:50:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:50:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:50:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:50:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:50:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:50:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:50:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:50:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:50:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:50:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:50:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:50:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:50:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:50:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:50:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:50:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:50:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:50:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:50:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:50:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:50:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:50:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:50:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:50:58 | INFO | valid | epoch 040 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.26 | nll_loss 3.752 | ppl 13.47 | bleu 22.83 | wps 4143.7 | wpb 7508.5 | bsz 272.7 | num_updates 22440 | best_bleu 22.98
2021-01-08 17:50:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 17:50:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:50:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:51:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 40 @ 22440 updates, score 22.83) (writing took 3.0191843174397945 seconds)
2021-01-08 17:51:01 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2021-01-08 17:51:01 | INFO | train | epoch 040 | symm_kl 0.377 | self_kl 0 | self_cv 0 | loss 3.268 | nll_loss 0.951 | ppl 1.93 | wps 15702.5 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 22440 | lr 1.46254e-05 | gnorm 0.892 | train_wall 344 | wall 14897
2021-01-08 17:51:01 | INFO | fairseq.trainer | begin training epoch 41
2021-01-08 17:51:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:51:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:51:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:51:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:51:40 | INFO | train_inner | epoch 041:     60 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.948, ppl=1.93, wps=11537.2, ups=1.11, wpb=10385.6, bsz=373, num_updates=22500, lr=1.46059e-05, gnorm=0.889, train_wall=61, wall=14936
2021-01-08 17:52:42 | INFO | train_inner | epoch 041:    160 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.269, nll_loss=0.95, ppl=1.93, wps=16970.7, ups=1.62, wpb=10494.1, bsz=345.8, num_updates=22600, lr=1.45736e-05, gnorm=0.899, train_wall=62, wall=14998
2021-01-08 17:53:44 | INFO | train_inner | epoch 041:    260 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.933, ppl=1.91, wps=17103.7, ups=1.62, wpb=10557.1, bsz=380.2, num_updates=22700, lr=1.45414e-05, gnorm=0.878, train_wall=62, wall=15060
2021-01-08 17:54:45 | INFO | train_inner | epoch 041:    360 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.958, ppl=1.94, wps=17115.8, ups=1.64, wpb=10438.5, bsz=394.9, num_updates=22800, lr=1.45095e-05, gnorm=0.884, train_wall=61, wall=15121
2021-01-08 17:55:46 | INFO | train_inner | epoch 041:    460 / 561 symm_kl=0.381, self_kl=0, self_cv=0, loss=3.283, nll_loss=0.96, ppl=1.95, wps=17001.7, ups=1.63, wpb=10456.4, bsz=370.5, num_updates=22900, lr=1.44778e-05, gnorm=0.901, train_wall=61, wall=15183
2021-01-08 17:56:48 | INFO | train_inner | epoch 041:    560 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.947, ppl=1.93, wps=17054.9, ups=1.62, wpb=10540.6, bsz=353.5, num_updates=23000, lr=1.44463e-05, gnorm=0.892, train_wall=62, wall=15244
2021-01-08 17:56:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 17:56:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:56:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:56:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:56:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:56:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:56:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:56:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:56:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:56:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:56:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:56:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:56:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:56:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:56:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:56:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:56:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:56:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:56:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:56:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:56:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:56:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:56:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:56:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:56:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:56:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:56:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:56:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:56:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:56:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:56:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:56:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:56:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:56:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:56:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:56:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:56:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:56:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 17:56:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 17:56:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 17:57:09 | INFO | valid | epoch 041 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.26 | nll_loss 3.752 | ppl 13.47 | bleu 22.73 | wps 4657.6 | wpb 7508.5 | bsz 272.7 | num_updates 23001 | best_bleu 22.98
2021-01-08 17:57:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 17:57:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:57:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:57:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:57:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:57:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 41 @ 23001 updates, score 22.73) (writing took 3.103354288265109 seconds)
2021-01-08 17:57:12 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2021-01-08 17:57:12 | INFO | train | epoch 041 | symm_kl 0.376 | self_kl 0 | self_cv 0 | loss 3.266 | nll_loss 0.95 | ppl 1.93 | wps 15826.9 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 23001 | lr 1.4446e-05 | gnorm 0.89 | train_wall 344 | wall 15269
2021-01-08 17:57:12 | INFO | fairseq.trainer | begin training epoch 42
2021-01-08 17:57:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 17:57:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 17:58:16 | INFO | train_inner | epoch 042:     99 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.944, ppl=1.92, wps=11952.8, ups=1.14, wpb=10470.5, bsz=353.7, num_updates=23100, lr=1.4415e-05, gnorm=0.889, train_wall=60, wall=15332
2021-01-08 17:59:17 | INFO | train_inner | epoch 042:    199 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.952, ppl=1.93, wps=16919.7, ups=1.63, wpb=10395.3, bsz=364.5, num_updates=23200, lr=1.43839e-05, gnorm=0.899, train_wall=61, wall=15393
2021-01-08 18:00:19 | INFO | train_inner | epoch 042:    299 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.942, ppl=1.92, wps=17114.9, ups=1.63, wpb=10517.8, bsz=379.2, num_updates=23300, lr=1.4353e-05, gnorm=0.886, train_wall=61, wall=15455
2021-01-08 18:01:20 | INFO | train_inner | epoch 042:    399 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.95, ppl=1.93, wps=17342.3, ups=1.63, wpb=10619.2, bsz=381.1, num_updates=23400, lr=1.43223e-05, gnorm=0.881, train_wall=61, wall=15516
2021-01-08 18:02:22 | INFO | train_inner | epoch 042:    499 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.947, ppl=1.93, wps=16914.1, ups=1.62, wpb=10432.1, bsz=368, num_updates=23500, lr=1.42918e-05, gnorm=0.895, train_wall=61, wall=15578
2021-01-08 18:03:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 18:03:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:03:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:03:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:03:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:03:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:03:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:03:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:03:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:03:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:03:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:03:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:03:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:03:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:03:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:03:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:03:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:03:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:03:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:03:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:03:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:03:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:03:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:03:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:03:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:03:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:03:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:03:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:03:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:03:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:03:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:03:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:03:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:03:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:03:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:03:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:03:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:03:21 | INFO | valid | epoch 042 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.255 | nll_loss 3.749 | ppl 13.45 | bleu 22.78 | wps 4563.2 | wpb 7508.5 | bsz 272.7 | num_updates 23562 | best_bleu 22.98
2021-01-08 18:03:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 18:03:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:03:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:03:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:03:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:03:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 42 @ 23562 updates, score 22.78) (writing took 3.077817151322961 seconds)
2021-01-08 18:03:24 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2021-01-08 18:03:24 | INFO | train | epoch 042 | symm_kl 0.374 | self_kl 0 | self_cv 0 | loss 3.264 | nll_loss 0.949 | ppl 1.93 | wps 15842.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 23562 | lr 1.4273e-05 | gnorm 0.89 | train_wall 343 | wall 15640
2021-01-08 18:03:24 | INFO | fairseq.trainer | begin training epoch 43
2021-01-08 18:03:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:03:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:03:50 | INFO | train_inner | epoch 043:     38 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.957, ppl=1.94, wps=11707.3, ups=1.13, wpb=10325.9, bsz=362.3, num_updates=23600, lr=1.42615e-05, gnorm=0.893, train_wall=61, wall=15666
2021-01-08 18:04:51 | INFO | train_inner | epoch 043:    138 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.943, ppl=1.92, wps=17164, ups=1.63, wpb=10524.1, bsz=376.6, num_updates=23700, lr=1.42314e-05, gnorm=0.884, train_wall=61, wall=15727
2021-01-08 18:05:52 | INFO | train_inner | epoch 043:    238 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.951, ppl=1.93, wps=17023.6, ups=1.63, wpb=10450.7, bsz=352.5, num_updates=23800, lr=1.42014e-05, gnorm=0.896, train_wall=61, wall=15789
2021-01-08 18:06:54 | INFO | train_inner | epoch 043:    338 / 561 symm_kl=0.376, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.956, ppl=1.94, wps=17161.8, ups=1.62, wpb=10589.2, bsz=368.9, num_updates=23900, lr=1.41717e-05, gnorm=0.89, train_wall=62, wall=15850
2021-01-08 18:07:56 | INFO | train_inner | epoch 043:    438 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.953, ppl=1.94, wps=16739.9, ups=1.61, wpb=10396.9, bsz=384.3, num_updates=24000, lr=1.41421e-05, gnorm=0.886, train_wall=62, wall=15912
2021-01-08 18:08:58 | INFO | train_inner | epoch 043:    538 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.947, ppl=1.93, wps=17101.5, ups=1.62, wpb=10570.1, bsz=376.1, num_updates=24100, lr=1.41128e-05, gnorm=0.882, train_wall=62, wall=15974
2021-01-08 18:09:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 18:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:09:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:09:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:09:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:09:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:09:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:09:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:09:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:09:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:09:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:09:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:09:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:09:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:09:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:09:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:09:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:09:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:09:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:09:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:09:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:09:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:09:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:09:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:09:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:09:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:09:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:09:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:09:33 | INFO | valid | epoch 043 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.257 | nll_loss 3.747 | ppl 13.43 | bleu 22.86 | wps 4591.8 | wpb 7508.5 | bsz 272.7 | num_updates 24123 | best_bleu 22.98
2021-01-08 18:09:33 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 18:09:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:09:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:09:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:09:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:09:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 43 @ 24123 updates, score 22.86) (writing took 3.1024147402495146 seconds)
2021-01-08 18:09:36 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2021-01-08 18:09:36 | INFO | train | epoch 043 | symm_kl 0.374 | self_kl 0 | self_cv 0 | loss 3.262 | nll_loss 0.949 | ppl 1.93 | wps 15792.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 24123 | lr 1.4106e-05 | gnorm 0.889 | train_wall 344 | wall 16012
2021-01-08 18:09:36 | INFO | fairseq.trainer | begin training epoch 44
2021-01-08 18:09:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:09:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:10:26 | INFO | train_inner | epoch 044:     77 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.947, ppl=1.93, wps=11893.3, ups=1.14, wpb=10441.4, bsz=372.2, num_updates=24200, lr=1.40836e-05, gnorm=0.909, train_wall=60, wall=16062
2021-01-08 18:11:27 | INFO | train_inner | epoch 044:    177 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.928, ppl=1.9, wps=17054.3, ups=1.63, wpb=10486.7, bsz=363.9, num_updates=24300, lr=1.40546e-05, gnorm=0.88, train_wall=61, wall=16124
2021-01-08 18:12:29 | INFO | train_inner | epoch 044:    277 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.925, ppl=1.9, wps=16984, ups=1.62, wpb=10509.5, bsz=373.7, num_updates=24400, lr=1.40257e-05, gnorm=0.874, train_wall=62, wall=16185
2021-01-08 18:13:31 | INFO | train_inner | epoch 044:    377 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.951, ppl=1.93, wps=17089, ups=1.63, wpb=10504.8, bsz=386.4, num_updates=24500, lr=1.39971e-05, gnorm=0.881, train_wall=61, wall=16247
2021-01-08 18:14:32 | INFO | train_inner | epoch 044:    477 / 561 symm_kl=0.38, self_kl=0, self_cv=0, loss=3.294, nll_loss=0.975, ppl=1.97, wps=17075.1, ups=1.64, wpb=10427.2, bsz=360.4, num_updates=24600, lr=1.39686e-05, gnorm=0.902, train_wall=61, wall=16308
2021-01-08 18:15:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 18:15:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:15:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:15:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:15:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:15:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:15:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:15:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:15:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:15:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:15:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:15:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:15:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:15:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:15:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:15:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:15:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:15:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:15:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:15:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:15:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:15:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:15:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:15:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:15:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:15:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:15:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:15:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:15:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:15:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:15:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:15:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:15:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:15:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:15:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:15:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:15:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:15:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:15:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:15:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:15:44 | INFO | valid | epoch 044 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.258 | nll_loss 3.748 | ppl 13.44 | bleu 22.98 | wps 4617.6 | wpb 7508.5 | bsz 272.7 | num_updates 24684 | best_bleu 22.98
2021-01-08 18:15:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 18:15:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:15:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:15:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:15:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:15:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 44 @ 24684 updates, score 22.98) (writing took 5.153719978407025 seconds)
2021-01-08 18:15:49 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2021-01-08 18:15:49 | INFO | train | epoch 044 | symm_kl 0.373 | self_kl 0 | self_cv 0 | loss 3.26 | nll_loss 0.948 | ppl 1.93 | wps 15753.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 24684 | lr 1.39448e-05 | gnorm 0.888 | train_wall 343 | wall 16386
2021-01-08 18:15:49 | INFO | fairseq.trainer | begin training epoch 45
2021-01-08 18:15:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:15:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:16:02 | INFO | train_inner | epoch 045:     16 / 561 symm_kl=0.378, self_kl=0, self_cv=0, loss=3.279, nll_loss=0.961, ppl=1.95, wps=11595.9, ups=1.11, wpb=10489.5, bsz=351.2, num_updates=24700, lr=1.39403e-05, gnorm=0.893, train_wall=61, wall=16398
2021-01-08 18:17:03 | INFO | train_inner | epoch 045:    116 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.944, ppl=1.92, wps=17197.2, ups=1.65, wpb=10419.3, bsz=373.3, num_updates=24800, lr=1.39122e-05, gnorm=0.868, train_wall=60, wall=16459
2021-01-08 18:18:04 | INFO | train_inner | epoch 045:    216 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.945, ppl=1.93, wps=17132.7, ups=1.63, wpb=10490.5, bsz=385.9, num_updates=24900, lr=1.38842e-05, gnorm=0.88, train_wall=61, wall=16520
2021-01-08 18:19:06 | INFO | train_inner | epoch 045:    316 / 561 symm_kl=0.377, self_kl=0, self_cv=0, loss=3.28, nll_loss=0.963, ppl=1.95, wps=16958.8, ups=1.62, wpb=10437, bsz=359.9, num_updates=25000, lr=1.38564e-05, gnorm=0.908, train_wall=61, wall=16582
2021-01-08 18:20:07 | INFO | train_inner | epoch 045:    416 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.935, ppl=1.91, wps=17265.9, ups=1.64, wpb=10557.3, bsz=367.8, num_updates=25100, lr=1.38288e-05, gnorm=0.885, train_wall=61, wall=16643
2021-01-08 18:21:08 | INFO | train_inner | epoch 045:    516 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.946, ppl=1.93, wps=17211.8, ups=1.63, wpb=10577.4, bsz=369.7, num_updates=25200, lr=1.38013e-05, gnorm=0.88, train_wall=61, wall=16704
2021-01-08 18:21:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 18:21:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:21:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:21:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:21:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:21:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:21:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:21:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:21:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:21:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:21:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:21:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:21:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:21:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:21:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:21:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:21:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:21:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:21:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:21:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:21:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:21:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:21:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:21:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:21:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:21:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:21:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:21:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:21:57 | INFO | valid | epoch 045 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.259 | nll_loss 3.751 | ppl 13.46 | bleu 22.8 | wps 4578.4 | wpb 7508.5 | bsz 272.7 | num_updates 25245 | best_bleu 22.98
2021-01-08 18:21:57 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 18:21:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:21:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:21:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:21:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:22:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 45 @ 25245 updates, score 22.8) (writing took 3.151372279971838 seconds)
2021-01-08 18:22:00 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2021-01-08 18:22:00 | INFO | train | epoch 045 | symm_kl 0.372 | self_kl 0 | self_cv 0 | loss 3.258 | nll_loss 0.947 | ppl 1.93 | wps 15878.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 25245 | lr 1.3789e-05 | gnorm 0.886 | train_wall 342 | wall 16756
2021-01-08 18:22:00 | INFO | fairseq.trainer | begin training epoch 46
2021-01-08 18:22:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:22:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:22:36 | INFO | train_inner | epoch 046:     55 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.94, ppl=1.92, wps=11933.7, ups=1.14, wpb=10483.7, bsz=360.6, num_updates=25300, lr=1.3774e-05, gnorm=0.894, train_wall=60, wall=16792
2021-01-08 18:23:37 | INFO | train_inner | epoch 046:    155 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.262, nll_loss=0.95, ppl=1.93, wps=16791.1, ups=1.63, wpb=10304.7, bsz=379.3, num_updates=25400, lr=1.37469e-05, gnorm=0.893, train_wall=61, wall=16854
2021-01-08 18:24:39 | INFO | train_inner | epoch 046:    255 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.941, ppl=1.92, wps=17164.2, ups=1.62, wpb=10582.9, bsz=378.7, num_updates=25500, lr=1.37199e-05, gnorm=0.875, train_wall=61, wall=16915
2021-01-08 18:25:41 | INFO | train_inner | epoch 046:    355 / 561 symm_kl=0.373, self_kl=0, self_cv=0, loss=3.261, nll_loss=0.948, ppl=1.93, wps=17194.8, ups=1.62, wpb=10612.9, bsz=365.6, num_updates=25600, lr=1.36931e-05, gnorm=0.881, train_wall=62, wall=16977
2021-01-08 18:26:42 | INFO | train_inner | epoch 046:    455 / 561 symm_kl=0.371, self_kl=0, self_cv=0, loss=3.257, nll_loss=0.946, ppl=1.93, wps=17117.1, ups=1.63, wpb=10503, bsz=362.3, num_updates=25700, lr=1.36664e-05, gnorm=0.893, train_wall=61, wall=17038
2021-01-08 18:27:44 | INFO | train_inner | epoch 046:    555 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.954, ppl=1.94, wps=16796.2, ups=1.62, wpb=10397.9, bsz=371.5, num_updates=25800, lr=1.36399e-05, gnorm=0.886, train_wall=62, wall=17100
2021-01-08 18:27:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 18:27:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:27:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:27:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:27:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:27:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:27:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:27:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:27:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:27:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:27:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:27:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:27:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:27:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:27:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:27:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:27:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:27:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:27:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:27:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:27:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:27:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:27:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:27:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:27:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:27:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:27:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:27:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:27:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:27:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:27:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:27:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:27:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:27:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:27:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:27:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:27:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:27:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:27:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:27:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:28:09 | INFO | valid | epoch 046 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.258 | nll_loss 3.749 | ppl 13.45 | bleu 22.86 | wps 4626.8 | wpb 7508.5 | bsz 272.7 | num_updates 25806 | best_bleu 22.98
2021-01-08 18:28:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 18:28:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:28:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:28:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:28:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:28:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 46 @ 25806 updates, score 22.86) (writing took 3.1015409138053656 seconds)
2021-01-08 18:28:12 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2021-01-08 18:28:12 | INFO | train | epoch 046 | symm_kl 0.371 | self_kl 0 | self_cv 0 | loss 3.257 | nll_loss 0.947 | ppl 1.93 | wps 15816.4 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 25806 | lr 1.36383e-05 | gnorm 0.885 | train_wall 344 | wall 17128
2021-01-08 18:28:12 | INFO | fairseq.trainer | begin training epoch 47
2021-01-08 18:28:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:28:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:29:12 | INFO | train_inner | epoch 047:     94 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.252, nll_loss=0.943, ppl=1.92, wps=12051.5, ups=1.14, wpb=10585.2, bsz=362.2, num_updates=25900, lr=1.36135e-05, gnorm=0.878, train_wall=61, wall=17188
2021-01-08 18:30:14 | INFO | train_inner | epoch 047:    194 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.948, ppl=1.93, wps=16933.8, ups=1.62, wpb=10442.5, bsz=350.3, num_updates=26000, lr=1.35873e-05, gnorm=0.896, train_wall=61, wall=17250
2021-01-08 18:31:15 | INFO | train_inner | epoch 047:    294 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.943, ppl=1.92, wps=17095.1, ups=1.63, wpb=10503.2, bsz=374.6, num_updates=26100, lr=1.35613e-05, gnorm=0.88, train_wall=61, wall=17311
2021-01-08 18:32:17 | INFO | train_inner | epoch 047:    394 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.946, ppl=1.93, wps=16962.2, ups=1.62, wpb=10454.3, bsz=371.8, num_updates=26200, lr=1.35354e-05, gnorm=0.88, train_wall=61, wall=17373
2021-01-08 18:33:18 | INFO | train_inner | epoch 047:    494 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.944, ppl=1.92, wps=16947.9, ups=1.62, wpb=10484.2, bsz=388.4, num_updates=26300, lr=1.35096e-05, gnorm=0.874, train_wall=62, wall=17435
2021-01-08 18:34:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 18:34:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:34:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:34:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:34:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:34:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:34:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:34:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:34:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:34:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:34:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:34:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:34:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:34:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:34:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:34:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:34:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:34:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:34:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:34:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:34:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:34:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:34:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:34:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:34:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:34:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:34:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:34:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:34:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:34:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:34:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:34:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:34:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:34:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:34:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:34:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:34:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:34:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:34:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:34:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:34:21 | INFO | valid | epoch 047 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.258 | nll_loss 3.752 | ppl 13.47 | bleu 22.72 | wps 4542.7 | wpb 7508.5 | bsz 272.7 | num_updates 26367 | best_bleu 22.98
2021-01-08 18:34:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 18:34:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:34:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:34:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:34:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:34:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 47 @ 26367 updates, score 22.72) (writing took 3.123385639861226 seconds)
2021-01-08 18:34:24 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2021-01-08 18:34:24 | INFO | train | epoch 047 | symm_kl 0.37 | self_kl 0 | self_cv 0 | loss 3.254 | nll_loss 0.946 | ppl 1.93 | wps 15786.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 26367 | lr 1.34924e-05 | gnorm 0.885 | train_wall 344 | wall 17500
2021-01-08 18:34:24 | INFO | fairseq.trainer | begin training epoch 48
2021-01-08 18:34:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:34:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:34:47 | INFO | train_inner | epoch 048:     33 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.252, nll_loss=0.945, ppl=1.93, wps=11788.2, ups=1.13, wpb=10450.7, bsz=374.3, num_updates=26400, lr=1.3484e-05, gnorm=0.894, train_wall=61, wall=17523
2021-01-08 18:35:48 | INFO | train_inner | epoch 048:    133 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.266, nll_loss=0.952, ppl=1.94, wps=16987.7, ups=1.63, wpb=10402.3, bsz=369.9, num_updates=26500, lr=1.34585e-05, gnorm=0.883, train_wall=61, wall=17585
2021-01-08 18:36:50 | INFO | train_inner | epoch 048:    233 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.949, ppl=1.93, wps=17030.7, ups=1.62, wpb=10517.5, bsz=360.6, num_updates=26600, lr=1.34332e-05, gnorm=0.878, train_wall=62, wall=17646
2021-01-08 18:37:51 | INFO | train_inner | epoch 048:    333 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.264, nll_loss=0.953, ppl=1.94, wps=17046.4, ups=1.63, wpb=10463.7, bsz=354.2, num_updates=26700, lr=1.3408e-05, gnorm=0.885, train_wall=61, wall=17708
2021-01-08 18:38:53 | INFO | train_inner | epoch 048:    433 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.939, ppl=1.92, wps=17096.9, ups=1.62, wpb=10537.4, bsz=385, num_updates=26800, lr=1.3383e-05, gnorm=0.867, train_wall=61, wall=17769
2021-01-08 18:39:55 | INFO | train_inner | epoch 048:    533 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.944, ppl=1.92, wps=16966.3, ups=1.62, wpb=10468.4, bsz=375.1, num_updates=26900, lr=1.33581e-05, gnorm=0.883, train_wall=62, wall=17831
2021-01-08 18:40:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 18:40:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:40:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:40:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:40:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:40:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:40:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:40:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:40:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:40:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:40:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:40:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:40:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:40:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:40:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:40:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:40:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:40:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:40:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:40:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:40:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:40:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:40:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:40:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:40:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:40:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:40:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:40:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:40:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:40:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:40:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:40:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:40:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:40:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:40:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:40:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:40:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:40:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:40:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:40:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:40:35 | INFO | valid | epoch 048 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.255 | nll_loss 3.749 | ppl 13.44 | bleu 22.72 | wps 4070.6 | wpb 7508.5 | bsz 272.7 | num_updates 26928 | best_bleu 22.98
2021-01-08 18:40:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 18:40:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:40:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:40:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:40:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:40:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 48 @ 26928 updates, score 22.72) (writing took 3.0433873925358057 seconds)
2021-01-08 18:40:38 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2021-01-08 18:40:38 | INFO | train | epoch 048 | symm_kl 0.369 | self_kl 0 | self_cv 0 | loss 3.253 | nll_loss 0.946 | ppl 1.93 | wps 15726.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 26928 | lr 1.33511e-05 | gnorm 0.877 | train_wall 344 | wall 17874
2021-01-08 18:40:38 | INFO | fairseq.trainer | begin training epoch 49
2021-01-08 18:40:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:40:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:41:25 | INFO | train_inner | epoch 049:     72 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.937, ppl=1.91, wps=11617.9, ups=1.11, wpb=10466.8, bsz=374.3, num_updates=27000, lr=1.33333e-05, gnorm=0.869, train_wall=61, wall=17921
2021-01-08 18:42:26 | INFO | train_inner | epoch 049:    172 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.946, ppl=1.93, wps=16903.4, ups=1.62, wpb=10408.2, bsz=375.4, num_updates=27100, lr=1.33087e-05, gnorm=0.89, train_wall=61, wall=17983
2021-01-08 18:43:28 | INFO | train_inner | epoch 049:    272 / 561 symm_kl=0.375, self_kl=0, self_cv=0, loss=3.271, nll_loss=0.956, ppl=1.94, wps=17019.2, ups=1.62, wpb=10517.2, bsz=356.2, num_updates=27200, lr=1.32842e-05, gnorm=0.887, train_wall=62, wall=18045
2021-01-08 18:44:30 | INFO | train_inner | epoch 049:    372 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.949, ppl=1.93, wps=16931.9, ups=1.63, wpb=10381.6, bsz=372.5, num_updates=27300, lr=1.32599e-05, gnorm=0.873, train_wall=61, wall=18106
2021-01-08 18:45:31 | INFO | train_inner | epoch 049:    472 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.938, ppl=1.92, wps=17146.9, ups=1.63, wpb=10545.9, bsz=360.4, num_updates=27400, lr=1.32357e-05, gnorm=0.872, train_wall=61, wall=18167
2021-01-08 18:46:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 18:46:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:46:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:46:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:46:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:46:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:46:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:46:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:46:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:46:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:46:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:46:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:46:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:46:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:46:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:46:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:46:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:46:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:46:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:46:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:46:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:46:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:46:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:46:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:46:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:46:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:46:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:46:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:46:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:46:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:46:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:46:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:46:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:46:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:46:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:46:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:46:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:46:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:46:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:46:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:46:46 | INFO | valid | epoch 049 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.257 | nll_loss 3.747 | ppl 13.43 | bleu 22.84 | wps 4643.3 | wpb 7508.5 | bsz 272.7 | num_updates 27489 | best_bleu 22.98
2021-01-08 18:46:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 18:46:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:46:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:46:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:46:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:46:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 49 @ 27489 updates, score 22.84) (writing took 3.16880657710135 seconds)
2021-01-08 18:46:50 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2021-01-08 18:46:50 | INFO | train | epoch 049 | symm_kl 0.368 | self_kl 0 | self_cv 0 | loss 3.252 | nll_loss 0.946 | ppl 1.93 | wps 15832.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 27489 | lr 1.32142e-05 | gnorm 0.877 | train_wall 343 | wall 18246
2021-01-08 18:46:50 | INFO | fairseq.trainer | begin training epoch 50
2021-01-08 18:46:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:46:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:47:00 | INFO | train_inner | epoch 050:     11 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.945, ppl=1.92, wps=11880.9, ups=1.13, wpb=10504.5, bsz=380.2, num_updates=27500, lr=1.32116e-05, gnorm=0.872, train_wall=61, wall=18256
2021-01-08 18:48:00 | INFO | train_inner | epoch 050:    111 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.947, ppl=1.93, wps=17300.9, ups=1.64, wpb=10546.8, bsz=356.6, num_updates=27600, lr=1.31876e-05, gnorm=0.883, train_wall=61, wall=18317
2021-01-08 18:49:02 | INFO | train_inner | epoch 050:    211 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.938, ppl=1.92, wps=16945.2, ups=1.62, wpb=10440.7, bsz=360.7, num_updates=27700, lr=1.31638e-05, gnorm=0.88, train_wall=61, wall=18378
2021-01-08 18:50:03 | INFO | train_inner | epoch 050:    311 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.249, nll_loss=0.944, ppl=1.92, wps=17182.3, ups=1.63, wpb=10539.4, bsz=374.1, num_updates=27800, lr=1.31401e-05, gnorm=0.872, train_wall=61, wall=18440
2021-01-08 18:51:05 | INFO | train_inner | epoch 050:    411 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.941, ppl=1.92, wps=17055.4, ups=1.63, wpb=10476, bsz=379.4, num_updates=27900, lr=1.31165e-05, gnorm=0.878, train_wall=61, wall=18501
2021-01-08 18:52:07 | INFO | train_inner | epoch 050:    511 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.947, ppl=1.93, wps=17005.3, ups=1.62, wpb=10527.3, bsz=380.4, num_updates=28000, lr=1.30931e-05, gnorm=0.875, train_wall=62, wall=18563
2021-01-08 18:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 18:52:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:52:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:52:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:52:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:52:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:52:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:52:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:52:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:52:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:52:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:52:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:52:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:52:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:52:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:52:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:52:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:52:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:52:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:52:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:52:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:52:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:52:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:52:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:52:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:52:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:52:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:52:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:52:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:52:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:52:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:52:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:52:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:52:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:52:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:52:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:52:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:52:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:52:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:52:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:53:00 | INFO | valid | epoch 050 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.254 | nll_loss 3.748 | ppl 13.43 | bleu 22.86 | wps 4312.6 | wpb 7508.5 | bsz 272.7 | num_updates 28050 | best_bleu 22.98
2021-01-08 18:53:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 18:53:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:53:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:53:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:53:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:53:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 50 @ 28050 updates, score 22.86) (writing took 3.16976435482502 seconds)
2021-01-08 18:53:03 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2021-01-08 18:53:03 | INFO | train | epoch 050 | symm_kl 0.368 | self_kl 0 | self_cv 0 | loss 3.25 | nll_loss 0.945 | ppl 1.93 | wps 15745.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 28050 | lr 1.30814e-05 | gnorm 0.88 | train_wall 344 | wall 18619
2021-01-08 18:53:03 | INFO | fairseq.trainer | begin training epoch 51
2021-01-08 18:53:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:53:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:53:36 | INFO | train_inner | epoch 051:     50 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.272, nll_loss=0.96, ppl=1.94, wps=11590.4, ups=1.12, wpb=10389.2, bsz=352.9, num_updates=28100, lr=1.30698e-05, gnorm=0.899, train_wall=60, wall=18653
2021-01-08 18:54:38 | INFO | train_inner | epoch 051:    150 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.938, ppl=1.92, wps=17074.9, ups=1.62, wpb=10509.5, bsz=378.9, num_updates=28200, lr=1.30466e-05, gnorm=0.873, train_wall=61, wall=18714
2021-01-08 18:55:39 | INFO | train_inner | epoch 051:    250 / 561 symm_kl=0.37, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.956, ppl=1.94, wps=17043.2, ups=1.63, wpb=10455.1, bsz=374, num_updates=28300, lr=1.30235e-05, gnorm=0.878, train_wall=61, wall=18776
2021-01-08 18:56:41 | INFO | train_inner | epoch 051:    350 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.947, ppl=1.93, wps=16944.5, ups=1.62, wpb=10446.7, bsz=371.6, num_updates=28400, lr=1.30005e-05, gnorm=0.883, train_wall=61, wall=18837
2021-01-08 18:57:43 | INFO | train_inner | epoch 051:    450 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.934, ppl=1.91, wps=17024.1, ups=1.62, wpb=10520.1, bsz=360.2, num_updates=28500, lr=1.29777e-05, gnorm=0.873, train_wall=62, wall=18899
2021-01-08 18:58:44 | INFO | train_inner | epoch 051:    550 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.94, ppl=1.92, wps=17102.3, ups=1.62, wpb=10528.9, bsz=374.8, num_updates=28600, lr=1.2955e-05, gnorm=0.874, train_wall=61, wall=18961
2021-01-08 18:58:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 18:58:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:58:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:58:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:58:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:58:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:58:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:58:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:58:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:58:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:58:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:58:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:58:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:58:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:58:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:58:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:58:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:58:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:58:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:58:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:58:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:58:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:58:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:58:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:58:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:58:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:58:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:58:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:58:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:58:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:58:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:58:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:58:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:58:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:58:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:58:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:58:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:59:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 18:59:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 18:59:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 18:59:12 | INFO | valid | epoch 051 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.251 | nll_loss 3.745 | ppl 13.41 | bleu 22.89 | wps 4602.5 | wpb 7508.5 | bsz 272.7 | num_updates 28611 | best_bleu 22.98
2021-01-08 18:59:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 18:59:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:59:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:59:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:59:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 18:59:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 51 @ 28611 updates, score 22.89) (writing took 3.0748408269137144 seconds)
2021-01-08 18:59:15 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2021-01-08 18:59:15 | INFO | train | epoch 051 | symm_kl 0.367 | self_kl 0 | self_cv 0 | loss 3.248 | nll_loss 0.944 | ppl 1.92 | wps 15822.2 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 28611 | lr 1.29525e-05 | gnorm 0.878 | train_wall 343 | wall 18991
2021-01-08 18:59:15 | INFO | fairseq.trainer | begin training epoch 52
2021-01-08 18:59:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 18:59:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:00:12 | INFO | train_inner | epoch 052:     89 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.939, ppl=1.92, wps=11992.9, ups=1.14, wpb=10531.1, bsz=358.2, num_updates=28700, lr=1.29324e-05, gnorm=0.873, train_wall=61, wall=19048
2021-01-08 19:01:14 | INFO | train_inner | epoch 052:    189 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.252, nll_loss=0.946, ppl=1.93, wps=17061.6, ups=1.62, wpb=10525.3, bsz=379.4, num_updates=28800, lr=1.29099e-05, gnorm=0.874, train_wall=62, wall=19110
2021-01-08 19:02:15 | INFO | train_inner | epoch 052:    289 / 561 symm_kl=0.374, self_kl=0, self_cv=0, loss=3.274, nll_loss=0.962, ppl=1.95, wps=17041.9, ups=1.63, wpb=10459.8, bsz=346.9, num_updates=28900, lr=1.28876e-05, gnorm=0.889, train_wall=61, wall=19171
2021-01-08 19:03:17 | INFO | train_inner | epoch 052:    389 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.218, nll_loss=0.923, ppl=1.9, wps=17036, ups=1.62, wpb=10505.1, bsz=372.6, num_updates=29000, lr=1.28654e-05, gnorm=0.867, train_wall=61, wall=19233
2021-01-08 19:04:18 | INFO | train_inner | epoch 052:    489 / 561 symm_kl=0.372, self_kl=0, self_cv=0, loss=3.265, nll_loss=0.954, ppl=1.94, wps=16768.8, ups=1.62, wpb=10334.4, bsz=367.1, num_updates=29100, lr=1.28432e-05, gnorm=0.896, train_wall=61, wall=19295
2021-01-08 19:05:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 19:05:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:05:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:05:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:05:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:05:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:05:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:05:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:05:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:05:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:05:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:05:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:05:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:05:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:05:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:05:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:05:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:05:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:05:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:05:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:05:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:05:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:05:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:05:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:05:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:05:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:05:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:05:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:05:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:05:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:05:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:05:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:05:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:05:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:05:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:05:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:05:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:05:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:05:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:05:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:05:24 | INFO | valid | epoch 052 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.252 | nll_loss 3.745 | ppl 13.41 | bleu 22.87 | wps 4563 | wpb 7508.5 | bsz 272.7 | num_updates 29172 | best_bleu 22.98
2021-01-08 19:05:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 19:05:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:05:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:05:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:05:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:05:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 52 @ 29172 updates, score 22.87) (writing took 3.0884218271821737 seconds)
2021-01-08 19:05:27 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2021-01-08 19:05:27 | INFO | train | epoch 052 | symm_kl 0.366 | self_kl 0 | self_cv 0 | loss 3.247 | nll_loss 0.944 | ppl 1.92 | wps 15809.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 29172 | lr 1.28274e-05 | gnorm 0.877 | train_wall 344 | wall 19363
2021-01-08 19:05:27 | INFO | fairseq.trainer | begin training epoch 53
2021-01-08 19:05:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:05:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:05:47 | INFO | train_inner | epoch 053:     28 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.218, nll_loss=0.93, ppl=1.9, wps=11978.9, ups=1.13, wpb=10565.3, bsz=394.1, num_updates=29200, lr=1.28212e-05, gnorm=0.859, train_wall=61, wall=19383
2021-01-08 19:06:48 | INFO | train_inner | epoch 053:    128 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.946, ppl=1.93, wps=17272.6, ups=1.64, wpb=10549.4, bsz=362.6, num_updates=29300, lr=1.27993e-05, gnorm=0.879, train_wall=61, wall=19444
2021-01-08 19:07:49 | INFO | train_inner | epoch 053:    228 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.253, nll_loss=0.945, ppl=1.93, wps=16904.5, ups=1.62, wpb=10404.1, bsz=355.6, num_updates=29400, lr=1.27775e-05, gnorm=0.882, train_wall=61, wall=19506
2021-01-08 19:08:51 | INFO | train_inner | epoch 053:    328 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.23, nll_loss=0.934, ppl=1.91, wps=17131.7, ups=1.62, wpb=10550, bsz=379.8, num_updates=29500, lr=1.27559e-05, gnorm=0.867, train_wall=61, wall=19567
2021-01-08 19:09:52 | INFO | train_inner | epoch 053:    428 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.935, ppl=1.91, wps=17031.9, ups=1.62, wpb=10490.5, bsz=387.4, num_updates=29600, lr=1.27343e-05, gnorm=0.871, train_wall=61, wall=19629
2021-01-08 19:10:54 | INFO | train_inner | epoch 053:    528 / 561 symm_kl=0.369, self_kl=0, self_cv=0, loss=3.263, nll_loss=0.957, ppl=1.94, wps=16933.8, ups=1.62, wpb=10444.8, bsz=366.7, num_updates=29700, lr=1.27128e-05, gnorm=0.887, train_wall=61, wall=19690
2021-01-08 19:11:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 19:11:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:11:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:11:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:11:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:11:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:11:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:11:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:11:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:11:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:11:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:11:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:11:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:11:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:11:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:11:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:11:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:11:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:11:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:11:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:11:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:11:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:11:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:11:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:11:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:11:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:11:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:11:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:11:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:11:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:11:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:11:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:11:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:11:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:11:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:11:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:11:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:11:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:11:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:11:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:11:36 | INFO | valid | epoch 053 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.252 | nll_loss 3.745 | ppl 13.41 | bleu 22.78 | wps 4486.5 | wpb 7508.5 | bsz 272.7 | num_updates 29733 | best_bleu 22.98
2021-01-08 19:11:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 19:11:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:11:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:11:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:11:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:11:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 53 @ 29733 updates, score 22.78) (writing took 3.122253969311714 seconds)
2021-01-08 19:11:39 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2021-01-08 19:11:39 | INFO | train | epoch 053 | symm_kl 0.365 | self_kl 0 | self_cv 0 | loss 3.246 | nll_loss 0.944 | ppl 1.92 | wps 15802.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 29733 | lr 1.27058e-05 | gnorm 0.877 | train_wall 343 | wall 19735
2021-01-08 19:11:39 | INFO | fairseq.trainer | begin training epoch 54
2021-01-08 19:11:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:11:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:12:22 | INFO | train_inner | epoch 054:     67 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.252, nll_loss=0.946, ppl=1.93, wps=11828.9, ups=1.13, wpb=10441.9, bsz=359.6, num_updates=29800, lr=1.26915e-05, gnorm=0.879, train_wall=61, wall=19779
2021-01-08 19:13:24 | INFO | train_inner | epoch 054:    167 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.94, ppl=1.92, wps=17119.3, ups=1.63, wpb=10526.1, bsz=368.2, num_updates=29900, lr=1.26702e-05, gnorm=0.87, train_wall=61, wall=19840
2021-01-08 19:14:25 | INFO | train_inner | epoch 054:    267 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.252, nll_loss=0.95, ppl=1.93, wps=17140.9, ups=1.63, wpb=10491.1, bsz=372.2, num_updates=30000, lr=1.26491e-05, gnorm=0.876, train_wall=61, wall=19901
2021-01-08 19:15:27 | INFO | train_inner | epoch 054:    367 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.935, ppl=1.91, wps=17179.4, ups=1.62, wpb=10584.5, bsz=376.3, num_updates=30100, lr=1.26281e-05, gnorm=0.862, train_wall=61, wall=19963
2021-01-08 19:16:28 | INFO | train_inner | epoch 054:    467 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.25, nll_loss=0.947, ppl=1.93, wps=17011.1, ups=1.62, wpb=10493.2, bsz=368.6, num_updates=30200, lr=1.26072e-05, gnorm=0.882, train_wall=62, wall=20025
2021-01-08 19:17:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 19:17:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:17:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:17:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:17:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:17:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:17:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:17:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:17:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:17:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:17:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:17:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:17:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:17:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:17:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:17:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:17:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:17:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:17:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:17:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:17:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:17:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:17:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:17:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:17:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:17:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:17:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:17:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:17:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:17:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:17:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:17:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:17:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:17:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:17:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:17:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:17:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:17:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:17:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:17:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:17:46 | INFO | valid | epoch 054 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.252 | nll_loss 3.744 | ppl 13.4 | bleu 22.89 | wps 4759.6 | wpb 7508.5 | bsz 272.7 | num_updates 30294 | best_bleu 22.98
2021-01-08 19:17:46 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 19:17:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:17:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:17:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:17:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:17:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 54 @ 30294 updates, score 22.89) (writing took 3.043275384232402 seconds)
2021-01-08 19:17:49 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2021-01-08 19:17:49 | INFO | train | epoch 054 | symm_kl 0.364 | self_kl 0 | self_cv 0 | loss 3.243 | nll_loss 0.942 | ppl 1.92 | wps 15879.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 30294 | lr 1.25876e-05 | gnorm 0.876 | train_wall 343 | wall 20106
2021-01-08 19:17:49 | INFO | fairseq.trainer | begin training epoch 55
2021-01-08 19:17:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:17:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:17:56 | INFO | train_inner | epoch 055:      6 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.943, ppl=1.92, wps=11750.8, ups=1.14, wpb=10303.8, bsz=364.4, num_updates=30300, lr=1.25863e-05, gnorm=0.891, train_wall=61, wall=20112
2021-01-08 19:18:57 | INFO | train_inner | epoch 055:    106 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.255, nll_loss=0.95, ppl=1.93, wps=17217.2, ups=1.64, wpb=10511.4, bsz=376.5, num_updates=30400, lr=1.25656e-05, gnorm=0.87, train_wall=61, wall=20173
2021-01-08 19:19:59 | INFO | train_inner | epoch 055:    206 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.939, ppl=1.92, wps=17030, ups=1.62, wpb=10500.2, bsz=383.4, num_updates=30500, lr=1.2545e-05, gnorm=0.862, train_wall=61, wall=20235
2021-01-08 19:21:00 | INFO | train_inner | epoch 055:    306 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.94, ppl=1.92, wps=17056.1, ups=1.62, wpb=10505.5, bsz=358.1, num_updates=30600, lr=1.25245e-05, gnorm=0.883, train_wall=61, wall=20297
2021-01-08 19:22:02 | INFO | train_inner | epoch 055:    406 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.939, ppl=1.92, wps=17247.6, ups=1.63, wpb=10591.9, bsz=368.2, num_updates=30700, lr=1.25041e-05, gnorm=0.862, train_wall=61, wall=20358
2021-01-08 19:23:03 | INFO | train_inner | epoch 055:    506 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.243, nll_loss=0.943, ppl=1.92, wps=16912.3, ups=1.63, wpb=10394.2, bsz=368.7, num_updates=30800, lr=1.24838e-05, gnorm=0.882, train_wall=61, wall=20420
2021-01-08 19:23:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 19:23:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:23:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:23:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:23:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:23:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:23:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:23:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:23:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:23:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:23:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:23:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:23:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:23:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:23:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:23:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:23:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:23:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:23:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:23:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:23:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:23:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:23:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:23:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:23:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:23:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:23:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:23:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:23:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:23:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:23:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:23:58 | INFO | valid | epoch 055 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.251 | nll_loss 3.745 | ppl 13.41 | bleu 22.84 | wps 4651.5 | wpb 7508.5 | bsz 272.7 | num_updates 30855 | best_bleu 22.98
2021-01-08 19:23:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 19:23:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:23:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:24:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:24:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:24:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 55 @ 30855 updates, score 22.84) (writing took 3.0444861743599176 seconds)
2021-01-08 19:24:01 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2021-01-08 19:24:01 | INFO | train | epoch 055 | symm_kl 0.364 | self_kl 0 | self_cv 0 | loss 3.242 | nll_loss 0.942 | ppl 1.92 | wps 15835 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 30855 | lr 1.24726e-05 | gnorm 0.875 | train_wall 343 | wall 20477
2021-01-08 19:24:01 | INFO | fairseq.trainer | begin training epoch 56
2021-01-08 19:24:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:24:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:24:31 | INFO | train_inner | epoch 056:     45 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.947, ppl=1.93, wps=11805.2, ups=1.14, wpb=10331.1, bsz=356.5, num_updates=30900, lr=1.24635e-05, gnorm=0.894, train_wall=60, wall=20507
2021-01-08 19:25:32 | INFO | train_inner | epoch 056:    145 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.239, nll_loss=0.943, ppl=1.92, wps=17199.9, ups=1.62, wpb=10597, bsz=377, num_updates=31000, lr=1.24434e-05, gnorm=0.857, train_wall=61, wall=20569
2021-01-08 19:26:34 | INFO | train_inner | epoch 056:    245 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.221, nll_loss=0.933, ppl=1.91, wps=17174.8, ups=1.62, wpb=10582.9, bsz=408.2, num_updates=31100, lr=1.24234e-05, gnorm=0.852, train_wall=61, wall=20630
2021-01-08 19:27:35 | INFO | train_inner | epoch 056:    345 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.942, ppl=1.92, wps=16992.3, ups=1.63, wpb=10400.7, bsz=344.2, num_updates=31200, lr=1.24035e-05, gnorm=0.89, train_wall=61, wall=20691
2021-01-08 19:28:37 | INFO | train_inner | epoch 056:    445 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.94, ppl=1.92, wps=16762.3, ups=1.61, wpb=10398.5, bsz=371.8, num_updates=31300, lr=1.23836e-05, gnorm=0.886, train_wall=62, wall=20754
2021-01-08 19:29:39 | INFO | train_inner | epoch 056:    545 / 561 symm_kl=0.365, self_kl=0, self_cv=0, loss=3.248, nll_loss=0.948, ppl=1.93, wps=17238.5, ups=1.63, wpb=10556.1, bsz=363.9, num_updates=31400, lr=1.23639e-05, gnorm=0.869, train_wall=61, wall=20815
2021-01-08 19:29:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 19:29:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:29:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:29:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:29:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:29:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:29:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:29:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:29:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:29:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:29:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:29:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:29:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:29:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:29:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:29:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:29:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:29:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:29:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:29:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:29:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:29:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:29:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:29:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:29:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:29:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:29:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:29:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:29:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:29:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:29:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:30:09 | INFO | valid | epoch 056 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.25 | nll_loss 3.744 | ppl 13.4 | bleu 22.81 | wps 4589.4 | wpb 7508.5 | bsz 272.7 | num_updates 31416 | best_bleu 22.98
2021-01-08 19:30:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 19:30:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:30:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:30:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:30:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:30:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 56 @ 31416 updates, score 22.81) (writing took 3.0296945590525866 seconds)
2021-01-08 19:30:12 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2021-01-08 19:30:12 | INFO | train | epoch 056 | symm_kl 0.363 | self_kl 0 | self_cv 0 | loss 3.241 | nll_loss 0.942 | ppl 1.92 | wps 15832.5 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 31416 | lr 1.23608e-05 | gnorm 0.872 | train_wall 343 | wall 20848
2021-01-08 19:30:12 | INFO | fairseq.trainer | begin training epoch 57
2021-01-08 19:30:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:30:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:31:07 | INFO | train_inner | epoch 057:     84 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.944, ppl=1.92, wps=11834.1, ups=1.14, wpb=10411.4, bsz=373.3, num_updates=31500, lr=1.23443e-05, gnorm=0.867, train_wall=61, wall=20903
2021-01-08 19:32:08 | INFO | train_inner | epoch 057:    184 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.245, nll_loss=0.949, ppl=1.93, wps=17015.9, ups=1.62, wpb=10476.3, bsz=375.3, num_updates=31600, lr=1.23247e-05, gnorm=0.875, train_wall=61, wall=20964
2021-01-08 19:33:10 | INFO | train_inner | epoch 057:    284 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.244, nll_loss=0.939, ppl=1.92, wps=16921, ups=1.62, wpb=10418.2, bsz=348.9, num_updates=31700, lr=1.23053e-05, gnorm=0.882, train_wall=61, wall=21026
2021-01-08 19:34:11 | INFO | train_inner | epoch 057:    384 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.94, ppl=1.92, wps=17044, ups=1.62, wpb=10518.8, bsz=377.6, num_updates=31800, lr=1.22859e-05, gnorm=0.862, train_wall=62, wall=21088
2021-01-08 19:35:13 | INFO | train_inner | epoch 057:    484 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.24, nll_loss=0.943, ppl=1.92, wps=17075.9, ups=1.61, wpb=10597.3, bsz=371.9, num_updates=31900, lr=1.22666e-05, gnorm=0.863, train_wall=62, wall=21150
2021-01-08 19:36:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 19:36:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:36:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:36:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:36:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:36:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:36:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:36:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:36:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:36:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:36:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:36:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:36:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:36:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:36:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:36:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:36:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:36:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:36:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:36:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:36:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:36:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:36:24 | INFO | valid | epoch 057 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.248 | nll_loss 3.743 | ppl 13.39 | bleu 22.86 | wps 4160.6 | wpb 7508.5 | bsz 272.7 | num_updates 31977 | best_bleu 22.98
2021-01-08 19:36:24 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 19:36:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:36:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:36:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:36:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:36:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 57 @ 31977 updates, score 22.86) (writing took 3.4428767319768667 seconds)
2021-01-08 19:36:27 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2021-01-08 19:36:27 | INFO | train | epoch 057 | symm_kl 0.363 | self_kl 0 | self_cv 0 | loss 3.24 | nll_loss 0.942 | ppl 1.92 | wps 15689.9 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 31977 | lr 1.22519e-05 | gnorm 0.87 | train_wall 344 | wall 21223
2021-01-08 19:36:27 | INFO | fairseq.trainer | begin training epoch 58
2021-01-08 19:36:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:36:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:36:44 | INFO | train_inner | epoch 058:     23 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.936, ppl=1.91, wps=11540.7, ups=1.1, wpb=10448.7, bsz=359.8, num_updates=32000, lr=1.22474e-05, gnorm=0.876, train_wall=61, wall=21240
2021-01-08 19:37:45 | INFO | train_inner | epoch 058:    123 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.22, nll_loss=0.929, ppl=1.9, wps=17246.4, ups=1.63, wpb=10578.5, bsz=386.2, num_updates=32100, lr=1.22284e-05, gnorm=0.854, train_wall=61, wall=21302
2021-01-08 19:38:47 | INFO | train_inner | epoch 058:    223 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.259, nll_loss=0.955, ppl=1.94, wps=16905.2, ups=1.63, wpb=10363.2, bsz=372.5, num_updates=32200, lr=1.22094e-05, gnorm=0.895, train_wall=61, wall=21363
2021-01-08 19:39:49 | INFO | train_inner | epoch 058:    323 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.233, nll_loss=0.935, ppl=1.91, wps=17042.1, ups=1.61, wpb=10573, bsz=359.3, num_updates=32300, lr=1.21904e-05, gnorm=0.869, train_wall=62, wall=21425
2021-01-08 19:40:50 | INFO | train_inner | epoch 058:    423 / 561 symm_kl=0.367, self_kl=0, self_cv=0, loss=3.254, nll_loss=0.949, ppl=1.93, wps=17000.8, ups=1.63, wpb=10422.2, bsz=360.6, num_updates=32400, lr=1.21716e-05, gnorm=0.895, train_wall=61, wall=21486
2021-01-08 19:41:51 | INFO | train_inner | epoch 058:    523 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.946, ppl=1.93, wps=17070.6, ups=1.63, wpb=10446.5, bsz=361.1, num_updates=32500, lr=1.21529e-05, gnorm=0.87, train_wall=61, wall=21547
2021-01-08 19:42:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 19:42:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:42:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:42:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:42:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:42:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:42:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:42:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:42:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:42:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:42:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:42:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:42:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:42:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:42:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:42:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:42:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:42:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:42:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:42:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:42:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:42:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:42:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:42:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:42:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:42:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:42:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:42:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:42:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:42:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:42:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:42:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:42:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:42:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:42:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:42:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:42:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:42:35 | INFO | valid | epoch 058 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.249 | nll_loss 3.743 | ppl 13.39 | bleu 22.78 | wps 4670.7 | wpb 7508.5 | bsz 272.7 | num_updates 32538 | best_bleu 22.98
2021-01-08 19:42:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 19:42:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:42:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:42:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:42:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:42:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 58 @ 32538 updates, score 22.78) (writing took 3.075181847438216 seconds)
2021-01-08 19:42:38 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2021-01-08 19:42:38 | INFO | train | epoch 058 | symm_kl 0.362 | self_kl 0 | self_cv 0 | loss 3.239 | nll_loss 0.941 | ppl 1.92 | wps 15834.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 32538 | lr 1.21458e-05 | gnorm 0.875 | train_wall 343 | wall 21595
2021-01-08 19:42:38 | INFO | fairseq.trainer | begin training epoch 59
2021-01-08 19:42:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:42:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:43:19 | INFO | train_inner | epoch 059:     62 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.943, ppl=1.92, wps=11929.2, ups=1.14, wpb=10453.1, bsz=379.3, num_updates=32600, lr=1.21342e-05, gnorm=0.868, train_wall=61, wall=21635
2021-01-08 19:44:20 | INFO | train_inner | epoch 059:    162 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.932, ppl=1.91, wps=17128.2, ups=1.63, wpb=10539.2, bsz=364.2, num_updates=32700, lr=1.21157e-05, gnorm=0.879, train_wall=61, wall=21697
2021-01-08 19:45:22 | INFO | train_inner | epoch 059:    262 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.228, nll_loss=0.937, ppl=1.91, wps=16792.2, ups=1.61, wpb=10439.9, bsz=364.1, num_updates=32800, lr=1.20972e-05, gnorm=0.87, train_wall=62, wall=21759
2021-01-08 19:46:24 | INFO | train_inner | epoch 059:    362 / 561 symm_kl=0.368, self_kl=0, self_cv=0, loss=3.256, nll_loss=0.95, ppl=1.93, wps=16929.9, ups=1.63, wpb=10395.9, bsz=351, num_updates=32900, lr=1.20788e-05, gnorm=0.896, train_wall=61, wall=21820
2021-01-08 19:47:26 | INFO | train_inner | epoch 059:    462 / 561 symm_kl=0.353, self_kl=0, self_cv=0, loss=3.213, nll_loss=0.927, ppl=1.9, wps=17223.3, ups=1.62, wpb=10653.2, bsz=400.8, num_updates=33000, lr=1.20605e-05, gnorm=0.845, train_wall=62, wall=21882
2021-01-08 19:48:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 19:48:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:48:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:48:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:48:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:48:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:48:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:48:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:48:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:48:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:48:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:48:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:48:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:48:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:48:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:48:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:48:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:48:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:48:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:48:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:48:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:48:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:48:48 | INFO | valid | epoch 059 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.249 | nll_loss 3.741 | ppl 13.37 | bleu 22.97 | wps 4663 | wpb 7508.5 | bsz 272.7 | num_updates 33099 | best_bleu 22.98
2021-01-08 19:48:48 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 19:48:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:48:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:48:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:48:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:48:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 59 @ 33099 updates, score 22.97) (writing took 3.0332078970968723 seconds)
2021-01-08 19:48:51 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2021-01-08 19:48:51 | INFO | train | epoch 059 | symm_kl 0.361 | self_kl 0 | self_cv 0 | loss 3.237 | nll_loss 0.941 | ppl 1.92 | wps 15795.8 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 33099 | lr 1.20424e-05 | gnorm 0.874 | train_wall 344 | wall 21967
2021-01-08 19:48:51 | INFO | fairseq.trainer | begin training epoch 60
2021-01-08 19:48:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:48:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:48:55 | INFO | train_inner | epoch 060:      1 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.952, ppl=1.93, wps=11722.2, ups=1.12, wpb=10421, bsz=369.7, num_updates=33100, lr=1.20422e-05, gnorm=0.879, train_wall=62, wall=21971
2021-01-08 19:49:55 | INFO | train_inner | epoch 060:    101 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.936, ppl=1.91, wps=17474.2, ups=1.66, wpb=10558, bsz=375.6, num_updates=33200, lr=1.20241e-05, gnorm=0.864, train_wall=60, wall=22031
2021-01-08 19:50:56 | INFO | train_inner | epoch 060:    201 / 561 symm_kl=0.366, self_kl=0, self_cv=0, loss=3.251, nll_loss=0.948, ppl=1.93, wps=16953.8, ups=1.63, wpb=10401.3, bsz=359.1, num_updates=33300, lr=1.2006e-05, gnorm=0.875, train_wall=61, wall=22093
2021-01-08 19:51:59 | INFO | train_inner | epoch 060:    301 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.235, nll_loss=0.938, ppl=1.92, wps=16853.8, ups=1.61, wpb=10476.6, bsz=373.2, num_updates=33400, lr=1.1988e-05, gnorm=0.872, train_wall=62, wall=22155
2021-01-08 19:53:00 | INFO | train_inner | epoch 060:    401 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.946, ppl=1.93, wps=17062.5, ups=1.62, wpb=10552.6, bsz=366.6, num_updates=33500, lr=1.19701e-05, gnorm=0.876, train_wall=62, wall=22217
2021-01-08 19:54:02 | INFO | train_inner | epoch 060:    501 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.232, nll_loss=0.938, ppl=1.92, wps=16987.7, ups=1.62, wpb=10479.3, bsz=376.6, num_updates=33600, lr=1.19523e-05, gnorm=0.869, train_wall=61, wall=22278
2021-01-08 19:54:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 19:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:54:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:54:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:54:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:54:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:54:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:54:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:54:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:54:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:54:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:54:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:54:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:54:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:54:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:54:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:54:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:54:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:54:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:54:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:54:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:54:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:54:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:54:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:54:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 19:54:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 19:54:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 19:55:00 | INFO | valid | epoch 060 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.25 | nll_loss 3.745 | ppl 13.41 | bleu 22.87 | wps 4635.1 | wpb 7508.5 | bsz 272.7 | num_updates 33660 | best_bleu 22.98
2021-01-08 19:55:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 19:55:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:55:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:55:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:55:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:55:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 60 @ 33660 updates, score 22.87) (writing took 3.085556510835886 seconds)
2021-01-08 19:55:03 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2021-01-08 19:55:03 | INFO | train | epoch 060 | symm_kl 0.361 | self_kl 0 | self_cv 0 | loss 3.236 | nll_loss 0.941 | ppl 1.92 | wps 15799.1 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 33660 | lr 1.19416e-05 | gnorm 0.875 | train_wall 344 | wall 22339
2021-01-08 19:55:03 | INFO | fairseq.trainer | begin training epoch 61
2021-01-08 19:55:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 19:55:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 19:55:30 | INFO | train_inner | epoch 061:     40 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.222, nll_loss=0.932, ppl=1.91, wps=11870.3, ups=1.13, wpb=10470.8, bsz=371.7, num_updates=33700, lr=1.19345e-05, gnorm=0.885, train_wall=61, wall=22367
2021-01-08 19:56:32 | INFO | train_inner | epoch 061:    140 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.944, ppl=1.92, wps=16945.9, ups=1.63, wpb=10423.5, bsz=371.8, num_updates=33800, lr=1.19169e-05, gnorm=0.871, train_wall=61, wall=22428
2021-01-08 19:57:33 | INFO | train_inner | epoch 061:    240 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.94, ppl=1.92, wps=17006.6, ups=1.62, wpb=10466.2, bsz=380.2, num_updates=33900, lr=1.18993e-05, gnorm=0.868, train_wall=61, wall=22490
2021-01-08 19:58:35 | INFO | train_inner | epoch 061:    340 / 561 symm_kl=0.364, self_kl=0, self_cv=0, loss=3.246, nll_loss=0.946, ppl=1.93, wps=16975.8, ups=1.62, wpb=10463.9, bsz=356.2, num_updates=34000, lr=1.18818e-05, gnorm=0.897, train_wall=61, wall=22551
2021-01-08 19:59:37 | INFO | train_inner | epoch 061:    440 / 561 symm_kl=0.356, self_kl=0, self_cv=0, loss=3.219, nll_loss=0.929, ppl=1.9, wps=17222.9, ups=1.62, wpb=10642.9, bsz=368.1, num_updates=34100, lr=1.18643e-05, gnorm=0.862, train_wall=62, wall=22613
2021-01-08 20:00:38 | INFO | train_inner | epoch 061:    540 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.939, ppl=1.92, wps=17026.2, ups=1.63, wpb=10444.1, bsz=371.7, num_updates=34200, lr=1.1847e-05, gnorm=0.864, train_wall=61, wall=22674
2021-01-08 20:00:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 20:00:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:00:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:00:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:00:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:00:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:00:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:00:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:00:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:00:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:00:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:00:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:00:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:00:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:00:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:00:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:00:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:00:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:00:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:00:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:00:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:00:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:00:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:00:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:00:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:00:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:00:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:00:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:01:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:01:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:01:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:01:12 | INFO | valid | epoch 061 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.25 | nll_loss 3.742 | ppl 13.38 | bleu 22.82 | wps 4645.6 | wpb 7508.5 | bsz 272.7 | num_updates 34221 | best_bleu 22.98
2021-01-08 20:01:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 20:01:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:01:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:01:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:01:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:01:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 61 @ 34221 updates, score 22.82) (writing took 3.072134844958782 seconds)
2021-01-08 20:01:15 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2021-01-08 20:01:15 | INFO | train | epoch 061 | symm_kl 0.36 | self_kl 0 | self_cv 0 | loss 3.234 | nll_loss 0.94 | ppl 1.92 | wps 15822.6 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 34221 | lr 1.18433e-05 | gnorm 0.872 | train_wall 343 | wall 22711
2021-01-08 20:01:15 | INFO | fairseq.trainer | begin training epoch 62
2021-01-08 20:01:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:01:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:02:06 | INFO | train_inner | epoch 062:     79 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.242, nll_loss=0.946, ppl=1.93, wps=11934.9, ups=1.14, wpb=10437.3, bsz=367.2, num_updates=34300, lr=1.18297e-05, gnorm=0.875, train_wall=60, wall=22762
2021-01-08 20:03:07 | INFO | train_inner | epoch 062:    179 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.928, ppl=1.9, wps=17038.5, ups=1.62, wpb=10489, bsz=366.9, num_updates=34400, lr=1.18125e-05, gnorm=0.869, train_wall=61, wall=22823
2021-01-08 20:04:09 | INFO | train_inner | epoch 062:    279 / 561 symm_kl=0.36, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.941, ppl=1.92, wps=17104.4, ups=1.62, wpb=10532.2, bsz=370.1, num_updates=34500, lr=1.17954e-05, gnorm=0.863, train_wall=61, wall=22885
2021-01-08 20:05:10 | INFO | train_inner | epoch 062:    379 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.939, ppl=1.92, wps=17087, ups=1.63, wpb=10486.1, bsz=382.8, num_updates=34600, lr=1.17783e-05, gnorm=0.863, train_wall=61, wall=22946
2021-01-08 20:06:12 | INFO | train_inner | epoch 062:    479 / 561 symm_kl=0.362, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.945, ppl=1.93, wps=16799.1, ups=1.62, wpb=10399.9, bsz=357.1, num_updates=34700, lr=1.17613e-05, gnorm=0.878, train_wall=62, wall=23008
2021-01-08 20:07:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 20:07:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:07:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:07:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:07:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:07:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:07:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:07:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:07:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:07:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:07:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:07:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:07:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:07:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:07:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:07:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:07:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:07:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:07:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:07:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:07:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:07:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:07:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:07:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:07:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:07:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:07:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:07:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:07:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:07:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:07:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:07:23 | INFO | valid | epoch 062 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.251 | nll_loss 3.743 | ppl 13.39 | bleu 22.93 | wps 4689.5 | wpb 7508.5 | bsz 272.7 | num_updates 34782 | best_bleu 22.98
2021-01-08 20:07:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 20:07:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:07:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:07:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:07:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:07:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 62 @ 34782 updates, score 22.93) (writing took 2.924310954287648 seconds)
2021-01-08 20:07:26 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2021-01-08 20:07:26 | INFO | train | epoch 062 | symm_kl 0.359 | self_kl 0 | self_cv 0 | loss 3.233 | nll_loss 0.939 | ppl 1.92 | wps 15844.7 | ups 1.51 | wpb 10483.4 | bsz 369.6 | num_updates 34782 | lr 1.17474e-05 | gnorm 0.868 | train_wall 343 | wall 23082
2021-01-08 20:07:26 | INFO | fairseq.trainer | begin training epoch 63
2021-01-08 20:07:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:07:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:07:40 | INFO | train_inner | epoch 063:     18 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.227, nll_loss=0.935, ppl=1.91, wps=11934.9, ups=1.14, wpb=10483.3, bsz=366, num_updates=34800, lr=1.17444e-05, gnorm=0.866, train_wall=61, wall=23096
2021-01-08 20:08:41 | INFO | train_inner | epoch 063:    118 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.937, ppl=1.91, wps=17453.4, ups=1.64, wpb=10670.8, bsz=364.8, num_updates=34900, lr=1.17276e-05, gnorm=0.862, train_wall=61, wall=23157
2021-01-08 20:09:42 | INFO | train_inner | epoch 063:    218 / 561 symm_kl=0.358, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.939, ppl=1.92, wps=17058.3, ups=1.63, wpb=10481.1, bsz=372.8, num_updates=35000, lr=1.17108e-05, gnorm=0.884, train_wall=61, wall=23219
2021-01-08 20:10:44 | INFO | train_inner | epoch 063:    318 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.225, nll_loss=0.934, ppl=1.91, wps=16995.7, ups=1.62, wpb=10497.3, bsz=383.2, num_updates=35100, lr=1.16941e-05, gnorm=0.865, train_wall=62, wall=23280
2021-01-08 20:11:46 | INFO | train_inner | epoch 063:    418 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.95, ppl=1.93, wps=16909.7, ups=1.63, wpb=10385, bsz=363.8, num_updates=35200, lr=1.16775e-05, gnorm=0.865, train_wall=61, wall=23342
2021-01-08 20:12:47 | INFO | train_inner | epoch 063:    518 / 561 symm_kl=0.363, self_kl=0, self_cv=0, loss=3.247, nll_loss=0.95, ppl=1.93, wps=16861, ups=1.62, wpb=10433, bsz=380.1, num_updates=35300, lr=1.16609e-05, gnorm=0.881, train_wall=62, wall=23404
2021-01-08 20:13:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 20:13:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:13:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:13:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:13:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:13:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:13:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:13:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:13:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:13:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:13:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:13:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:13:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:13:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:13:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:13:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:13:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:13:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:13:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:13:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:13:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:13:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:13:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:13:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:13:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:13:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:13:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:13:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:13:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:13:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:13:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:13:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:13:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:13:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:13:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:13:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:13:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:13:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:13:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:13:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:13:34 | INFO | valid | epoch 063 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.246 | nll_loss 3.74 | ppl 13.36 | bleu 22.98 | wps 4744.2 | wpb 7508.5 | bsz 272.7 | num_updates 35343 | best_bleu 22.98
2021-01-08 20:13:34 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 20:13:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:13:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:13:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:13:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:13:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_best.pt (epoch 63 @ 35343 updates, score 22.98) (writing took 5.16318878903985 seconds)
2021-01-08 20:13:39 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2021-01-08 20:13:39 | INFO | train | epoch 063 | symm_kl 0.359 | self_kl 0 | self_cv 0 | loss 3.232 | nll_loss 0.939 | ppl 1.92 | wps 15748.8 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 35343 | lr 1.16538e-05 | gnorm 0.871 | train_wall 344 | wall 23456
2021-01-08 20:13:39 | INFO | fairseq.trainer | begin training epoch 64
2021-01-08 20:13:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:13:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:14:17 | INFO | train_inner | epoch 064:     57 / 561 symm_kl=0.354, self_kl=0, self_cv=0, loss=3.21, nll_loss=0.921, ppl=1.89, wps=11732.4, ups=1.12, wpb=10499.9, bsz=366.3, num_updates=35400, lr=1.16445e-05, gnorm=0.859, train_wall=61, wall=23493
2021-01-08 20:15:19 | INFO | train_inner | epoch 064:    157 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.234, nll_loss=0.94, ppl=1.92, wps=16871.5, ups=1.61, wpb=10487, bsz=366.4, num_updates=35500, lr=1.1628e-05, gnorm=0.868, train_wall=62, wall=23555
2021-01-08 20:16:22 | INFO | train_inner | epoch 064:    257 / 561 symm_kl=0.357, self_kl=0, self_cv=0, loss=3.231, nll_loss=0.942, ppl=1.92, wps=16791.6, ups=1.6, wpb=10474.1, bsz=362.2, num_updates=35600, lr=1.16117e-05, gnorm=0.858, train_wall=62, wall=23618
2021-01-08 20:17:23 | INFO | train_inner | epoch 064:    357 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.238, nll_loss=0.943, ppl=1.92, wps=17095.2, ups=1.61, wpb=10589.6, bsz=368.4, num_updates=35700, lr=1.15954e-05, gnorm=0.86, train_wall=62, wall=23680
2021-01-08 20:18:25 | INFO | train_inner | epoch 064:    457 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.236, nll_loss=0.939, ppl=1.92, wps=16846.1, ups=1.62, wpb=10413.9, bsz=365.5, num_updates=35800, lr=1.15792e-05, gnorm=0.879, train_wall=62, wall=23742
2021-01-08 20:19:27 | INFO | train_inner | epoch 064:    557 / 561 symm_kl=0.355, self_kl=0, self_cv=0, loss=3.223, nll_loss=0.935, ppl=1.91, wps=17028.2, ups=1.62, wpb=10487.9, bsz=378.7, num_updates=35900, lr=1.15631e-05, gnorm=0.857, train_wall=61, wall=23803
2021-01-08 20:19:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-08 20:19:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:19:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:19:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:19:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:19:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:19:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:19:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:19:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:19:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:19:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:19:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:19:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:19:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:19:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:19:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:19:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:19:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:19:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:19:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:19:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:19:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:19:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:19:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:19:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:19:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-08 20:19:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-08 20:19:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-08 20:19:50 | INFO | valid | epoch 064 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.248 | nll_loss 3.742 | ppl 13.38 | bleu 22.91 | wps 4720.2 | wpb 7508.5 | bsz 272.7 | num_updates 35904 | best_bleu 22.98
2021-01-08 20:19:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-08 20:19:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:19:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:19:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:19:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:19:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/change_p/checkpoint_last.pt (epoch 64 @ 35904 updates, score 22.91) (writing took 3.026718482375145 seconds)
2021-01-08 20:19:53 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2021-01-08 20:19:53 | INFO | train | epoch 064 | symm_kl 0.358 | self_kl 0 | self_cv 0 | loss 3.231 | nll_loss 0.938 | ppl 1.92 | wps 15751.6 | ups 1.5 | wpb 10483.4 | bsz 369.6 | num_updates 35904 | lr 1.15624e-05 | gnorm 0.864 | train_wall 346 | wall 23829
2021-01-08 20:19:53 | INFO | fairseq.trainer | begin training epoch 65
2021-01-08 20:19:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-08 20:19:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-08 20:20:54 | INFO | train_inner | epoch 065:     96 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.237, nll_loss=0.944, ppl=1.92, wps=11879, ups=1.15, wpb=10335.8, bsz=360.5, num_updates=36000, lr=1.1547e-05, gnorm=0.875, train_wall=60, wall=23890
2021-01-08 20:21:56 | INFO | train_inner | epoch 065:    196 / 561 symm_kl=0.359, self_kl=0, self_cv=0, loss=3.229, nll_loss=0.935, ppl=1.91, wps=16936.1, ups=1.61, wpb=10512.1, bsz=360.4, num_updates=36100, lr=1.1531e-05, gnorm=0.874, train_wall=62, wall=23952
2021-01-08 20:22:58 | INFO | train_inner | epoch 065:    296 / 561 symm_kl=0.361, self_kl=0, self_cv=0, loss=3.241, nll_loss=0.945, ppl=1.93, wps=16853.6, ups=1.62, wpb=10413.8, bsz=379.9, num_updates=36200, lr=1.15151e-05, gnorm=0.881, train_wall=62, wall=24014
Traceback (most recent call last):
  File "train.py", line 14, in <module>
    cli_main()
  File "/home/rcduan/fairseq/fairseq/fairseq_cli/train.py", line 362, in cli_main
    distributed_utils.call_main(args, main)
  File "/home/rcduan/fairseq/fairseq/fairseq/distributed_utils.py", line 237, in call_main
    torch.multiprocessing.spawn(
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 200, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 158, in start_processes
    while not context.join():
  File "/home/rcduan/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 106, in join
    raise Exception(
Exception: process 0 terminated with signal SIGKILL
examples/entr/bash/kl_train.sh: 69: examples/entr/bash/kl_train.sh: Syntax error: "in" unexpected
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 420 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
