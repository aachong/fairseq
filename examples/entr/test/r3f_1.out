nohup: ignoring input
save_dir=./examples/entr/bash/../checkpoints/closer-all
在最后一层正则化之前输出，模型是baseline_continue
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.00004
lrscheduler=inverse_sqrt
warmup_updates=3000
max_epoch=200
r3f_lambda=1
extr='--warmup-init-lr 1e-07 --reset-optimizer'
2021-01-28 11:28:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:28:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:28:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:28:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:28:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:28:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:28:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:28:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:28:11 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:16396
2021-01-28 11:28:11 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:16396
2021-01-28 11:28:11 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:16396
2021-01-28 11:28:12 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2021-01-28 11:28:12 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2021-01-28 11:28:12 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2021-01-28 11:28:15 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, cv=False, cv_lambda=0.0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', de_std=False, decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:16396', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-06, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[4e-05], lr_scheduler='inverse_sqrt', max_epoch=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=False, noised_no_grad=False, nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=1.0, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer-all', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, self_training_drc=False, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=3000, weight_decay=0.0, zero_sharding='none')
2021-01-28 11:28:15 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2021-01-28 11:28:15 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2021-01-28 11:28:15 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2021-01-28 11:28:15 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2021-01-28 11:28:15 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2021-01-28 11:28:16 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2021-01-28 11:28:16 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2021-01-28 11:28:16 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2021-01-28 11:28:16 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2021-01-28 11:28:16 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2021-01-28 11:28:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2021-01-28 11:28:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2021-01-28 11:28:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-28 11:28:16 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-28 11:28:16 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-28 11:28:16 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2021-01-28 11:28:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2021-01-28 11:28:16 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2021-01-28 11:28:16 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2021-01-28 11:28:17 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 131 @ 0 updates)
2021-01-28 11:28:17 | INFO | fairseq.optim.adam | using FusedAdam
2021-01-28 11:28:17 | INFO | fairseq.trainer | loading train data for epoch 131
2021-01-28 11:28:17 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2021-01-28 11:28:17 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2021-01-28 11:28:17 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2021-01-28 11:28:17 | INFO | fairseq.trainer | begin training epoch 131
2021-01-28 11:28:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:28:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:28:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:28:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:28:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:28:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:29:20 | INFO | train_inner | epoch 131:    100 / 561 symm_kl=0.321, self_kl=0, self_cv=0, loss=3.155, nll_loss=0.911, ppl=1.88, wps=14569.4, ups=1.38, wpb=10553.2, bsz=381, num_updates=100, lr=1.43e-06, gnorm=0.833, train_wall=108, wall=0
2021-01-28 11:30:18 | INFO | train_inner | epoch 131:    200 / 561 symm_kl=0.322, self_kl=0, self_cv=0, loss=3.162, nll_loss=0.917, ppl=1.89, wps=17793, ups=1.71, wpb=10404.1, bsz=367, num_updates=200, lr=2.76e-06, gnorm=0.835, train_wall=58, wall=0
2021-01-28 11:31:17 | INFO | train_inner | epoch 131:    300 / 561 symm_kl=0.33, self_kl=0, self_cv=0, loss=3.192, nll_loss=0.938, ppl=1.92, wps=17836.3, ups=1.71, wpb=10428.8, bsz=363.2, num_updates=300, lr=4.09e-06, gnorm=0.857, train_wall=58, wall=0
2021-01-28 11:32:16 | INFO | train_inner | epoch 131:    400 / 561 symm_kl=0.319, self_kl=0, self_cv=0, loss=3.145, nll_loss=0.903, ppl=1.87, wps=17753.1, ups=1.69, wpb=10483.8, bsz=367.3, num_updates=400, lr=5.42e-06, gnorm=0.829, train_wall=59, wall=0
2021-01-28 11:33:15 | INFO | train_inner | epoch 131:    500 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.144, nll_loss=0.903, ppl=1.87, wps=17950.8, ups=1.68, wpb=10660, bsz=365.6, num_updates=500, lr=6.75e-06, gnorm=0.811, train_wall=59, wall=0
2021-01-28 11:33:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 11:33:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:33:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:33:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:33:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:33:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:33:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:33:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:33:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:33:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:33:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:33:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:33:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:33:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:33:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:33:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:33:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:33:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:33:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:33:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:33:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:33:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:33:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:33:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:33:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:34:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:34:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:34:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:34:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:34:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:34:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:34:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:34:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:34:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:34:12 | INFO | valid | epoch 131 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.28 | nll_loss 3.736 | ppl 13.32 | bleu 23.17 | wps 4708.9 | wpb 7508.5 | bsz 272.7 | num_updates 561
2021-01-28 11:34:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 11:34:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:34:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:34:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:34:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:34:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 131 @ 561 updates, score 23.17) (writing took 5.026214045996312 seconds)
2021-01-28 11:34:17 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2021-01-28 11:34:17 | INFO | train | epoch 131 | symm_kl 0.324 | self_kl 0 | self_cv 0 | loss 3.162 | nll_loss 0.915 | ppl 1.88 | wps 16468 | ups 1.57 | wpb 10483.4 | bsz 369.6 | num_updates 561 | lr 7.5613e-06 | gnorm 0.838 | train_wall 659 | wall 0
2021-01-28 11:34:17 | INFO | fairseq.trainer | begin training epoch 132
2021-01-28 11:34:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:34:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:34:42 | INFO | train_inner | epoch 132:     39 / 561 symm_kl=0.324, self_kl=0, self_cv=0, loss=3.167, nll_loss=0.919, ppl=1.89, wps=11871.9, ups=1.15, wpb=10337.9, bsz=363, num_updates=600, lr=8.08e-06, gnorm=0.839, train_wall=58, wall=0
2021-01-28 11:35:41 | INFO | train_inner | epoch 132:    139 / 561 symm_kl=0.323, self_kl=0, self_cv=0, loss=3.171, nll_loss=0.925, ppl=1.9, wps=18015.8, ups=1.7, wpb=10615.2, bsz=366.4, num_updates=700, lr=9.41e-06, gnorm=0.827, train_wall=59, wall=0
2021-01-28 11:36:40 | INFO | train_inner | epoch 132:    239 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.144, nll_loss=0.905, ppl=1.87, wps=17828.2, ups=1.69, wpb=10527.1, bsz=380.5, num_updates=800, lr=1.074e-05, gnorm=0.826, train_wall=59, wall=0
2021-01-28 11:37:40 | INFO | train_inner | epoch 132:    339 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.138, nll_loss=0.902, ppl=1.87, wps=18038.5, ups=1.69, wpb=10678.1, bsz=368.9, num_updates=900, lr=1.207e-05, gnorm=0.817, train_wall=59, wall=0
2021-01-28 11:38:38 | INFO | train_inner | epoch 132:    439 / 561 symm_kl=0.323, self_kl=0, self_cv=0, loss=3.166, nll_loss=0.919, ppl=1.89, wps=17690.3, ups=1.7, wpb=10381.4, bsz=376.4, num_updates=1000, lr=1.34e-05, gnorm=0.842, train_wall=59, wall=0
2021-01-28 11:39:37 | INFO | train_inner | epoch 132:    539 / 561 symm_kl=0.326, self_kl=0, self_cv=0, loss=3.176, nll_loss=0.926, ppl=1.9, wps=17588.1, ups=1.71, wpb=10303.3, bsz=363, num_updates=1100, lr=1.473e-05, gnorm=0.844, train_wall=58, wall=0
2021-01-28 11:39:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 11:39:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:39:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:39:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:39:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:39:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:39:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:39:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:39:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:39:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:39:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:39:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:39:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:39:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:39:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:39:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:39:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:39:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:39:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:39:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:39:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:39:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:39:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:39:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:39:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:39:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:39:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:39:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:39:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:39:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:39:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:39:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:39:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:39:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:39:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:39:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:39:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:39:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:39:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:39:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:40:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:40:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:40:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:40:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:40:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:40:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:40:10 | INFO | valid | epoch 132 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.27 | nll_loss 3.73 | ppl 13.27 | bleu 23.18 | wps 4654.6 | wpb 7508.5 | bsz 272.7 | num_updates 1122 | best_bleu 23.18
2021-01-28 11:40:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 11:40:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:40:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:40:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:40:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:40:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 132 @ 1122 updates, score 23.18) (writing took 5.03696336603025 seconds)
2021-01-28 11:40:16 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2021-01-28 11:40:16 | INFO | train | epoch 132 | symm_kl 0.321 | self_kl 0 | self_cv 0 | loss 3.159 | nll_loss 0.915 | ppl 1.89 | wps 16399.4 | ups 1.56 | wpb 10483.4 | bsz 369.6 | num_updates 1122 | lr 1.50226e-05 | gnorm 0.832 | train_wall 328 | wall 0
2021-01-28 11:40:16 | INFO | fairseq.trainer | begin training epoch 133
2021-01-28 11:40:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:40:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:41:04 | INFO | train_inner | epoch 133:     78 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.13, nll_loss=0.896, ppl=1.86, wps=11994.7, ups=1.15, wpb=10458.5, bsz=382.4, num_updates=1200, lr=1.606e-05, gnorm=0.825, train_wall=58, wall=0
2021-01-28 11:42:03 | INFO | train_inner | epoch 133:    178 / 561 symm_kl=0.324, self_kl=0, self_cv=0, loss=3.172, nll_loss=0.926, ppl=1.9, wps=17838, ups=1.71, wpb=10445.5, bsz=370.7, num_updates=1300, lr=1.739e-05, gnorm=0.84, train_wall=58, wall=0
2021-01-28 11:43:02 | INFO | train_inner | epoch 133:    278 / 561 symm_kl=0.322, self_kl=0, self_cv=0, loss=3.166, nll_loss=0.921, ppl=1.89, wps=17720.1, ups=1.69, wpb=10469.6, bsz=356.1, num_updates=1400, lr=1.872e-05, gnorm=0.84, train_wall=59, wall=0
2021-01-28 11:44:01 | INFO | train_inner | epoch 133:    378 / 561 symm_kl=0.32, self_kl=0, self_cv=0, loss=3.165, nll_loss=0.925, ppl=1.9, wps=17784.5, ups=1.7, wpb=10486.1, bsz=381.8, num_updates=1500, lr=2.005e-05, gnorm=0.825, train_wall=59, wall=0
2021-01-28 11:45:00 | INFO | train_inner | epoch 133:    478 / 561 symm_kl=0.319, self_kl=0, self_cv=0, loss=3.161, nll_loss=0.922, ppl=1.89, wps=17832.8, ups=1.7, wpb=10512.6, bsz=361.4, num_updates=1600, lr=2.138e-05, gnorm=0.833, train_wall=59, wall=0
2021-01-28 11:45:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 11:45:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:45:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:45:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:45:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:45:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:45:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:45:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:45:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:45:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:45:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:45:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:45:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:45:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:45:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:45:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:45:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:45:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:45:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:45:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:45:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:45:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:45:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:45:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:45:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:45:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:45:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:45:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:45:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:45:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:45:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:45:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:45:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:45:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:45:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:45:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:45:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:46:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:46:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:46:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:46:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:46:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:46:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:46:09 | INFO | valid | epoch 133 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.258 | nll_loss 3.724 | ppl 13.21 | bleu 23.16 | wps 4707.2 | wpb 7508.5 | bsz 272.7 | num_updates 1683 | best_bleu 23.18
2021-01-28 11:46:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 11:46:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:46:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:46:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:46:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:46:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 133 @ 1683 updates, score 23.16) (writing took 3.130889891006518 seconds)
2021-01-28 11:46:12 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2021-01-28 11:46:12 | INFO | train | epoch 133 | symm_kl 0.32 | self_kl 0 | self_cv 0 | loss 3.16 | nll_loss 0.918 | ppl 1.89 | wps 16485.4 | ups 1.57 | wpb 10483.4 | bsz 369.6 | num_updates 1683 | lr 2.24839e-05 | gnorm 0.832 | train_wall 329 | wall 0
2021-01-28 11:46:12 | INFO | fairseq.trainer | begin training epoch 134
2021-01-28 11:46:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:46:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:46:25 | INFO | train_inner | epoch 134:     17 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.151, nll_loss=0.915, ppl=1.89, wps=12194.5, ups=1.17, wpb=10453.9, bsz=368.6, num_updates=1700, lr=2.271e-05, gnorm=0.832, train_wall=59, wall=0
2021-01-28 11:47:24 | INFO | train_inner | epoch 134:    117 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.147, nll_loss=0.908, ppl=1.88, wps=17864.7, ups=1.71, wpb=10470.9, bsz=371.7, num_updates=1800, lr=2.404e-05, gnorm=0.837, train_wall=58, wall=0
2021-01-28 11:48:23 | INFO | train_inner | epoch 134:    217 / 561 symm_kl=0.323, self_kl=0, self_cv=0, loss=3.173, nll_loss=0.929, ppl=1.9, wps=17560.6, ups=1.7, wpb=10342.3, bsz=362.4, num_updates=1900, lr=2.537e-05, gnorm=0.842, train_wall=59, wall=0
2021-01-28 11:49:22 | INFO | train_inner | epoch 134:    317 / 561 symm_kl=0.319, self_kl=0, self_cv=0, loss=3.165, nll_loss=0.926, ppl=1.9, wps=17688.3, ups=1.68, wpb=10501.4, bsz=380.8, num_updates=2000, lr=2.67e-05, gnorm=0.829, train_wall=59, wall=0
2021-01-28 11:50:21 | INFO | train_inner | epoch 134:    417 / 561 symm_kl=0.325, self_kl=0, self_cv=0, loss=3.175, nll_loss=0.928, ppl=1.9, wps=17902.1, ups=1.71, wpb=10491.9, bsz=347.6, num_updates=2100, lr=2.803e-05, gnorm=0.852, train_wall=58, wall=0
2021-01-28 11:51:20 | INFO | train_inner | epoch 134:    517 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.159, nll_loss=0.923, ppl=1.9, wps=17926.3, ups=1.7, wpb=10544.7, bsz=369.8, num_updates=2200, lr=2.936e-05, gnorm=0.829, train_wall=59, wall=0
2021-01-28 11:51:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 11:51:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:51:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:51:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:51:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:51:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:51:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:51:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:51:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:51:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:51:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:51:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:51:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:51:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:51:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:51:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:51:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:51:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:51:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:51:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:51:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:51:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:51:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:51:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:51:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:51:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:51:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:51:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:51:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:51:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:51:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:51:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:51:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:51:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:51:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:51:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:51:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:51:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:51:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:51:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:52:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:52:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:52:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:52:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:52:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:52:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:52:06 | INFO | valid | epoch 134 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.246 | nll_loss 3.717 | ppl 13.15 | bleu 23.29 | wps 4658.9 | wpb 7508.5 | bsz 272.7 | num_updates 2244 | best_bleu 23.29
2021-01-28 11:52:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 11:52:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:52:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:52:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:52:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:52:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 134 @ 2244 updates, score 23.29) (writing took 4.975221433036495 seconds)
2021-01-28 11:52:11 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2021-01-28 11:52:11 | INFO | train | epoch 134 | symm_kl 0.32 | self_kl 0 | self_cv 0 | loss 3.163 | nll_loss 0.923 | ppl 1.9 | wps 16376.8 | ups 1.56 | wpb 10483.4 | bsz 369.6 | num_updates 2244 | lr 2.99452e-05 | gnorm 0.838 | train_wall 329 | wall 0
2021-01-28 11:52:11 | INFO | fairseq.trainer | begin training epoch 135
2021-01-28 11:52:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:52:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:52:47 | INFO | train_inner | epoch 135:     56 / 561 symm_kl=0.321, self_kl=0, self_cv=0, loss=3.175, nll_loss=0.935, ppl=1.91, wps=11973.5, ups=1.15, wpb=10436.3, bsz=381.8, num_updates=2300, lr=3.069e-05, gnorm=0.854, train_wall=58, wall=0
2021-01-28 11:53:46 | INFO | train_inner | epoch 135:    156 / 561 symm_kl=0.321, self_kl=0, self_cv=0, loss=3.176, nll_loss=0.935, ppl=1.91, wps=17787.2, ups=1.68, wpb=10585, bsz=369.6, num_updates=2400, lr=3.202e-05, gnorm=0.837, train_wall=59, wall=0
2021-01-28 11:54:46 | INFO | train_inner | epoch 135:    256 / 561 symm_kl=0.319, self_kl=0, self_cv=0, loss=3.164, nll_loss=0.926, ppl=1.9, wps=17741.7, ups=1.68, wpb=10535.8, bsz=371, num_updates=2500, lr=3.335e-05, gnorm=0.834, train_wall=59, wall=0
2021-01-28 11:55:45 | INFO | train_inner | epoch 135:    356 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.158, nll_loss=0.922, ppl=1.9, wps=17728.8, ups=1.68, wpb=10577.6, bsz=370.7, num_updates=2600, lr=3.468e-05, gnorm=0.832, train_wall=59, wall=0
2021-01-28 11:56:44 | INFO | train_inner | epoch 135:    456 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.155, nll_loss=0.92, ppl=1.89, wps=17538.1, ups=1.69, wpb=10353, bsz=362.9, num_updates=2700, lr=3.601e-05, gnorm=0.846, train_wall=59, wall=0
2021-01-28 11:57:44 | INFO | train_inner | epoch 135:    556 / 561 symm_kl=0.32, self_kl=0, self_cv=0, loss=3.17, nll_loss=0.931, ppl=1.91, wps=17601.2, ups=1.67, wpb=10510.9, bsz=373.8, num_updates=2800, lr=3.734e-05, gnorm=0.84, train_wall=60, wall=0
2021-01-28 11:57:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 11:57:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:57:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:57:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:57:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:57:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:57:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:57:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:57:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:57:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:57:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:57:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:57:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:57:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:57:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:57:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:57:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:57:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:57:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:57:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:57:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:57:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:57:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:57:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:57:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:57:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:57:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:57:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:57:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:57:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:57:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:57:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:57:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:57:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:57:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:57:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:57:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:57:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:57:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:57:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:58:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:58:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:58:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:58:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 11:58:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 11:58:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 11:58:08 | INFO | valid | epoch 135 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.252 | nll_loss 3.724 | ppl 13.22 | bleu 23.18 | wps 4680.6 | wpb 7508.5 | bsz 272.7 | num_updates 2805 | best_bleu 23.29
2021-01-28 11:58:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 11:58:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:58:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:58:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:58:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:58:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 135 @ 2805 updates, score 23.18) (writing took 3.15054413897451 seconds)
2021-01-28 11:58:11 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2021-01-28 11:58:11 | INFO | train | epoch 135 | symm_kl 0.319 | self_kl 0 | self_cv 0 | loss 3.166 | nll_loss 0.928 | ppl 1.9 | wps 16358.3 | ups 1.56 | wpb 10483.4 | bsz 369.6 | num_updates 2805 | lr 3.74065e-05 | gnorm 0.84 | train_wall 331 | wall 0
2021-01-28 11:58:11 | INFO | fairseq.trainer | begin training epoch 136
2021-01-28 11:58:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 11:58:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 11:59:09 | INFO | train_inner | epoch 136:     95 / 561 symm_kl=0.324, self_kl=0, self_cv=0, loss=3.183, nll_loss=0.938, ppl=1.92, wps=12033.3, ups=1.17, wpb=10263.3, bsz=356.5, num_updates=2900, lr=3.867e-05, gnorm=0.862, train_wall=58, wall=0
2021-01-28 12:00:09 | INFO | train_inner | epoch 136:    195 / 561 symm_kl=0.321, self_kl=0, self_cv=0, loss=3.168, nll_loss=0.928, ppl=1.9, wps=17864.3, ups=1.69, wpb=10568.8, bsz=357, num_updates=3000, lr=4e-05, gnorm=0.846, train_wall=59, wall=0
2021-01-28 12:01:08 | INFO | train_inner | epoch 136:    295 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.159, nll_loss=0.928, ppl=1.9, wps=17625.5, ups=1.68, wpb=10467.1, bsz=390.1, num_updates=3100, lr=3.93496e-05, gnorm=0.845, train_wall=59, wall=0
2021-01-28 12:02:07 | INFO | train_inner | epoch 136:    395 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.167, nll_loss=0.932, ppl=1.91, wps=17727.7, ups=1.69, wpb=10506.4, bsz=371.5, num_updates=3200, lr=3.87298e-05, gnorm=0.837, train_wall=59, wall=0
2021-01-28 12:03:06 | INFO | train_inner | epoch 136:    495 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.162, nll_loss=0.926, ppl=1.9, wps=17827.5, ups=1.69, wpb=10574.9, bsz=372.9, num_updates=3300, lr=3.81385e-05, gnorm=0.838, train_wall=59, wall=0
2021-01-28 12:03:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 12:03:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:03:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:03:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:03:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:03:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:03:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:03:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:03:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:03:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:03:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:03:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:03:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:03:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:03:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:03:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:03:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:03:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:03:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:03:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:03:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:03:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:03:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:03:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:03:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:03:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:03:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:03:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:03:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:03:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:03:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:03:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:03:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:03:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:03:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:03:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:03:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:03:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:03:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:03:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:04:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:04:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:04:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:04:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:04:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:04:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:04:08 | INFO | valid | epoch 136 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.243 | nll_loss 3.719 | ppl 13.17 | bleu 23.39 | wps 4731.7 | wpb 7508.5 | bsz 272.7 | num_updates 3366 | best_bleu 23.39
2021-01-28 12:04:08 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 12:04:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:04:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:04:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:04:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:04:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_best.pt (epoch 136 @ 3366 updates, score 23.39) (writing took 5.000424325000495 seconds)
2021-01-28 12:04:13 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2021-01-28 12:04:13 | INFO | train | epoch 136 | symm_kl 0.32 | self_kl 0 | self_cv 0 | loss 3.17 | nll_loss 0.932 | ppl 1.91 | wps 16221.1 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 3366 | lr 3.77627e-05 | gnorm 0.846 | train_wall 330 | wall 0
2021-01-28 12:04:13 | INFO | fairseq.trainer | begin training epoch 137
2021-01-28 12:04:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:04:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:04:36 | INFO | train_inner | epoch 137:     34 / 561 symm_kl=0.321, self_kl=0, self_cv=0, loss=3.181, nll_loss=0.941, ppl=1.92, wps=11640.7, ups=1.11, wpb=10451.5, bsz=362.2, num_updates=3400, lr=3.75735e-05, gnorm=0.859, train_wall=58, wall=0
2021-01-28 12:05:35 | INFO | train_inner | epoch 137:    134 / 561 symm_kl=0.319, self_kl=0, self_cv=0, loss=3.167, nll_loss=0.928, ppl=1.9, wps=17804.1, ups=1.7, wpb=10487, bsz=352.6, num_updates=3500, lr=3.70328e-05, gnorm=0.842, train_wall=59, wall=0
2021-01-28 12:06:35 | INFO | train_inner | epoch 137:    234 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.145, nll_loss=0.914, ppl=1.88, wps=17628.3, ups=1.67, wpb=10576.3, bsz=397.2, num_updates=3600, lr=3.65148e-05, gnorm=0.827, train_wall=60, wall=0
2021-01-28 12:07:35 | INFO | train_inner | epoch 137:    334 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.163, nll_loss=0.929, ppl=1.9, wps=17754.8, ups=1.69, wpb=10533, bsz=375.2, num_updates=3700, lr=3.6018e-05, gnorm=0.844, train_wall=59, wall=0
2021-01-28 12:08:34 | INFO | train_inner | epoch 137:    434 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.17, nll_loss=0.939, ppl=1.92, wps=17674.1, ups=1.68, wpb=10519.8, bsz=385.3, num_updates=3800, lr=3.55409e-05, gnorm=0.828, train_wall=59, wall=0
2021-01-28 12:09:33 | INFO | train_inner | epoch 137:    534 / 561 symm_kl=0.323, self_kl=0, self_cv=0, loss=3.186, nll_loss=0.945, ppl=1.92, wps=17534.7, ups=1.68, wpb=10408.1, bsz=351.7, num_updates=3900, lr=3.50823e-05, gnorm=0.857, train_wall=59, wall=0
2021-01-28 12:09:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 12:09:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:09:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:09:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:09:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:09:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:09:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:09:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:09:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:09:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:09:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:09:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:09:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:09:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:09:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:09:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:09:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:09:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:09:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:09:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:09:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:09:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:09:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:09:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:09:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:09:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:09:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:09:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:09:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:09:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:09:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:09:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:09:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:09:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:09:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:09:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:09:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:09:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:09:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:09:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:10:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:10:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:10:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:10:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:10:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:10:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:10:10 | INFO | valid | epoch 137 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.23 | nll_loss 3.712 | ppl 13.1 | bleu 23.15 | wps 4584.1 | wpb 7508.5 | bsz 272.7 | num_updates 3927 | best_bleu 23.39
2021-01-28 12:10:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 12:10:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:10:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:10:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:10:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:10:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 137 @ 3927 updates, score 23.15) (writing took 3.1967497330042534 seconds)
2021-01-28 12:10:14 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2021-01-28 12:10:14 | INFO | train | epoch 137 | symm_kl 0.318 | self_kl 0 | self_cv 0 | loss 3.167 | nll_loss 0.931 | ppl 1.91 | wps 16335.1 | ups 1.56 | wpb 10483.4 | bsz 369.6 | num_updates 3927 | lr 3.49615e-05 | gnorm 0.846 | train_wall 331 | wall 0
2021-01-28 12:10:14 | INFO | fairseq.trainer | begin training epoch 138
2021-01-28 12:10:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:10:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:10:59 | INFO | train_inner | epoch 138:     73 / 561 symm_kl=0.319, self_kl=0, self_cv=0, loss=3.164, nll_loss=0.926, ppl=1.9, wps=12009.7, ups=1.16, wpb=10329.7, bsz=348.7, num_updates=4000, lr=3.4641e-05, gnorm=0.87, train_wall=58, wall=0
2021-01-28 12:11:59 | INFO | train_inner | epoch 138:    173 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.156, nll_loss=0.926, ppl=1.9, wps=17777, ups=1.69, wpb=10541.5, bsz=375.8, num_updates=4100, lr=3.4216e-05, gnorm=0.829, train_wall=59, wall=0
2021-01-28 12:12:59 | INFO | train_inner | epoch 138:    273 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.143, nll_loss=0.916, ppl=1.89, wps=17611.6, ups=1.67, wpb=10569.1, bsz=379.3, num_updates=4200, lr=3.38062e-05, gnorm=0.819, train_wall=60, wall=0
2021-01-28 12:13:58 | INFO | train_inner | epoch 138:    373 / 561 symm_kl=0.321, self_kl=0, self_cv=0, loss=3.174, nll_loss=0.934, ppl=1.91, wps=17576.5, ups=1.68, wpb=10447.1, bsz=365.4, num_updates=4300, lr=3.34108e-05, gnorm=0.847, train_wall=59, wall=0
2021-01-28 12:14:57 | INFO | train_inner | epoch 138:    473 / 561 symm_kl=0.321, self_kl=0, self_cv=0, loss=3.184, nll_loss=0.946, ppl=1.93, wps=17626.9, ups=1.69, wpb=10412, bsz=367.8, num_updates=4400, lr=3.30289e-05, gnorm=0.853, train_wall=59, wall=0
2021-01-28 12:15:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 12:15:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:15:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:15:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:15:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:15:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:15:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:15:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:15:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:15:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:15:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:15:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:15:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:15:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:15:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:15:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:15:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:15:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:15:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:15:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:15:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:15:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:15:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:15:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:15:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:15:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:15:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:15:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:15:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:15:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:15:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:15:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:15:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:15:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:15:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:15:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:15:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:16:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:16:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:16:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:16:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:16:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:16:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:16:13 | INFO | valid | epoch 138 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.238 | nll_loss 3.718 | ppl 13.16 | bleu 22.98 | wps 4112 | wpb 7508.5 | bsz 272.7 | num_updates 4488 | best_bleu 23.39
2021-01-28 12:16:13 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 12:16:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:16:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:16:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:16:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:16:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 138 @ 4488 updates, score 22.98) (writing took 3.1000162109849043 seconds)
2021-01-28 12:16:16 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2021-01-28 12:16:16 | INFO | train | epoch 138 | symm_kl 0.317 | self_kl 0 | self_cv 0 | loss 3.164 | nll_loss 0.93 | ppl 1.91 | wps 16237.1 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 4488 | lr 3.27035e-05 | gnorm 0.838 | train_wall 332 | wall 0
2021-01-28 12:16:16 | INFO | fairseq.trainer | begin training epoch 139
2021-01-28 12:16:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:16:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:16:26 | INFO | train_inner | epoch 139:     12 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.158, nll_loss=0.929, ppl=1.9, wps=11813.3, ups=1.13, wpb=10490.6, bsz=377.6, num_updates=4500, lr=3.26599e-05, gnorm=0.828, train_wall=59, wall=0
2021-01-28 12:17:25 | INFO | train_inner | epoch 139:    112 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.148, nll_loss=0.916, ppl=1.89, wps=17998.8, ups=1.71, wpb=10549.4, bsz=378, num_updates=4600, lr=3.23029e-05, gnorm=0.826, train_wall=58, wall=0
2021-01-28 12:18:24 | INFO | train_inner | epoch 139:    212 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.151, nll_loss=0.918, ppl=1.89, wps=17721.8, ups=1.68, wpb=10562.7, bsz=361.5, num_updates=4700, lr=3.19574e-05, gnorm=0.83, train_wall=59, wall=0
2021-01-28 12:19:23 | INFO | train_inner | epoch 139:    312 / 561 symm_kl=0.319, self_kl=0, self_cv=0, loss=3.171, nll_loss=0.936, ppl=1.91, wps=17890.7, ups=1.69, wpb=10602.9, bsz=382.6, num_updates=4800, lr=3.16228e-05, gnorm=0.845, train_wall=59, wall=0
2021-01-28 12:20:23 | INFO | train_inner | epoch 139:    412 / 561 symm_kl=0.319, self_kl=0, self_cv=0, loss=3.178, nll_loss=0.942, ppl=1.92, wps=17610.5, ups=1.69, wpb=10408.6, bsz=363.9, num_updates=4900, lr=3.12984e-05, gnorm=0.844, train_wall=59, wall=0
2021-01-28 12:21:22 | INFO | train_inner | epoch 139:    512 / 561 symm_kl=0.319, self_kl=0, self_cv=0, loss=3.176, nll_loss=0.94, ppl=1.92, wps=17617.7, ups=1.68, wpb=10508.6, bsz=364.6, num_updates=5000, lr=3.09839e-05, gnorm=0.842, train_wall=59, wall=0
2021-01-28 12:21:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 12:21:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:21:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:21:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:21:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:21:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:21:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:21:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:21:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:21:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:21:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:21:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:21:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:21:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:21:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:21:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:21:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:21:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:21:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:21:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:21:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:21:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:21:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:21:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:21:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:21:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:21:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:21:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:21:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:21:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:21:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:21:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:21:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:21:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:21:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:21:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:21:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:22:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:22:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:22:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:22:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:22:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:22:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:22:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:22:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:22:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:22:12 | INFO | valid | epoch 139 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.239 | nll_loss 3.718 | ppl 13.16 | bleu 23.3 | wps 4667.8 | wpb 7508.5 | bsz 272.7 | num_updates 5049 | best_bleu 23.39
2021-01-28 12:22:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 12:22:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:22:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:22:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:22:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:22:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 139 @ 5049 updates, score 23.3) (writing took 3.0836745529668406 seconds)
2021-01-28 12:22:15 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2021-01-28 12:22:15 | INFO | train | epoch 139 | symm_kl 0.316 | self_kl 0 | self_cv 0 | loss 3.162 | nll_loss 0.929 | ppl 1.9 | wps 16366.9 | ups 1.56 | wpb 10483.4 | bsz 369.6 | num_updates 5049 | lr 3.08332e-05 | gnorm 0.839 | train_wall 331 | wall 0
2021-01-28 12:22:15 | INFO | fairseq.trainer | begin training epoch 140
2021-01-28 12:22:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:22:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:22:48 | INFO | train_inner | epoch 140:     51 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.159, nll_loss=0.925, ppl=1.9, wps=12023.6, ups=1.17, wpb=10261.6, bsz=354.5, num_updates=5100, lr=3.06786e-05, gnorm=0.848, train_wall=58, wall=0
2021-01-28 12:23:47 | INFO | train_inner | epoch 140:    151 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.144, nll_loss=0.915, ppl=1.89, wps=17653.1, ups=1.68, wpb=10533.8, bsz=377.5, num_updates=5200, lr=3.03822e-05, gnorm=0.831, train_wall=59, wall=0
2021-01-28 12:24:47 | INFO | train_inner | epoch 140:    251 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.154, nll_loss=0.92, ppl=1.89, wps=17520.8, ups=1.68, wpb=10398.4, bsz=371.2, num_updates=5300, lr=3.00942e-05, gnorm=0.836, train_wall=59, wall=0
2021-01-28 12:25:46 | INFO | train_inner | epoch 140:    351 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.162, nll_loss=0.933, ppl=1.91, wps=17720, ups=1.68, wpb=10563.8, bsz=385.6, num_updates=5400, lr=2.98142e-05, gnorm=0.825, train_wall=59, wall=0
2021-01-28 12:26:46 | INFO | train_inner | epoch 140:    451 / 561 symm_kl=0.32, self_kl=0, self_cv=0, loss=3.175, nll_loss=0.938, ppl=1.92, wps=17692.7, ups=1.69, wpb=10496.5, bsz=352.3, num_updates=5500, lr=2.9542e-05, gnorm=0.84, train_wall=59, wall=0
2021-01-28 12:27:45 | INFO | train_inner | epoch 140:    551 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.162, nll_loss=0.931, ppl=1.91, wps=17541.9, ups=1.67, wpb=10493.1, bsz=361.1, num_updates=5600, lr=2.9277e-05, gnorm=0.845, train_wall=60, wall=0
2021-01-28 12:27:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 12:27:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:27:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:27:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:27:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:27:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:27:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:27:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:27:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:27:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:27:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:27:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:27:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:27:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:27:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:27:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:27:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:27:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:27:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:27:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:27:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:27:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:27:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:27:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:27:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:27:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:27:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:27:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:27:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:27:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:27:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:27:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:27:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:27:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:28:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:28:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:28:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:28:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:28:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:28:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:28:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:28:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:28:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:28:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:28:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:28:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:28:14 | INFO | valid | epoch 140 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.233 | nll_loss 3.713 | ppl 13.11 | bleu 23.27 | wps 4090.1 | wpb 7508.5 | bsz 272.7 | num_updates 5610 | best_bleu 23.39
2021-01-28 12:28:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 12:28:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:28:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:28:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:28:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:28:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 140 @ 5610 updates, score 23.27) (writing took 3.19665002200054 seconds)
2021-01-28 12:28:18 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2021-01-28 12:28:18 | INFO | train | epoch 140 | symm_kl 0.315 | self_kl 0 | self_cv 0 | loss 3.158 | nll_loss 0.926 | ppl 1.9 | wps 16223.5 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 5610 | lr 2.92509e-05 | gnorm 0.836 | train_wall 332 | wall 0
2021-01-28 12:28:18 | INFO | fairseq.trainer | begin training epoch 141
2021-01-28 12:28:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:28:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:29:13 | INFO | train_inner | epoch 141:     90 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.161, nll_loss=0.93, ppl=1.91, wps=11793.3, ups=1.14, wpb=10343.4, bsz=381, num_updates=5700, lr=2.90191e-05, gnorm=0.838, train_wall=58, wall=0
2021-01-28 12:30:13 | INFO | train_inner | epoch 141:    190 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.147, nll_loss=0.918, ppl=1.89, wps=17515.2, ups=1.67, wpb=10478.4, bsz=365, num_updates=5800, lr=2.87678e-05, gnorm=0.832, train_wall=60, wall=0
2021-01-28 12:31:12 | INFO | train_inner | epoch 141:    290 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.161, nll_loss=0.929, ppl=1.9, wps=17736.2, ups=1.69, wpb=10508.1, bsz=372.2, num_updates=5900, lr=2.8523e-05, gnorm=0.83, train_wall=59, wall=0
2021-01-28 12:32:12 | INFO | train_inner | epoch 141:    390 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.138, nll_loss=0.912, ppl=1.88, wps=17724.7, ups=1.68, wpb=10573.9, bsz=371.7, num_updates=6000, lr=2.82843e-05, gnorm=0.823, train_wall=59, wall=0
2021-01-28 12:33:11 | INFO | train_inner | epoch 141:    490 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.16, nll_loss=0.93, ppl=1.9, wps=17790.6, ups=1.68, wpb=10560.9, bsz=383.4, num_updates=6100, lr=2.80515e-05, gnorm=0.83, train_wall=59, wall=0
2021-01-28 12:33:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 12:33:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:33:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:33:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:33:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:33:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:33:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:33:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:33:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:33:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:33:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:33:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:33:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:34:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:34:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:34:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:34:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:34:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:34:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:34:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:34:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:34:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:34:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:34:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:34:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:34:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:34:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:34:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:34:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:34:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:34:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:34:14 | INFO | valid | epoch 141 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.235 | nll_loss 3.718 | ppl 13.16 | bleu 23.15 | wps 4660.6 | wpb 7508.5 | bsz 272.7 | num_updates 6171 | best_bleu 23.39
2021-01-28 12:34:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 12:34:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:34:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:34:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:34:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:34:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 141 @ 6171 updates, score 23.15) (writing took 3.0931610710103996 seconds)
2021-01-28 12:34:17 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2021-01-28 12:34:17 | INFO | train | epoch 141 | symm_kl 0.314 | self_kl 0 | self_cv 0 | loss 3.156 | nll_loss 0.925 | ppl 1.9 | wps 16355.4 | ups 1.56 | wpb 10483.4 | bsz 369.6 | num_updates 6171 | lr 2.78896e-05 | gnorm 0.832 | train_wall 331 | wall 0
2021-01-28 12:34:17 | INFO | fairseq.trainer | begin training epoch 142
2021-01-28 12:34:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:34:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:34:37 | INFO | train_inner | epoch 142:     29 / 561 symm_kl=0.32, self_kl=0, self_cv=0, loss=3.163, nll_loss=0.923, ppl=1.9, wps=12073.5, ups=1.16, wpb=10398.4, bsz=348.5, num_updates=6200, lr=2.78243e-05, gnorm=0.855, train_wall=59, wall=0
2021-01-28 12:35:36 | INFO | train_inner | epoch 142:    129 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.149, nll_loss=0.924, ppl=1.9, wps=17983.9, ups=1.7, wpb=10591.3, bsz=385.4, num_updates=6300, lr=2.76026e-05, gnorm=0.823, train_wall=59, wall=0
2021-01-28 12:36:36 | INFO | train_inner | epoch 142:    229 / 561 symm_kl=0.318, self_kl=0, self_cv=0, loss=3.165, nll_loss=0.929, ppl=1.9, wps=17583.4, ups=1.68, wpb=10453.7, bsz=371.2, num_updates=6400, lr=2.73861e-05, gnorm=0.845, train_wall=59, wall=0
2021-01-28 12:37:35 | INFO | train_inner | epoch 142:    329 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.137, nll_loss=0.912, ppl=1.88, wps=17661.7, ups=1.67, wpb=10558.3, bsz=367.9, num_updates=6500, lr=2.71746e-05, gnorm=0.829, train_wall=60, wall=0
2021-01-28 12:38:35 | INFO | train_inner | epoch 142:    429 / 561 symm_kl=0.316, self_kl=0, self_cv=0, loss=3.16, nll_loss=0.927, ppl=1.9, wps=17762.3, ups=1.68, wpb=10549.5, bsz=358.6, num_updates=6600, lr=2.6968e-05, gnorm=0.844, train_wall=59, wall=0
2021-01-28 12:39:34 | INFO | train_inner | epoch 142:    529 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.158, nll_loss=0.928, ppl=1.9, wps=17380.5, ups=1.68, wpb=10318.2, bsz=371.8, num_updates=6700, lr=2.6766e-05, gnorm=0.851, train_wall=59, wall=0
2021-01-28 12:39:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 12:39:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:39:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:39:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:39:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:39:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:39:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:39:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:39:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:39:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:39:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:39:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:39:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:39:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:39:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:39:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:39:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:39:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:39:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:39:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:39:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:39:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:39:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:39:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:39:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:40:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:40:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:40:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:40:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:40:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:40:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:40:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:40:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:40:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:40:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:40:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:40:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:40:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:40:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:40:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:40:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:40:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:40:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:40:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:40:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:40:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:40:14 | INFO | valid | epoch 142 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.232 | nll_loss 3.714 | ppl 13.12 | bleu 23.05 | wps 4621 | wpb 7508.5 | bsz 272.7 | num_updates 6732 | best_bleu 23.39
2021-01-28 12:40:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 12:40:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:40:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:40:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 142 @ 6732 updates, score 23.05) (writing took 3.1585861410130747 seconds)
2021-01-28 12:40:17 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2021-01-28 12:40:17 | INFO | train | epoch 142 | symm_kl 0.314 | self_kl 0 | self_cv 0 | loss 3.154 | nll_loss 0.924 | ppl 1.9 | wps 16322.6 | ups 1.56 | wpb 10483.4 | bsz 369.6 | num_updates 6732 | lr 2.67023e-05 | gnorm 0.84 | train_wall 331 | wall 0
2021-01-28 12:40:17 | INFO | fairseq.trainer | begin training epoch 143
2021-01-28 12:40:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:40:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:40:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:40:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:41:00 | INFO | train_inner | epoch 143:     68 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.126, nll_loss=0.903, ppl=1.87, wps=12146.2, ups=1.16, wpb=10476.3, bsz=369.8, num_updates=6800, lr=2.65684e-05, gnorm=0.831, train_wall=58, wall=0
2021-01-28 12:41:59 | INFO | train_inner | epoch 143:    168 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.16, nll_loss=0.93, ppl=1.91, wps=17619.1, ups=1.7, wpb=10389.7, bsz=368.8, num_updates=6900, lr=2.63752e-05, gnorm=0.829, train_wall=59, wall=0
2021-01-28 12:42:59 | INFO | train_inner | epoch 143:    268 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.148, nll_loss=0.916, ppl=1.89, wps=17921.6, ups=1.68, wpb=10643.2, bsz=358.4, num_updates=7000, lr=2.61861e-05, gnorm=0.84, train_wall=59, wall=0
2021-01-28 12:43:58 | INFO | train_inner | epoch 143:    368 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.152, nll_loss=0.924, ppl=1.9, wps=17705.6, ups=1.68, wpb=10535.7, bsz=369.2, num_updates=7100, lr=2.60011e-05, gnorm=0.829, train_wall=59, wall=0
2021-01-28 12:44:58 | INFO | train_inner | epoch 143:    468 / 561 symm_kl=0.315, self_kl=0, self_cv=0, loss=3.164, nll_loss=0.935, ppl=1.91, wps=17549.5, ups=1.69, wpb=10408.3, bsz=378.8, num_updates=7200, lr=2.58199e-05, gnorm=0.835, train_wall=59, wall=0
2021-01-28 12:45:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 12:45:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:45:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:45:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:45:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:45:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:45:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:45:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:45:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:45:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:45:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:45:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:45:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:45:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:45:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:45:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:46:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:46:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:46:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:46:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:46:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:46:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:46:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:46:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:46:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:46:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:46:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:46:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:46:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:46:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:46:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:46:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:46:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:46:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:46:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:46:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:46:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:46:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:46:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:46:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:46:17 | INFO | valid | epoch 143 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.233 | nll_loss 3.713 | ppl 13.11 | bleu 23.13 | wps 4107.3 | wpb 7508.5 | bsz 272.7 | num_updates 7293 | best_bleu 23.39
2021-01-28 12:46:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 12:46:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:46:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:46:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 143 @ 7293 updates, score 23.13) (writing took 3.1102511240169406 seconds)
2021-01-28 12:46:20 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2021-01-28 12:46:20 | INFO | train | epoch 143 | symm_kl 0.313 | self_kl 0 | self_cv 0 | loss 3.151 | nll_loss 0.922 | ppl 1.9 | wps 16238.5 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 7293 | lr 2.56547e-05 | gnorm 0.833 | train_wall 331 | wall 0
2021-01-28 12:46:20 | INFO | fairseq.trainer | begin training epoch 144
2021-01-28 12:46:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:46:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:46:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:46:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:46:27 | INFO | train_inner | epoch 144:      7 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.157, nll_loss=0.928, ppl=1.9, wps=11586.1, ups=1.12, wpb=10383.6, bsz=369.2, num_updates=7300, lr=2.56424e-05, gnorm=0.842, train_wall=59, wall=0
2021-01-28 12:47:26 | INFO | train_inner | epoch 144:    107 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.138, nll_loss=0.914, ppl=1.88, wps=18105.1, ups=1.7, wpb=10620.5, bsz=391.1, num_updates=7400, lr=2.54686e-05, gnorm=0.809, train_wall=58, wall=0
2021-01-28 12:48:26 | INFO | train_inner | epoch 144:    207 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.139, nll_loss=0.914, ppl=1.88, wps=17666.1, ups=1.68, wpb=10536.3, bsz=364.5, num_updates=7500, lr=2.52982e-05, gnorm=0.82, train_wall=59, wall=0
2021-01-28 12:49:25 | INFO | train_inner | epoch 144:    307 / 561 symm_kl=0.317, self_kl=0, self_cv=0, loss=3.165, nll_loss=0.93, ppl=1.91, wps=17530.8, ups=1.69, wpb=10399.7, bsz=357.6, num_updates=7600, lr=2.51312e-05, gnorm=0.841, train_wall=59, wall=0
2021-01-28 12:50:24 | INFO | train_inner | epoch 144:    407 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.148, nll_loss=0.921, ppl=1.89, wps=17550.3, ups=1.68, wpb=10420.3, bsz=387.7, num_updates=7700, lr=2.49675e-05, gnorm=0.833, train_wall=59, wall=0
2021-01-28 12:51:24 | INFO | train_inner | epoch 144:    507 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.142, nll_loss=0.915, ppl=1.89, wps=17435.2, ups=1.66, wpb=10505.4, bsz=354.7, num_updates=7800, lr=2.48069e-05, gnorm=0.835, train_wall=60, wall=0
2021-01-28 12:51:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 12:51:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:51:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:51:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:52:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:52:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:52:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:52:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:52:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:52:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:52:21 | INFO | valid | epoch 144 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.235 | nll_loss 3.716 | ppl 13.14 | bleu 23.04 | wps 4052.3 | wpb 7508.5 | bsz 272.7 | num_updates 7854 | best_bleu 23.39
2021-01-28 12:52:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 12:52:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:52:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:52:24 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 144 @ 7854 updates, score 23.04) (writing took 3.0574689019704238 seconds)
2021-01-28 12:52:24 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2021-01-28 12:52:24 | INFO | train | epoch 144 | symm_kl 0.312 | self_kl 0 | self_cv 0 | loss 3.148 | nll_loss 0.92 | ppl 1.89 | wps 16150.5 | ups 1.54 | wpb 10483.4 | bsz 369.6 | num_updates 7854 | lr 2.47215e-05 | gnorm 0.831 | train_wall 332 | wall 0
2021-01-28 12:52:24 | INFO | fairseq.trainer | begin training epoch 145
2021-01-28 12:52:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:52:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:52:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:52:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:52:54 | INFO | train_inner | epoch 145:     46 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.15, nll_loss=0.923, ppl=1.9, wps=11558.9, ups=1.12, wpb=10324.9, bsz=368.4, num_updates=7900, lr=2.46494e-05, gnorm=0.85, train_wall=58, wall=0
2021-01-28 12:53:53 | INFO | train_inner | epoch 145:    146 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.132, nll_loss=0.909, ppl=1.88, wps=17792.7, ups=1.69, wpb=10540.3, bsz=372.5, num_updates=8000, lr=2.44949e-05, gnorm=0.824, train_wall=59, wall=0
2021-01-28 12:54:53 | INFO | train_inner | epoch 145:    246 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.142, nll_loss=0.917, ppl=1.89, wps=17677.1, ups=1.67, wpb=10604.9, bsz=385.2, num_updates=8100, lr=2.43432e-05, gnorm=0.816, train_wall=60, wall=0
2021-01-28 12:55:52 | INFO | train_inner | epoch 145:    346 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.154, nll_loss=0.922, ppl=1.9, wps=17627.5, ups=1.68, wpb=10475.5, bsz=362, num_updates=8200, lr=2.41943e-05, gnorm=0.837, train_wall=59, wall=0
2021-01-28 12:56:52 | INFO | train_inner | epoch 145:    446 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.152, nll_loss=0.921, ppl=1.89, wps=17460.7, ups=1.67, wpb=10426.6, bsz=363.8, num_updates=8300, lr=2.40481e-05, gnorm=0.838, train_wall=60, wall=0
2021-01-28 12:57:51 | INFO | train_inner | epoch 145:    546 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.165, nll_loss=0.935, ppl=1.91, wps=17661.4, ups=1.69, wpb=10458.1, bsz=356.2, num_updates=8400, lr=2.39046e-05, gnorm=0.836, train_wall=59, wall=0
2021-01-28 12:58:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 12:58:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:58:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:58:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:58:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:58:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:58:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:58:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 12:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 12:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 12:58:23 | INFO | valid | epoch 145 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.23 | nll_loss 3.712 | ppl 13.11 | bleu 23.2 | wps 4653.1 | wpb 7508.5 | bsz 272.7 | num_updates 8415 | best_bleu 23.39
2021-01-28 12:58:23 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 12:58:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:58:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:58:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 145 @ 8415 updates, score 23.2) (writing took 2.942920310946647 seconds)
2021-01-28 12:58:26 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2021-01-28 12:58:26 | INFO | train | epoch 145 | symm_kl 0.311 | self_kl 0 | self_cv 0 | loss 3.147 | nll_loss 0.919 | ppl 1.89 | wps 16227.9 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 8415 | lr 2.38833e-05 | gnorm 0.831 | train_wall 332 | wall 0
2021-01-28 12:58:26 | INFO | fairseq.trainer | begin training epoch 146
2021-01-28 12:58:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 12:58:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:58:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:58:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 12:59:20 | INFO | train_inner | epoch 146:     85 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.148, nll_loss=0.918, ppl=1.89, wps=11811.7, ups=1.13, wpb=10407.9, bsz=368.7, num_updates=8500, lr=2.37635e-05, gnorm=0.839, train_wall=58, wall=0
2021-01-28 13:00:19 | INFO | train_inner | epoch 146:    185 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.137, nll_loss=0.911, ppl=1.88, wps=17615.1, ups=1.68, wpb=10494.3, bsz=369.5, num_updates=8600, lr=2.3625e-05, gnorm=0.83, train_wall=59, wall=0
2021-01-28 13:01:19 | INFO | train_inner | epoch 146:    285 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.15, nll_loss=0.923, ppl=1.9, wps=17692, ups=1.68, wpb=10514.5, bsz=377.3, num_updates=8700, lr=2.34888e-05, gnorm=0.829, train_wall=59, wall=0
2021-01-28 13:02:18 | INFO | train_inner | epoch 146:    385 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.152, nll_loss=0.922, ppl=1.89, wps=17654.7, ups=1.67, wpb=10547.8, bsz=360.3, num_updates=8800, lr=2.3355e-05, gnorm=0.828, train_wall=60, wall=0
2021-01-28 13:03:18 | INFO | train_inner | epoch 146:    485 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.134, nll_loss=0.912, ppl=1.88, wps=17620, ups=1.68, wpb=10511.5, bsz=373.9, num_updates=8900, lr=2.32234e-05, gnorm=0.834, train_wall=59, wall=0
2021-01-28 13:04:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 13:04:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:04:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:04:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:04:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:04:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:04:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:04:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:04:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:04:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:04:27 | INFO | valid | epoch 146 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.233 | nll_loss 3.715 | ppl 13.13 | bleu 23.19 | wps 4636.8 | wpb 7508.5 | bsz 272.7 | num_updates 8976 | best_bleu 23.39
2021-01-28 13:04:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 13:04:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:04:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:04:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 146 @ 8976 updates, score 23.19) (writing took 3.067376865015831 seconds)
2021-01-28 13:04:30 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2021-01-28 13:04:30 | INFO | train | epoch 146 | symm_kl 0.311 | self_kl 0 | self_cv 0 | loss 3.144 | nll_loss 0.917 | ppl 1.89 | wps 16180.9 | ups 1.54 | wpb 10483.4 | bsz 369.6 | num_updates 8976 | lr 2.31249e-05 | gnorm 0.832 | train_wall 332 | wall 0
2021-01-28 13:04:30 | INFO | fairseq.trainer | begin training epoch 147
2021-01-28 13:04:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:04:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:04:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:04:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:04:48 | INFO | train_inner | epoch 147:     24 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.139, nll_loss=0.914, ppl=1.88, wps=11641.8, ups=1.12, wpb=10439.5, bsz=376.5, num_updates=9000, lr=2.3094e-05, gnorm=0.836, train_wall=59, wall=0
2021-01-28 13:05:47 | INFO | train_inner | epoch 147:    124 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.12, nll_loss=0.899, ppl=1.86, wps=17810.1, ups=1.69, wpb=10569.4, bsz=371.8, num_updates=9100, lr=2.29668e-05, gnorm=0.813, train_wall=59, wall=0
2021-01-28 13:06:46 | INFO | train_inner | epoch 147:    224 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.14, nll_loss=0.915, ppl=1.89, wps=17780.2, ups=1.68, wpb=10562.6, bsz=363.8, num_updates=9200, lr=2.28416e-05, gnorm=0.823, train_wall=59, wall=0
2021-01-28 13:07:49 | INFO | train_inner | epoch 147:    324 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.149, nll_loss=0.917, ppl=1.89, wps=16701.4, ups=1.61, wpb=10401.4, bsz=363.9, num_updates=9300, lr=2.27185e-05, gnorm=0.84, train_wall=62, wall=0
2021-01-28 13:08:51 | INFO | train_inner | epoch 147:    424 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.151, nll_loss=0.923, ppl=1.9, wps=16897.1, ups=1.6, wpb=10538.8, bsz=360, num_updates=9400, lr=2.25973e-05, gnorm=0.827, train_wall=62, wall=0
2021-01-28 13:09:51 | INFO | train_inner | epoch 147:    524 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.142, nll_loss=0.919, ppl=1.89, wps=17634.2, ups=1.68, wpb=10491.2, bsz=372.1, num_updates=9500, lr=2.24781e-05, gnorm=0.825, train_wall=59, wall=0
2021-01-28 13:10:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 13:10:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:10:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:10:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:10:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:10:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:10:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:10:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:10:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:10:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:10:36 | INFO | valid | epoch 147 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.228 | nll_loss 3.711 | ppl 13.1 | bleu 23.15 | wps 4079.2 | wpb 7508.5 | bsz 272.7 | num_updates 9537 | best_bleu 23.39
2021-01-28 13:10:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 13:10:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:10:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:10:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 147 @ 9537 updates, score 23.15) (writing took 2.9910412689787336 seconds)
2021-01-28 13:10:39 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2021-01-28 13:10:39 | INFO | train | epoch 147 | symm_kl 0.31 | self_kl 0 | self_cv 0 | loss 3.142 | nll_loss 0.916 | ppl 1.89 | wps 15902.6 | ups 1.52 | wpb 10483.4 | bsz 369.6 | num_updates 9537 | lr 2.24344e-05 | gnorm 0.83 | train_wall 338 | wall 0
2021-01-28 13:10:39 | INFO | fairseq.trainer | begin training epoch 148
2021-01-28 13:10:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:10:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:10:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:10:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:11:20 | INFO | train_inner | epoch 148:     63 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.146, nll_loss=0.919, ppl=1.89, wps=11519.7, ups=1.12, wpb=10270.6, bsz=378.4, num_updates=9600, lr=2.23607e-05, gnorm=0.842, train_wall=58, wall=0
2021-01-28 13:12:19 | INFO | train_inner | epoch 148:    163 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.142, nll_loss=0.917, ppl=1.89, wps=17464.1, ups=1.68, wpb=10410.3, bsz=371.5, num_updates=9700, lr=2.22451e-05, gnorm=0.836, train_wall=59, wall=0
2021-01-28 13:13:19 | INFO | train_inner | epoch 148:    263 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.125, nll_loss=0.901, ppl=1.87, wps=17607.7, ups=1.66, wpb=10578.5, bsz=380.9, num_updates=9800, lr=2.21313e-05, gnorm=0.826, train_wall=60, wall=0
2021-01-28 13:14:19 | INFO | train_inner | epoch 148:    363 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.135, nll_loss=0.914, ppl=1.88, wps=17707.8, ups=1.68, wpb=10532.1, bsz=365.5, num_updates=9900, lr=2.20193e-05, gnorm=0.827, train_wall=59, wall=0
2021-01-28 13:15:19 | INFO | train_inner | epoch 148:    463 / 561 symm_kl=0.314, self_kl=0, self_cv=0, loss=3.158, nll_loss=0.929, ppl=1.9, wps=17417, ups=1.67, wpb=10454.6, bsz=369.4, num_updates=10000, lr=2.19089e-05, gnorm=0.836, train_wall=60, wall=0
2021-01-28 13:16:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 13:16:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:16:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:16:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:16:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:16:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:16:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:16:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:16:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:16:41 | INFO | valid | epoch 148 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.233 | nll_loss 3.713 | ppl 13.12 | bleu 23.16 | wps 4646.9 | wpb 7508.5 | bsz 272.7 | num_updates 10098 | best_bleu 23.39
2021-01-28 13:16:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 13:16:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:16:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:16:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 148 @ 10098 updates, score 23.16) (writing took 3.0277990780305117 seconds)
2021-01-28 13:16:44 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2021-01-28 13:16:44 | INFO | train | epoch 148 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.14 | nll_loss 0.915 | ppl 1.89 | wps 16150 | ups 1.54 | wpb 10483.4 | bsz 369.6 | num_updates 10098 | lr 2.18023e-05 | gnorm 0.831 | train_wall 333 | wall 0
2021-01-28 13:16:44 | INFO | fairseq.trainer | begin training epoch 149
2021-01-28 13:16:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:16:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:16:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:16:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:16:49 | INFO | train_inner | epoch 149:      2 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.142, nll_loss=0.917, ppl=1.89, wps=11662.9, ups=1.11, wpb=10489.8, bsz=365, num_updates=10100, lr=2.18002e-05, gnorm=0.835, train_wall=60, wall=0
2021-01-28 13:17:50 | INFO | train_inner | epoch 149:    102 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.147, nll_loss=0.919, ppl=1.89, wps=17060.4, ups=1.63, wpb=10463.3, bsz=364.3, num_updates=10200, lr=2.1693e-05, gnorm=0.834, train_wall=61, wall=0
2021-01-28 13:18:52 | INFO | train_inner | epoch 149:    202 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.141, nll_loss=0.912, ppl=1.88, wps=16638.6, ups=1.62, wpb=10271, bsz=359, num_updates=10300, lr=2.15875e-05, gnorm=0.845, train_wall=61, wall=0
2021-01-28 13:19:52 | INFO | train_inner | epoch 149:    302 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.138, nll_loss=0.915, ppl=1.89, wps=17725.9, ups=1.67, wpb=10594.7, bsz=379, num_updates=10400, lr=2.14834e-05, gnorm=0.826, train_wall=60, wall=0
2021-01-28 13:20:52 | INFO | train_inner | epoch 149:    402 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.132, nll_loss=0.909, ppl=1.88, wps=17690.9, ups=1.66, wpb=10664.2, bsz=364.6, num_updates=10500, lr=2.13809e-05, gnorm=0.832, train_wall=60, wall=0
2021-01-28 13:21:58 | INFO | train_inner | epoch 149:    502 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.141, nll_loss=0.914, ppl=1.88, wps=15794.1, ups=1.51, wpb=10444.4, bsz=369.7, num_updates=10600, lr=2.12798e-05, gnorm=0.833, train_wall=66, wall=0
2021-01-28 13:22:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 13:22:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:22:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:22:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:22:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:22:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:22:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:22:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:22:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:22:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:22:56 | INFO | valid | epoch 149 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.229 | nll_loss 3.712 | ppl 13.11 | bleu 23.14 | wps 4639.7 | wpb 7508.5 | bsz 272.7 | num_updates 10659 | best_bleu 23.39
2021-01-28 13:22:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 13:22:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:22:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:22:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:22:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:22:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 149 @ 10659 updates, score 23.14) (writing took 2.9689295789576136 seconds)
2021-01-28 13:22:59 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2021-01-28 13:22:59 | INFO | train | epoch 149 | symm_kl 0.309 | self_kl 0 | self_cv 0 | loss 3.139 | nll_loss 0.914 | ppl 1.88 | wps 15656.7 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 10659 | lr 2.12208e-05 | gnorm 0.833 | train_wall 345 | wall 0
2021-01-28 13:22:59 | INFO | fairseq.trainer | begin training epoch 150
2021-01-28 13:23:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:23:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:23:26 | INFO | train_inner | epoch 150:     41 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.131, nll_loss=0.911, ppl=1.88, wps=11863.4, ups=1.14, wpb=10452.2, bsz=381.3, num_updates=10700, lr=2.11801e-05, gnorm=0.818, train_wall=60, wall=0
2021-01-28 13:24:26 | INFO | train_inner | epoch 150:    141 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.132, nll_loss=0.908, ppl=1.88, wps=17497.5, ups=1.67, wpb=10493.5, bsz=371, num_updates=10800, lr=2.10819e-05, gnorm=0.834, train_wall=60, wall=0
2021-01-28 13:25:33 | INFO | train_inner | epoch 150:    241 / 561 symm_kl=0.313, self_kl=0, self_cv=0, loss=3.155, nll_loss=0.925, ppl=1.9, wps=15582.1, ups=1.5, wpb=10404.4, bsz=362.6, num_updates=10900, lr=2.09849e-05, gnorm=0.839, train_wall=66, wall=0
2021-01-28 13:26:38 | INFO | train_inner | epoch 150:    341 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.134, nll_loss=0.912, ppl=1.88, wps=15911.1, ups=1.53, wpb=10425, bsz=369.8, num_updates=11000, lr=2.08893e-05, gnorm=0.822, train_wall=65, wall=0
2021-01-28 13:27:45 | INFO | train_inner | epoch 150:    441 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.123, nll_loss=0.907, ppl=1.88, wps=16100.8, ups=1.51, wpb=10662.2, bsz=394.5, num_updates=11100, lr=2.0795e-05, gnorm=0.808, train_wall=66, wall=0
2021-01-28 13:28:46 | INFO | train_inner | epoch 150:    541 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.146, nll_loss=0.918, ppl=1.89, wps=17059.5, ups=1.63, wpb=10490.6, bsz=357.6, num_updates=11200, lr=2.0702e-05, gnorm=0.834, train_wall=61, wall=0
2021-01-28 13:28:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 13:28:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:28:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:28:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:29:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:29:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:29:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:29:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:29:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:29:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:29:20 | INFO | valid | epoch 150 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.228 | nll_loss 3.71 | ppl 13.09 | bleu 23.25 | wps 4528.9 | wpb 7508.5 | bsz 272.7 | num_updates 11220 | best_bleu 23.39
2021-01-28 13:29:20 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 13:29:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:29:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:29:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 150 @ 11220 updates, score 23.25) (writing took 3.0882262329687364 seconds)
2021-01-28 13:29:23 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2021-01-28 13:29:24 | INFO | train | epoch 150 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 3.137 | nll_loss 0.913 | ppl 1.88 | wps 15305.9 | ups 1.46 | wpb 10483.4 | bsz 369.6 | num_updates 11220 | lr 2.06835e-05 | gnorm 0.828 | train_wall 353 | wall 0
2021-01-28 13:29:24 | INFO | fairseq.trainer | begin training epoch 151
2021-01-28 13:29:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:29:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:29:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:29:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:30:14 | INFO | train_inner | epoch 151:     80 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.118, nll_loss=0.898, ppl=1.86, wps=11949.7, ups=1.14, wpb=10481.2, bsz=365.2, num_updates=11300, lr=2.06102e-05, gnorm=0.829, train_wall=58, wall=0
2021-01-28 13:31:15 | INFO | train_inner | epoch 151:    180 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.144, nll_loss=0.916, ppl=1.89, wps=17240.5, ups=1.64, wpb=10484.9, bsz=361.9, num_updates=11400, lr=2.05196e-05, gnorm=0.835, train_wall=61, wall=0
2021-01-28 13:32:19 | INFO | train_inner | epoch 151:    280 / 561 symm_kl=0.311, self_kl=0, self_cv=0, loss=3.139, nll_loss=0.91, ppl=1.88, wps=16059.2, ups=1.55, wpb=10389.3, bsz=354, num_updates=11500, lr=2.04302e-05, gnorm=0.835, train_wall=64, wall=0
2021-01-28 13:33:26 | INFO | train_inner | epoch 151:    380 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.129, nll_loss=0.908, ppl=1.88, wps=15663.3, ups=1.5, wpb=10426.9, bsz=384.3, num_updates=11600, lr=2.03419e-05, gnorm=0.827, train_wall=66, wall=0
2021-01-28 13:34:33 | INFO | train_inner | epoch 151:    480 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.137, nll_loss=0.916, ppl=1.89, wps=15777, ups=1.48, wpb=10659.9, bsz=376, num_updates=11700, lr=2.02548e-05, gnorm=0.825, train_wall=67, wall=0
2021-01-28 13:35:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 13:35:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:35:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:35:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:35:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:35:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:35:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:35:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:35:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:35:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:35:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:35:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:35:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:35:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:35:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:35:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:35:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:35:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:35:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:35:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:35:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:35:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:35:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:35:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:35:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:35:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:35:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:35:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:36:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:36:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:36:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:36:04 | INFO | valid | epoch 151 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.232 | nll_loss 3.713 | ppl 13.12 | bleu 23.28 | wps 2936.3 | wpb 7508.5 | bsz 272.7 | num_updates 11781 | best_bleu 23.39
2021-01-28 13:36:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 13:36:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:36:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:36:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:36:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:36:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 151 @ 11781 updates, score 23.28) (writing took 3.1236448199488223 seconds)
2021-01-28 13:36:07 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2021-01-28 13:36:07 | INFO | train | epoch 151 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 3.136 | nll_loss 0.912 | ppl 1.88 | wps 14564.9 | ups 1.39 | wpb 10483.4 | bsz 369.6 | num_updates 11781 | lr 2.0185e-05 | gnorm 0.83 | train_wall 359 | wall 0
2021-01-28 13:36:07 | INFO | fairseq.trainer | begin training epoch 152
2021-01-28 13:36:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:36:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:36:27 | INFO | train_inner | epoch 152:     19 / 561 symm_kl=0.31, self_kl=0, self_cv=0, loss=3.143, nll_loss=0.919, ppl=1.89, wps=9195.6, ups=0.88, wpb=10410.8, bsz=364, num_updates=11800, lr=2.01688e-05, gnorm=0.837, train_wall=66, wall=0
2021-01-28 13:37:32 | INFO | train_inner | epoch 152:    119 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.12, nll_loss=0.898, ppl=1.86, wps=16145.2, ups=1.53, wpb=10566.8, bsz=368.9, num_updates=11900, lr=2.00839e-05, gnorm=0.814, train_wall=65, wall=0
2021-01-28 13:38:39 | INFO | train_inner | epoch 152:    219 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.129, nll_loss=0.909, ppl=1.88, wps=15622.5, ups=1.49, wpb=10477.5, bsz=375.9, num_updates=12000, lr=2e-05, gnorm=0.818, train_wall=66, wall=0
2021-01-28 13:39:46 | INFO | train_inner | epoch 152:    319 / 561 symm_kl=0.312, self_kl=0, self_cv=0, loss=3.148, nll_loss=0.918, ppl=1.89, wps=15793.1, ups=1.51, wpb=10467.6, bsz=362.5, num_updates=12100, lr=1.99172e-05, gnorm=0.834, train_wall=66, wall=0
2021-01-28 13:40:54 | INFO | train_inner | epoch 152:    419 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.139, nll_loss=0.916, ppl=1.89, wps=15362.5, ups=1.46, wpb=10529.3, bsz=364.6, num_updates=12200, lr=1.98354e-05, gnorm=0.826, train_wall=68, wall=0
2021-01-28 13:42:02 | INFO | train_inner | epoch 152:    519 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.137, nll_loss=0.916, ppl=1.89, wps=15535.7, ups=1.48, wpb=10511.6, bsz=381.3, num_updates=12300, lr=1.97546e-05, gnorm=0.828, train_wall=67, wall=0
2021-01-28 13:42:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 13:42:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:42:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:42:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:42:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:42:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:42:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:42:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:42:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:42:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:42:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:42:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:42:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:42:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:42:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:42:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:42:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:42:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:42:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:42:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:42:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:42:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:42:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:42:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:42:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:42:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:42:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:42:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:42:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:42:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:42:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:42:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:42:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:42:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:42:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:42:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:42:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:42:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:42:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:42:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:43:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:43:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:43:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:43:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:43:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:43:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:43:03 | INFO | valid | epoch 152 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.223 | nll_loss 3.709 | ppl 13.08 | bleu 23.12 | wps 3174.6 | wpb 7508.5 | bsz 272.7 | num_updates 12342 | best_bleu 23.39
2021-01-28 13:43:03 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 13:43:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:43:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:43:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 152 @ 12342 updates, score 23.12) (writing took 3.3380813380354084 seconds)
2021-01-28 13:43:07 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2021-01-28 13:43:07 | INFO | train | epoch 152 | symm_kl 0.308 | self_kl 0 | self_cv 0 | loss 3.134 | nll_loss 0.911 | ppl 1.88 | wps 14028.3 | ups 1.34 | wpb 10483.4 | bsz 369.6 | num_updates 12342 | lr 1.9721e-05 | gnorm 0.828 | train_wall 373 | wall 0
2021-01-28 13:43:07 | INFO | fairseq.trainer | begin training epoch 153
2021-01-28 13:43:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:43:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:43:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:43:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:43:52 | INFO | train_inner | epoch 153:     58 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.127, nll_loss=0.904, ppl=1.87, wps=9353.2, ups=0.91, wpb=10319.1, bsz=366.2, num_updates=12400, lr=1.96748e-05, gnorm=0.852, train_wall=67, wall=0
2021-01-28 13:44:59 | INFO | train_inner | epoch 153:    158 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.132, nll_loss=0.911, ppl=1.88, wps=15534.5, ups=1.5, wpb=10376.7, bsz=380.5, num_updates=12500, lr=1.95959e-05, gnorm=0.825, train_wall=66, wall=0
2021-01-28 13:46:07 | INFO | train_inner | epoch 153:    258 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.138, nll_loss=0.913, ppl=1.88, wps=15491.3, ups=1.47, wpb=10544.8, bsz=371.1, num_updates=12600, lr=1.9518e-05, gnorm=0.83, train_wall=68, wall=0
2021-01-28 13:47:10 | INFO | train_inner | epoch 153:    358 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.142, nll_loss=0.918, ppl=1.89, wps=16657.1, ups=1.58, wpb=10526.6, bsz=370, num_updates=12700, lr=1.9441e-05, gnorm=0.837, train_wall=63, wall=0
2021-01-28 13:48:10 | INFO | train_inner | epoch 153:    458 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.142, nll_loss=0.918, ppl=1.89, wps=17501.8, ups=1.68, wpb=10444.2, bsz=358.6, num_updates=12800, lr=1.93649e-05, gnorm=0.828, train_wall=59, wall=0
2021-01-28 13:49:13 | INFO | train_inner | epoch 153:    558 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.123, nll_loss=0.901, ppl=1.87, wps=16802, ups=1.58, wpb=10606.9, bsz=372.4, num_updates=12900, lr=1.92897e-05, gnorm=0.828, train_wall=63, wall=0
2021-01-28 13:49:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 13:49:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:49:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:49:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:49:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:49:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:49:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:49:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:49:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:49:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:49:47 | INFO | valid | epoch 153 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.234 | nll_loss 3.718 | ppl 13.16 | bleu 23.13 | wps 2963.4 | wpb 7508.5 | bsz 272.7 | num_updates 12903 | best_bleu 23.39
2021-01-28 13:49:47 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 13:49:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:49:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:49:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 153 @ 12903 updates, score 23.13) (writing took 3.8811037780251354 seconds)
2021-01-28 13:49:51 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2021-01-28 13:49:51 | INFO | train | epoch 153 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 3.133 | nll_loss 0.91 | ppl 1.88 | wps 14534.4 | ups 1.39 | wpb 10483.4 | bsz 369.6 | num_updates 12903 | lr 1.92875e-05 | gnorm 0.831 | train_wall 359 | wall 0
2021-01-28 13:49:51 | INFO | fairseq.trainer | begin training epoch 154
2021-01-28 13:49:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:49:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:49:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:49:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:50:59 | INFO | train_inner | epoch 154:     97 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.125, nll_loss=0.904, ppl=1.87, wps=9799.8, ups=0.94, wpb=10431.4, bsz=368.6, num_updates=13000, lr=1.92154e-05, gnorm=0.821, train_wall=64, wall=0
2021-01-28 13:52:06 | INFO | train_inner | epoch 154:    197 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.131, nll_loss=0.906, ppl=1.87, wps=15814.7, ups=1.49, wpb=10582, bsz=356.1, num_updates=13100, lr=1.91419e-05, gnorm=0.824, train_wall=66, wall=0
2021-01-28 13:53:14 | INFO | train_inner | epoch 154:    297 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.132, nll_loss=0.91, ppl=1.88, wps=15493, ups=1.48, wpb=10458.3, bsz=378.5, num_updates=13200, lr=1.90693e-05, gnorm=0.823, train_wall=67, wall=0
2021-01-28 13:54:18 | INFO | train_inner | epoch 154:    397 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.129, nll_loss=0.906, ppl=1.87, wps=16490.1, ups=1.55, wpb=10631.7, bsz=364, num_updates=13300, lr=1.89974e-05, gnorm=0.824, train_wall=64, wall=0
2021-01-28 13:55:17 | INFO | train_inner | epoch 154:    497 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.138, nll_loss=0.915, ppl=1.89, wps=17545.4, ups=1.69, wpb=10368.7, bsz=370.9, num_updates=13400, lr=1.89264e-05, gnorm=0.834, train_wall=59, wall=0
2021-01-28 13:55:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 13:55:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:55:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:55:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:55:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:55:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:55:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:56:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 13:56:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 13:56:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 13:56:17 | INFO | valid | epoch 154 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.226 | nll_loss 3.71 | ppl 13.09 | bleu 23.13 | wps 4524.8 | wpb 7508.5 | bsz 272.7 | num_updates 13464 | best_bleu 23.39
2021-01-28 13:56:17 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 13:56:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:56:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:56:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:56:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:56:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 154 @ 13464 updates, score 23.13) (writing took 3.0992131900275126 seconds)
2021-01-28 13:56:20 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2021-01-28 13:56:20 | INFO | train | epoch 154 | symm_kl 0.307 | self_kl 0 | self_cv 0 | loss 3.131 | nll_loss 0.909 | ppl 1.88 | wps 15132.3 | ups 1.44 | wpb 10483.4 | bsz 369.6 | num_updates 13464 | lr 1.88814e-05 | gnorm 0.827 | train_wall 356 | wall 0
2021-01-28 13:56:20 | INFO | fairseq.trainer | begin training epoch 155
2021-01-28 13:56:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 13:56:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 13:56:44 | INFO | train_inner | epoch 155:     36 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.119, nll_loss=0.901, ppl=1.87, wps=12035.8, ups=1.15, wpb=10471.7, bsz=382.6, num_updates=13500, lr=1.88562e-05, gnorm=0.826, train_wall=59, wall=0
2021-01-28 13:57:50 | INFO | train_inner | epoch 155:    136 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.129, nll_loss=0.905, ppl=1.87, wps=15786.4, ups=1.52, wpb=10413.8, bsz=373.7, num_updates=13600, lr=1.87867e-05, gnorm=0.823, train_wall=65, wall=0
2021-01-28 13:58:55 | INFO | train_inner | epoch 155:    236 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.128, nll_loss=0.909, ppl=1.88, wps=16172, ups=1.54, wpb=10512.4, bsz=374.6, num_updates=13700, lr=1.8718e-05, gnorm=0.815, train_wall=65, wall=0
2021-01-28 14:00:01 | INFO | train_inner | epoch 155:    336 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.13, nll_loss=0.908, ppl=1.88, wps=15851.2, ups=1.52, wpb=10404.8, bsz=369.9, num_updates=13800, lr=1.86501e-05, gnorm=0.842, train_wall=65, wall=0
2021-01-28 14:01:07 | INFO | train_inner | epoch 155:    436 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.121, nll_loss=0.902, ppl=1.87, wps=15760.8, ups=1.5, wpb=10476.5, bsz=378, num_updates=13900, lr=1.85829e-05, gnorm=0.829, train_wall=66, wall=0
2021-01-28 14:02:14 | INFO | train_inner | epoch 155:    536 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.136, nll_loss=0.912, ppl=1.88, wps=15918.7, ups=1.51, wpb=10530.9, bsz=353.7, num_updates=14000, lr=1.85164e-05, gnorm=0.834, train_wall=66, wall=0
2021-01-28 14:02:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 14:02:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:02:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:02:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:02:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:02:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:02:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:02:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:02:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:02:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:02:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:02:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:02:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:02:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:02:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:02:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:02:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:02:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:02:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:02:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:02:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:02:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:03:04 | INFO | valid | epoch 155 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.231 | nll_loss 3.716 | ppl 13.14 | bleu 23.11 | wps 2823.3 | wpb 7508.5 | bsz 272.7 | num_updates 14025 | best_bleu 23.39
2021-01-28 14:03:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 14:03:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:03:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:03:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:03:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:03:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 155 @ 14025 updates, score 23.11) (writing took 3.2164779209997505 seconds)
2021-01-28 14:03:07 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2021-01-28 14:03:07 | INFO | train | epoch 155 | symm_kl 0.306 | self_kl 0 | self_cv 0 | loss 3.129 | nll_loss 0.907 | ppl 1.88 | wps 14451.9 | ups 1.38 | wpb 10483.4 | bsz 369.6 | num_updates 14025 | lr 1.84999e-05 | gnorm 0.829 | train_wall 365 | wall 0
2021-01-28 14:03:07 | INFO | fairseq.trainer | begin training epoch 156
2021-01-28 14:03:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:03:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:04:01 | INFO | train_inner | epoch 156:     75 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.134, nll_loss=0.911, ppl=1.88, wps=9925.5, ups=0.93, wpb=10623.1, bsz=370.3, num_updates=14100, lr=1.84506e-05, gnorm=0.833, train_wall=65, wall=0
2021-01-28 14:05:06 | INFO | train_inner | epoch 156:    175 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.123, nll_loss=0.903, ppl=1.87, wps=15927.9, ups=1.53, wpb=10390.8, bsz=368.8, num_updates=14200, lr=1.83855e-05, gnorm=0.826, train_wall=65, wall=0
2021-01-28 14:06:12 | INFO | train_inner | epoch 156:    275 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.137, nll_loss=0.912, ppl=1.88, wps=15841.2, ups=1.51, wpb=10522.1, bsz=360.2, num_updates=14300, lr=1.83211e-05, gnorm=0.826, train_wall=66, wall=0
2021-01-28 14:07:19 | INFO | train_inner | epoch 156:    375 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.128, nll_loss=0.908, ppl=1.88, wps=15725.5, ups=1.5, wpb=10451.7, bsz=383, num_updates=14400, lr=1.82574e-05, gnorm=0.822, train_wall=66, wall=0
2021-01-28 14:08:24 | INFO | train_inner | epoch 156:    475 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.127, nll_loss=0.904, ppl=1.87, wps=16034.2, ups=1.53, wpb=10503.1, bsz=358.6, num_updates=14500, lr=1.81944e-05, gnorm=0.832, train_wall=65, wall=0
2021-01-28 14:09:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 14:09:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:09:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:09:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:09:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:09:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:09:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:09:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:09:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:09:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:09:53 | INFO | valid | epoch 156 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.232 | nll_loss 3.717 | ppl 13.15 | bleu 23.08 | wps 3197.6 | wpb 7508.5 | bsz 272.7 | num_updates 14586 | best_bleu 23.39
2021-01-28 14:09:53 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 14:09:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:09:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:09:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:09:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 156 @ 14586 updates, score 23.08) (writing took 3.771332097996492 seconds)
2021-01-28 14:09:57 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2021-01-28 14:09:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:09:57 | INFO | train | epoch 156 | symm_kl 0.306 | self_kl 0 | self_cv 0 | loss 3.128 | nll_loss 0.906 | ppl 1.87 | wps 14341.5 | ups 1.37 | wpb 10483.4 | bsz 369.6 | num_updates 14586 | lr 1.81406e-05 | gnorm 0.826 | train_wall 367 | wall 0
2021-01-28 14:09:57 | INFO | fairseq.trainer | begin training epoch 157
2021-01-28 14:09:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:10:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:10:15 | INFO | train_inner | epoch 157:     14 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.132, nll_loss=0.911, ppl=1.88, wps=9395.4, ups=0.91, wpb=10362.3, bsz=369.9, num_updates=14600, lr=1.81319e-05, gnorm=0.83, train_wall=65, wall=0
2021-01-28 14:11:17 | INFO | train_inner | epoch 157:    114 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.129, nll_loss=0.905, ppl=1.87, wps=16756.4, ups=1.6, wpb=10457.9, bsz=363.6, num_updates=14700, lr=1.80702e-05, gnorm=0.829, train_wall=62, wall=0
2021-01-28 14:12:21 | INFO | train_inner | epoch 157:    214 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.137, nll_loss=0.912, ppl=1.88, wps=16301.4, ups=1.56, wpb=10465.8, bsz=365.5, num_updates=14800, lr=1.8009e-05, gnorm=0.833, train_wall=64, wall=0
2021-01-28 14:13:26 | INFO | train_inner | epoch 157:    314 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.116, nll_loss=0.899, ppl=1.86, wps=16072.6, ups=1.54, wpb=10467.7, bsz=376.2, num_updates=14900, lr=1.79485e-05, gnorm=0.829, train_wall=65, wall=0
2021-01-28 14:14:33 | INFO | train_inner | epoch 157:    414 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.127, nll_loss=0.904, ppl=1.87, wps=15766.4, ups=1.5, wpb=10499.4, bsz=371.8, num_updates=15000, lr=1.78885e-05, gnorm=0.836, train_wall=66, wall=0
2021-01-28 14:15:38 | INFO | train_inner | epoch 157:    514 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.129, nll_loss=0.909, ppl=1.88, wps=16051.4, ups=1.53, wpb=10522.9, bsz=370.2, num_updates=15100, lr=1.78292e-05, gnorm=0.825, train_wall=65, wall=0
2021-01-28 14:16:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 14:16:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:16:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:16:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:16:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:16:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:16:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:16:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:16:29 | INFO | valid | epoch 157 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.224 | nll_loss 3.71 | ppl 13.09 | bleu 23.13 | wps 4529.1 | wpb 7508.5 | bsz 272.7 | num_updates 15147 | best_bleu 23.39
2021-01-28 14:16:29 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 14:16:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:16:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:16:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:16:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:16:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 157 @ 15147 updates, score 23.13) (writing took 3.169024991977494 seconds)
2021-01-28 14:16:33 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2021-01-28 14:16:33 | INFO | train | epoch 157 | symm_kl 0.306 | self_kl 0 | self_cv 0 | loss 3.127 | nll_loss 0.906 | ppl 1.87 | wps 14861 | ups 1.42 | wpb 10483.4 | bsz 369.6 | num_updates 15147 | lr 1.78015e-05 | gnorm 0.83 | train_wall 358 | wall 0
2021-01-28 14:16:33 | INFO | fairseq.trainer | begin training epoch 158
2021-01-28 14:16:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:16:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:17:08 | INFO | train_inner | epoch 158:     53 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.138, nll_loss=0.916, ppl=1.89, wps=11673.8, ups=1.12, wpb=10457.4, bsz=353.4, num_updates=15200, lr=1.77705e-05, gnorm=0.842, train_wall=58, wall=0
2021-01-28 14:18:07 | INFO | train_inner | epoch 158:    153 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.113, nll_loss=0.896, ppl=1.86, wps=17757.8, ups=1.68, wpb=10558.1, bsz=376.6, num_updates=15300, lr=1.77123e-05, gnorm=0.818, train_wall=59, wall=0
2021-01-28 14:19:08 | INFO | train_inner | epoch 158:    253 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.105, nll_loss=0.89, ppl=1.85, wps=17585.6, ups=1.66, wpb=10566.9, bsz=391.4, num_updates=15400, lr=1.76547e-05, gnorm=0.805, train_wall=60, wall=0
2021-01-28 14:20:13 | INFO | train_inner | epoch 158:    353 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.14, nll_loss=0.917, ppl=1.89, wps=16260.7, ups=1.54, wpb=10570.3, bsz=354.6, num_updates=15500, lr=1.75977e-05, gnorm=0.828, train_wall=65, wall=0
2021-01-28 14:21:19 | INFO | train_inner | epoch 158:    453 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.124, nll_loss=0.904, ppl=1.87, wps=15720, ups=1.51, wpb=10400, bsz=373, num_updates=15600, lr=1.75412e-05, gnorm=0.828, train_wall=66, wall=0
2021-01-28 14:22:24 | INFO | train_inner | epoch 158:    553 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.129, nll_loss=0.908, ppl=1.88, wps=16095.3, ups=1.54, wpb=10463.9, bsz=374.4, num_updates=15700, lr=1.74852e-05, gnorm=0.823, train_wall=65, wall=0
2021-01-28 14:22:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 14:22:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:22:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:22:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:22:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:22:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:22:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:22:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:22:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:22:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:22:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:22:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:22:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:22:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:22:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:22:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:22:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:22:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:22:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:22:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:22:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:22:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:22:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:22:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:22:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:22:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:22:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:22:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:22:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:22:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:22:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:22:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:22:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:22:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:22:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:22:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:22:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:22:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:22:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:22:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:23:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:23:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:23:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:23:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:23:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:23:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:23:05 | INFO | valid | epoch 158 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.233 | nll_loss 3.718 | ppl 13.16 | bleu 22.95 | wps 3224.4 | wpb 7508.5 | bsz 272.7 | num_updates 15708 | best_bleu 23.39
2021-01-28 14:23:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 14:23:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:23:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:23:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 158 @ 15708 updates, score 22.95) (writing took 4.126421722990926 seconds)
2021-01-28 14:23:09 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2021-01-28 14:23:09 | INFO | train | epoch 158 | symm_kl 0.305 | self_kl 0 | self_cv 0 | loss 3.125 | nll_loss 0.905 | ppl 1.87 | wps 14818.9 | ups 1.41 | wpb 10483.4 | bsz 369.6 | num_updates 15708 | lr 1.74808e-05 | gnorm 0.827 | train_wall 350 | wall 0
2021-01-28 14:23:09 | INFO | fairseq.trainer | begin training epoch 159
2021-01-28 14:23:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:23:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:23:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:23:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:24:14 | INFO | train_inner | epoch 159:     92 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.124, nll_loss=0.897, ppl=1.86, wps=9523.8, ups=0.91, wpb=10516.6, bsz=341.5, num_updates=15800, lr=1.74298e-05, gnorm=0.833, train_wall=64, wall=0
2021-01-28 14:25:20 | INFO | train_inner | epoch 159:    192 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.124, nll_loss=0.905, ppl=1.87, wps=16073.1, ups=1.53, wpb=10530.1, bsz=378.6, num_updates=15900, lr=1.73749e-05, gnorm=0.82, train_wall=65, wall=0
2021-01-28 14:26:25 | INFO | train_inner | epoch 159:    292 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.133, nll_loss=0.909, ppl=1.88, wps=15794.1, ups=1.52, wpb=10365.3, bsz=375.3, num_updates=16000, lr=1.73205e-05, gnorm=0.848, train_wall=65, wall=0
2021-01-28 14:27:32 | INFO | train_inner | epoch 159:    392 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.115, nll_loss=0.9, ppl=1.87, wps=15671.8, ups=1.5, wpb=10455.5, bsz=375.4, num_updates=16100, lr=1.72666e-05, gnorm=0.824, train_wall=66, wall=0
2021-01-28 14:28:38 | INFO | train_inner | epoch 159:    492 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.123, nll_loss=0.906, ppl=1.87, wps=16076.7, ups=1.52, wpb=10571.7, bsz=370.7, num_updates=16200, lr=1.72133e-05, gnorm=0.819, train_wall=65, wall=0
2021-01-28 14:29:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 14:29:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:29:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:29:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:29:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:29:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:29:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:29:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:29:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:29:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:29:58 | INFO | valid | epoch 159 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.229 | nll_loss 3.714 | ppl 13.12 | bleu 23.06 | wps 3125.6 | wpb 7508.5 | bsz 272.7 | num_updates 16269 | best_bleu 23.39
2021-01-28 14:29:58 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 14:30:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:30:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:30:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:30:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:30:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 159 @ 16269 updates, score 23.06) (writing took 3.7180550739867613 seconds)
2021-01-28 14:30:02 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2021-01-28 14:30:02 | INFO | train | epoch 159 | symm_kl 0.305 | self_kl 0 | self_cv 0 | loss 3.124 | nll_loss 0.904 | ppl 1.87 | wps 14255.9 | ups 1.36 | wpb 10483.4 | bsz 369.6 | num_updates 16269 | lr 1.71767e-05 | gnorm 0.828 | train_wall 365 | wall 0
2021-01-28 14:30:02 | INFO | fairseq.trainer | begin training epoch 160
2021-01-28 14:30:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:30:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:30:28 | INFO | train_inner | epoch 160:     31 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.134, nll_loss=0.913, ppl=1.88, wps=9378.5, ups=0.91, wpb=10354.6, bsz=374.6, num_updates=16300, lr=1.71604e-05, gnorm=0.835, train_wall=65, wall=0
2021-01-28 14:31:34 | INFO | train_inner | epoch 160:    131 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.121, nll_loss=0.899, ppl=1.86, wps=15739.2, ups=1.52, wpb=10340.6, bsz=368.7, num_updates=16400, lr=1.7108e-05, gnorm=0.835, train_wall=65, wall=0
2021-01-28 14:32:40 | INFO | train_inner | epoch 160:    231 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.124, nll_loss=0.905, ppl=1.87, wps=15769.2, ups=1.51, wpb=10423.7, bsz=367.5, num_updates=16500, lr=1.70561e-05, gnorm=0.825, train_wall=66, wall=0
2021-01-28 14:33:46 | INFO | train_inner | epoch 160:    331 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.12, nll_loss=0.903, ppl=1.87, wps=16203.7, ups=1.52, wpb=10656.5, bsz=377.3, num_updates=16600, lr=1.70046e-05, gnorm=0.82, train_wall=65, wall=0
2021-01-28 14:34:52 | INFO | train_inner | epoch 160:    431 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.131, nll_loss=0.907, ppl=1.88, wps=15766, ups=1.52, wpb=10389.4, bsz=352.1, num_updates=16700, lr=1.69536e-05, gnorm=0.842, train_wall=65, wall=0
2021-01-28 14:35:59 | INFO | train_inner | epoch 160:    531 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.115, nll_loss=0.897, ppl=1.86, wps=15952, ups=1.49, wpb=10679.5, bsz=379.9, num_updates=16800, lr=1.69031e-05, gnorm=0.817, train_wall=67, wall=0
2021-01-28 14:36:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 14:36:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:36:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:36:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:36:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:36:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:36:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:36:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:36:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:36:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:36:52 | INFO | valid | epoch 160 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.23 | nll_loss 3.712 | ppl 13.1 | bleu 23.16 | wps 3070.7 | wpb 7508.5 | bsz 272.7 | num_updates 16830 | best_bleu 23.39
2021-01-28 14:36:52 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 14:36:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:36:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:36:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:36:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:36:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 160 @ 16830 updates, score 23.16) (writing took 3.581162538030185 seconds)
2021-01-28 14:36:55 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2021-01-28 14:36:55 | INFO | train | epoch 160 | symm_kl 0.305 | self_kl 0 | self_cv 0 | loss 3.123 | nll_loss 0.903 | ppl 1.87 | wps 14232.1 | ups 1.36 | wpb 10483.4 | bsz 369.6 | num_updates 16830 | lr 1.6888e-05 | gnorm 0.829 | train_wall 368 | wall 0
2021-01-28 14:36:55 | INFO | fairseq.trainer | begin training epoch 161
2021-01-28 14:36:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:36:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:37:46 | INFO | train_inner | epoch 161:     70 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.113, nll_loss=0.895, ppl=1.86, wps=9732.3, ups=0.93, wpb=10444.7, bsz=375, num_updates=16900, lr=1.6853e-05, gnorm=0.832, train_wall=64, wall=0
2021-01-28 14:38:54 | INFO | train_inner | epoch 161:    170 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.127, nll_loss=0.903, ppl=1.87, wps=15265.2, ups=1.48, wpb=10315, bsz=363.9, num_updates=17000, lr=1.68034e-05, gnorm=0.836, train_wall=67, wall=0
2021-01-28 14:40:00 | INFO | train_inner | epoch 161:    270 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.121, nll_loss=0.903, ppl=1.87, wps=15771.6, ups=1.51, wpb=10431, bsz=387.8, num_updates=17100, lr=1.67542e-05, gnorm=0.826, train_wall=66, wall=0
2021-01-28 14:41:06 | INFO | train_inner | epoch 161:    370 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.118, nll_loss=0.9, ppl=1.87, wps=16027.1, ups=1.52, wpb=10568.6, bsz=358.3, num_updates=17200, lr=1.67054e-05, gnorm=0.835, train_wall=65, wall=0
2021-01-28 14:42:12 | INFO | train_inner | epoch 161:    470 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.12, nll_loss=0.902, ppl=1.87, wps=15898, ups=1.51, wpb=10553.2, bsz=370, num_updates=17300, lr=1.6657e-05, gnorm=0.826, train_wall=66, wall=0
2021-01-28 14:43:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 14:43:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:43:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:43:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:43:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:43:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:43:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:43:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:43:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:43:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:43:44 | INFO | valid | epoch 161 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.226 | nll_loss 3.71 | ppl 13.09 | bleu 23.02 | wps 3053.1 | wpb 7508.5 | bsz 272.7 | num_updates 17391 | best_bleu 23.39
2021-01-28 14:43:44 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 14:43:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:43:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:43:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:43:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:43:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 161 @ 17391 updates, score 23.02) (writing took 3.591755973000545 seconds)
2021-01-28 14:43:47 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2021-01-28 14:43:47 | INFO | train | epoch 161 | symm_kl 0.304 | self_kl 0 | self_cv 0 | loss 3.122 | nll_loss 0.903 | ppl 1.87 | wps 14270.1 | ups 1.36 | wpb 10483.4 | bsz 369.6 | num_updates 17391 | lr 1.66134e-05 | gnorm 0.829 | train_wall 368 | wall 0
2021-01-28 14:43:47 | INFO | fairseq.trainer | begin training epoch 162
2021-01-28 14:43:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:43:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:43:59 | INFO | train_inner | epoch 162:      9 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.133, nll_loss=0.913, ppl=1.88, wps=9826.1, ups=0.93, wpb=10553.3, bsz=366.8, num_updates=17400, lr=1.66091e-05, gnorm=0.821, train_wall=65, wall=0
2021-01-28 14:44:59 | INFO | train_inner | epoch 162:    109 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.128, nll_loss=0.909, ppl=1.88, wps=17598.7, ups=1.67, wpb=10541.1, bsz=382.4, num_updates=17500, lr=1.65616e-05, gnorm=0.813, train_wall=60, wall=0
2021-01-28 14:45:58 | INFO | train_inner | epoch 162:    209 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.123, nll_loss=0.901, ppl=1.87, wps=17655.7, ups=1.7, wpb=10404.9, bsz=367.1, num_updates=17600, lr=1.65145e-05, gnorm=0.835, train_wall=59, wall=0
2021-01-28 14:46:58 | INFO | train_inner | epoch 162:    309 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.113, nll_loss=0.896, ppl=1.86, wps=17718.1, ups=1.68, wpb=10537.4, bsz=363.7, num_updates=17700, lr=1.64677e-05, gnorm=0.821, train_wall=59, wall=0
2021-01-28 14:47:57 | INFO | train_inner | epoch 162:    409 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.113, nll_loss=0.897, ppl=1.86, wps=17584.1, ups=1.67, wpb=10507.6, bsz=384.2, num_updates=17800, lr=1.64214e-05, gnorm=0.815, train_wall=60, wall=0
2021-01-28 14:48:57 | INFO | train_inner | epoch 162:    509 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.13, nll_loss=0.907, ppl=1.87, wps=17448, ups=1.67, wpb=10430, bsz=365, num_updates=17900, lr=1.63755e-05, gnorm=0.835, train_wall=60, wall=0
2021-01-28 14:49:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 14:49:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:49:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:49:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:49:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:49:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:49:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:49:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:49:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:49:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:49:49 | INFO | valid | epoch 162 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.228 | nll_loss 3.713 | ppl 13.12 | bleu 23.17 | wps 4451.1 | wpb 7508.5 | bsz 272.7 | num_updates 17952 | best_bleu 23.39
2021-01-28 14:49:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 14:49:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:49:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:49:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:49:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:49:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 162 @ 17952 updates, score 23.17) (writing took 3.1176848889444955 seconds)
2021-01-28 14:49:53 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2021-01-28 14:49:53 | INFO | train | epoch 162 | symm_kl 0.304 | self_kl 0 | self_cv 0 | loss 3.122 | nll_loss 0.902 | ppl 1.87 | wps 16103.6 | ups 1.54 | wpb 10483.4 | bsz 369.6 | num_updates 17952 | lr 1.63517e-05 | gnorm 0.823 | train_wall 333 | wall 0
2021-01-28 14:49:53 | INFO | fairseq.trainer | begin training epoch 163
2021-01-28 14:49:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:49:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:50:24 | INFO | train_inner | epoch 163:     48 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.115, nll_loss=0.898, ppl=1.86, wps=12041.7, ups=1.15, wpb=10483.8, bsz=351.2, num_updates=18000, lr=1.63299e-05, gnorm=0.823, train_wall=59, wall=0
2021-01-28 14:51:27 | INFO | train_inner | epoch 163:    148 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.122, nll_loss=0.903, ppl=1.87, wps=16675.8, ups=1.6, wpb=10454, bsz=365, num_updates=18100, lr=1.62848e-05, gnorm=0.818, train_wall=62, wall=0
2021-01-28 14:52:33 | INFO | train_inner | epoch 163:    248 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.112, nll_loss=0.895, ppl=1.86, wps=15722, ups=1.51, wpb=10445.8, bsz=365.7, num_updates=18200, lr=1.624e-05, gnorm=0.824, train_wall=66, wall=0
2021-01-28 14:53:41 | INFO | train_inner | epoch 163:    348 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.116, nll_loss=0.896, ppl=1.86, wps=15471.6, ups=1.48, wpb=10456.6, bsz=362, num_updates=18300, lr=1.61955e-05, gnorm=0.836, train_wall=67, wall=0
2021-01-28 14:54:47 | INFO | train_inner | epoch 163:    448 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.132, nll_loss=0.912, ppl=1.88, wps=15694.1, ups=1.5, wpb=10435, bsz=381.1, num_updates=18400, lr=1.61515e-05, gnorm=0.838, train_wall=66, wall=0
2021-01-28 14:55:54 | INFO | train_inner | epoch 163:    548 / 561 symm_kl=0.308, self_kl=0, self_cv=0, loss=3.136, nll_loss=0.912, ppl=1.88, wps=16051.9, ups=1.51, wpb=10660.9, bsz=374.3, num_updates=18500, lr=1.61077e-05, gnorm=0.825, train_wall=66, wall=0
2021-01-28 14:56:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 14:56:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:56:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:56:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:56:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:56:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:56:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:56:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 14:56:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 14:56:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 14:56:41 | INFO | valid | epoch 163 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.23 | nll_loss 3.712 | ppl 13.1 | bleu 23.12 | wps 2524.8 | wpb 7508.5 | bsz 272.7 | num_updates 18513 | best_bleu 23.39
2021-01-28 14:56:41 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 14:56:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:56:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:56:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:56:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:56:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 163 @ 18513 updates, score 23.12) (writing took 3.1254389719688334 seconds)
2021-01-28 14:56:44 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2021-01-28 14:56:44 | INFO | train | epoch 163 | symm_kl 0.304 | self_kl 0 | self_cv 0 | loss 3.121 | nll_loss 0.902 | ppl 1.87 | wps 14301.5 | ups 1.36 | wpb 10483.4 | bsz 369.6 | num_updates 18513 | lr 1.61021e-05 | gnorm 0.827 | train_wall 364 | wall 0
2021-01-28 14:56:44 | INFO | fairseq.trainer | begin training epoch 164
2021-01-28 14:56:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 14:56:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 14:57:46 | INFO | train_inner | epoch 164:     87 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.105, nll_loss=0.889, ppl=1.85, wps=9400.8, ups=0.89, wpb=10505.2, bsz=374.2, num_updates=18600, lr=1.60644e-05, gnorm=0.818, train_wall=66, wall=0
2021-01-28 14:58:52 | INFO | train_inner | epoch 164:    187 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.109, nll_loss=0.892, ppl=1.86, wps=16027.4, ups=1.51, wpb=10597.9, bsz=372.4, num_updates=18700, lr=1.60214e-05, gnorm=0.814, train_wall=66, wall=0
2021-01-28 14:59:58 | INFO | train_inner | epoch 164:    287 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.118, nll_loss=0.901, ppl=1.87, wps=15648.3, ups=1.5, wpb=10432.4, bsz=364.9, num_updates=18800, lr=1.59787e-05, gnorm=0.833, train_wall=66, wall=0
2021-01-28 15:01:05 | INFO | train_inner | epoch 164:    387 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.126, nll_loss=0.905, ppl=1.87, wps=15900.2, ups=1.51, wpb=10515.1, bsz=366.6, num_updates=18900, lr=1.59364e-05, gnorm=0.832, train_wall=66, wall=0
2021-01-28 15:02:12 | INFO | train_inner | epoch 164:    487 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.136, nll_loss=0.914, ppl=1.88, wps=15384.1, ups=1.47, wpb=10437.5, bsz=369.5, num_updates=19000, lr=1.58944e-05, gnorm=0.831, train_wall=67, wall=0
2021-01-28 15:03:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 15:03:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:03:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:03:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:03:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:03:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:03:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:03:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:03:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:03:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:03:36 | INFO | valid | epoch 164 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.225 | nll_loss 3.709 | ppl 13.08 | bleu 23.15 | wps 2959.3 | wpb 7508.5 | bsz 272.7 | num_updates 19074 | best_bleu 23.39
2021-01-28 15:03:36 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 15:03:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:03:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:03:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:03:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:03:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 164 @ 19074 updates, score 23.15) (writing took 3.0816505810362287 seconds)
2021-01-28 15:03:39 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2021-01-28 15:03:39 | INFO | train | epoch 164 | symm_kl 0.304 | self_kl 0 | self_cv 0 | loss 3.119 | nll_loss 0.9 | ppl 1.87 | wps 14154.3 | ups 1.35 | wpb 10483.4 | bsz 369.6 | num_updates 19074 | lr 1.58635e-05 | gnorm 0.827 | train_wall 370 | wall 0
2021-01-28 15:03:39 | INFO | fairseq.trainer | begin training epoch 165
2021-01-28 15:03:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:03:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:04:01 | INFO | train_inner | epoch 165:     26 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.123, nll_loss=0.905, ppl=1.87, wps=9521.4, ups=0.92, wpb=10340.7, bsz=369, num_updates=19100, lr=1.58527e-05, gnorm=0.839, train_wall=65, wall=0
2021-01-28 15:05:08 | INFO | train_inner | epoch 165:    126 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.116, nll_loss=0.896, ppl=1.86, wps=15786.9, ups=1.49, wpb=10569.9, bsz=358.2, num_updates=19200, lr=1.58114e-05, gnorm=0.812, train_wall=66, wall=0
2021-01-28 15:06:16 | INFO | train_inner | epoch 165:    226 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.112, nll_loss=0.896, ppl=1.86, wps=15533.6, ups=1.47, wpb=10568.2, bsz=381.9, num_updates=19300, lr=1.57704e-05, gnorm=0.817, train_wall=68, wall=0
2021-01-28 15:07:23 | INFO | train_inner | epoch 165:    326 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.112, nll_loss=0.895, ppl=1.86, wps=15638.1, ups=1.49, wpb=10474.5, bsz=375, num_updates=19400, lr=1.57297e-05, gnorm=0.831, train_wall=66, wall=0
2021-01-28 15:08:30 | INFO | train_inner | epoch 165:    426 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.127, nll_loss=0.908, ppl=1.88, wps=15571, ups=1.49, wpb=10421.6, bsz=367, num_updates=19500, lr=1.56893e-05, gnorm=0.838, train_wall=66, wall=0
2021-01-28 15:09:32 | INFO | train_inner | epoch 165:    526 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.11, nll_loss=0.896, ppl=1.86, wps=16894.2, ups=1.61, wpb=10504.4, bsz=380.7, num_updates=19600, lr=1.56492e-05, gnorm=0.821, train_wall=62, wall=0
2021-01-28 15:09:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 15:09:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:09:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:09:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:09:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:09:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:09:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:10:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:10:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:10:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:10:26 | INFO | valid | epoch 165 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.232 | nll_loss 3.715 | ppl 13.13 | bleu 23.05 | wps 3207.8 | wpb 7508.5 | bsz 272.7 | num_updates 19635 | best_bleu 23.39
2021-01-28 15:10:26 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 15:10:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:10:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:10:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 165 @ 19635 updates, score 23.05) (writing took 4.157379576994572 seconds)
2021-01-28 15:10:30 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2021-01-28 15:10:30 | INFO | train | epoch 165 | symm_kl 0.303 | self_kl 0 | self_cv 0 | loss 3.118 | nll_loss 0.9 | ppl 1.87 | wps 14322.9 | ups 1.37 | wpb 10483.4 | bsz 369.6 | num_updates 19635 | lr 1.56353e-05 | gnorm 0.827 | train_wall 368 | wall 0
2021-01-28 15:10:30 | INFO | fairseq.trainer | begin training epoch 166
2021-01-28 15:10:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:10:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:10:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:10:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:11:17 | INFO | train_inner | epoch 166:     65 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.115, nll_loss=0.895, ppl=1.86, wps=9863.9, ups=0.95, wpb=10339.8, bsz=375.1, num_updates=19700, lr=1.56094e-05, gnorm=0.84, train_wall=64, wall=0
2021-01-28 15:12:22 | INFO | train_inner | epoch 166:    165 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.111, nll_loss=0.892, ppl=1.86, wps=16299.6, ups=1.54, wpb=10574.9, bsz=350.9, num_updates=19800, lr=1.557e-05, gnorm=0.826, train_wall=64, wall=0
2021-01-28 15:13:26 | INFO | train_inner | epoch 166:    265 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.125, nll_loss=0.91, ppl=1.88, wps=16045.4, ups=1.55, wpb=10363.9, bsz=386.7, num_updates=19900, lr=1.55308e-05, gnorm=0.821, train_wall=64, wall=0
2021-01-28 15:14:31 | INFO | train_inner | epoch 166:    365 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.136, nll_loss=0.91, ppl=1.88, wps=16236.4, ups=1.56, wpb=10427.4, bsz=355.1, num_updates=20000, lr=1.54919e-05, gnorm=0.843, train_wall=64, wall=0
2021-01-28 15:15:37 | INFO | train_inner | epoch 166:    465 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.122, nll_loss=0.9, ppl=1.87, wps=15896.3, ups=1.5, wpb=10592.3, bsz=374, num_updates=20100, lr=1.54533e-05, gnorm=0.835, train_wall=66, wall=0
2021-01-28 15:16:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 15:16:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:16:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:16:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:16:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:16:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:16:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:16:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:16:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:16:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:16:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:16:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:16:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:16:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:16:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:16:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:16:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:16:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:16:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:16:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:16:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:16:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:17:00 | INFO | valid | epoch 166 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.229 | nll_loss 3.711 | ppl 13.1 | bleu 23.25 | wps 4683.9 | wpb 7508.5 | bsz 272.7 | num_updates 20196 | best_bleu 23.39
2021-01-28 15:17:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 15:17:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:17:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:17:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:17:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:17:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 166 @ 20196 updates, score 23.25) (writing took 3.089065608975943 seconds)
2021-01-28 15:17:03 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2021-01-28 15:17:03 | INFO | train | epoch 166 | symm_kl 0.303 | self_kl 0 | self_cv 0 | loss 3.117 | nll_loss 0.899 | ppl 1.86 | wps 14956.7 | ups 1.43 | wpb 10483.4 | bsz 369.6 | num_updates 20196 | lr 1.54166e-05 | gnorm 0.829 | train_wall 361 | wall 0
2021-01-28 15:17:03 | INFO | fairseq.trainer | begin training epoch 167
2021-01-28 15:17:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:17:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:17:09 | INFO | train_inner | epoch 167:      4 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.099, nll_loss=0.89, ppl=1.85, wps=11454.5, ups=1.09, wpb=10477.3, bsz=369, num_updates=20200, lr=1.5415e-05, gnorm=0.821, train_wall=64, wall=0
2021-01-28 15:18:07 | INFO | train_inner | epoch 167:    104 / 561 symm_kl=0.309, self_kl=0, self_cv=0, loss=3.136, nll_loss=0.909, ppl=1.88, wps=18028.3, ups=1.73, wpb=10430.6, bsz=353.2, num_updates=20300, lr=1.5377e-05, gnorm=0.845, train_wall=58, wall=0
2021-01-28 15:19:11 | INFO | train_inner | epoch 167:    204 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.103, nll_loss=0.891, ppl=1.85, wps=16569.7, ups=1.56, wpb=10619.8, bsz=380.2, num_updates=20400, lr=1.53393e-05, gnorm=0.81, train_wall=64, wall=0
2021-01-28 15:20:17 | INFO | train_inner | epoch 167:    304 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.114, nll_loss=0.897, ppl=1.86, wps=15810.2, ups=1.5, wpb=10515, bsz=365.2, num_updates=20500, lr=1.53018e-05, gnorm=0.822, train_wall=66, wall=0
2021-01-28 15:21:23 | INFO | train_inner | epoch 167:    404 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.122, nll_loss=0.905, ppl=1.87, wps=16111.8, ups=1.53, wpb=10537.5, bsz=376.5, num_updates=20600, lr=1.52647e-05, gnorm=0.821, train_wall=65, wall=0
2021-01-28 15:22:29 | INFO | train_inner | epoch 167:    504 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.104, nll_loss=0.89, ppl=1.85, wps=15880.7, ups=1.51, wpb=10510, bsz=372.2, num_updates=20700, lr=1.52277e-05, gnorm=0.819, train_wall=66, wall=0
2021-01-28 15:23:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 15:23:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:23:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:23:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:23:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:23:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:23:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:23:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:23:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:23:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:23:38 | INFO | valid | epoch 167 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.226 | nll_loss 3.71 | ppl 13.09 | bleu 23.25 | wps 2997.1 | wpb 7508.5 | bsz 272.7 | num_updates 20757 | best_bleu 23.39
2021-01-28 15:23:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 15:23:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:23:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:23:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:23:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 167 @ 20757 updates, score 23.25) (writing took 3.0843332179938443 seconds)
2021-01-28 15:23:41 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2021-01-28 15:23:41 | INFO | train | epoch 167 | symm_kl 0.303 | self_kl 0 | self_cv 0 | loss 3.116 | nll_loss 0.899 | ppl 1.86 | wps 14779.8 | ups 1.41 | wpb 10483.4 | bsz 369.6 | num_updates 20757 | lr 1.52068e-05 | gnorm 0.826 | train_wall 358 | wall 0
2021-01-28 15:23:41 | INFO | fairseq.trainer | begin training epoch 168
2021-01-28 15:23:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:23:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:23:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:24:14 | INFO | train_inner | epoch 168:     43 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.125, nll_loss=0.904, ppl=1.87, wps=9875.5, ups=0.95, wpb=10375.7, bsz=364.2, num_updates=20800, lr=1.51911e-05, gnorm=0.839, train_wall=65, wall=0
2021-01-28 15:25:20 | INFO | train_inner | epoch 168:    143 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.126, nll_loss=0.902, ppl=1.87, wps=15861, ups=1.52, wpb=10453.6, bsz=374.6, num_updates=20900, lr=1.51547e-05, gnorm=0.823, train_wall=65, wall=0
2021-01-28 15:26:25 | INFO | train_inner | epoch 168:    243 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.109, nll_loss=0.892, ppl=1.86, wps=16100.5, ups=1.53, wpb=10511.5, bsz=365.5, num_updates=21000, lr=1.51186e-05, gnorm=0.824, train_wall=65, wall=0
2021-01-28 15:27:31 | INFO | train_inner | epoch 168:    343 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.114, nll_loss=0.895, ppl=1.86, wps=15761.2, ups=1.51, wpb=10406.1, bsz=359.9, num_updates=21100, lr=1.50827e-05, gnorm=0.83, train_wall=66, wall=0
2021-01-28 15:28:36 | INFO | train_inner | epoch 168:    443 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.099, nll_loss=0.887, ppl=1.85, wps=16225.7, ups=1.53, wpb=10583.1, bsz=373.9, num_updates=21200, lr=1.50471e-05, gnorm=0.814, train_wall=65, wall=0
2021-01-28 15:29:43 | INFO | train_inner | epoch 168:    543 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.122, nll_loss=0.908, ppl=1.88, wps=15577.4, ups=1.49, wpb=10442.3, bsz=384.2, num_updates=21300, lr=1.50117e-05, gnorm=0.822, train_wall=66, wall=0
2021-01-28 15:29:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 15:29:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:29:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:29:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:29:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:30:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:30:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:30:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:30:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:30:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:30:28 | INFO | valid | epoch 168 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.227 | nll_loss 3.712 | ppl 13.1 | bleu 23.25 | wps 3108.1 | wpb 7508.5 | bsz 272.7 | num_updates 21318 | best_bleu 23.39
2021-01-28 15:30:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 15:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:30:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:30:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:30:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 168 @ 21318 updates, score 23.25) (writing took 3.6440505359787494 seconds)
2021-01-28 15:30:31 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2021-01-28 15:30:31 | INFO | train | epoch 168 | symm_kl 0.302 | self_kl 0 | self_cv 0 | loss 3.115 | nll_loss 0.897 | ppl 1.86 | wps 14339.4 | ups 1.37 | wpb 10483.4 | bsz 369.6 | num_updates 21318 | lr 1.50054e-05 | gnorm 0.823 | train_wall 366 | wall 0
2021-01-28 15:30:31 | INFO | fairseq.trainer | begin training epoch 169
2021-01-28 15:30:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:30:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:31:31 | INFO | train_inner | epoch 169:     82 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.109, nll_loss=0.891, ppl=1.85, wps=9736.1, ups=0.93, wpb=10447.9, bsz=370.2, num_updates=21400, lr=1.49766e-05, gnorm=0.821, train_wall=63, wall=0
2021-01-28 15:32:37 | INFO | train_inner | epoch 169:    182 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.094, nll_loss=0.882, ppl=1.84, wps=16008.8, ups=1.51, wpb=10595.9, bsz=381.8, num_updates=21500, lr=1.49417e-05, gnorm=0.818, train_wall=66, wall=0
2021-01-28 15:33:44 | INFO | train_inner | epoch 169:    282 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.131, nll_loss=0.907, ppl=1.87, wps=15785.7, ups=1.49, wpb=10588.3, bsz=342.3, num_updates=21600, lr=1.49071e-05, gnorm=0.831, train_wall=67, wall=0
2021-01-28 15:34:51 | INFO | train_inner | epoch 169:    382 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.108, nll_loss=0.894, ppl=1.86, wps=15593.8, ups=1.5, wpb=10420.6, bsz=373, num_updates=21700, lr=1.48727e-05, gnorm=0.84, train_wall=66, wall=0
2021-01-28 15:35:50 | INFO | train_inner | epoch 169:    482 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.124, nll_loss=0.908, ppl=1.88, wps=17506.4, ups=1.68, wpb=10425.8, bsz=374, num_updates=21800, lr=1.48386e-05, gnorm=0.828, train_wall=59, wall=0
2021-01-28 15:36:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 15:36:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:36:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:36:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:36:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:36:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:36:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:36:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:36:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:36:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:36:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:36:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:36:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:36:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:36:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:36:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:36:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:36:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:36:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:36:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:36:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:36:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:37:00 | INFO | valid | epoch 169 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.227 | nll_loss 3.712 | ppl 13.1 | bleu 23.1 | wps 4296.7 | wpb 7508.5 | bsz 272.7 | num_updates 21879 | best_bleu 23.39
2021-01-28 15:37:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 15:37:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:37:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:37:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:37:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:37:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 169 @ 21879 updates, score 23.1) (writing took 3.09503714897437 seconds)
2021-01-28 15:37:03 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2021-01-28 15:37:03 | INFO | train | epoch 169 | symm_kl 0.302 | self_kl 0 | self_cv 0 | loss 3.114 | nll_loss 0.897 | ppl 1.86 | wps 15017.6 | ups 1.43 | wpb 10483.4 | bsz 369.6 | num_updates 21879 | lr 1.48118e-05 | gnorm 0.828 | train_wall 357 | wall 0
2021-01-28 15:37:03 | INFO | fairseq.trainer | begin training epoch 170
2021-01-28 15:37:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:37:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:37:18 | INFO | train_inner | epoch 170:     21 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.114, nll_loss=0.901, ppl=1.87, wps=11784.3, ups=1.14, wpb=10363.8, bsz=376.1, num_updates=21900, lr=1.48047e-05, gnorm=0.823, train_wall=59, wall=0
2021-01-28 15:38:25 | INFO | train_inner | epoch 170:    121 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.119, nll_loss=0.903, ppl=1.87, wps=15698.3, ups=1.5, wpb=10494.3, bsz=365, num_updates=22000, lr=1.4771e-05, gnorm=0.815, train_wall=66, wall=0
2021-01-28 15:39:31 | INFO | train_inner | epoch 170:    221 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.121, nll_loss=0.899, ppl=1.86, wps=15826.7, ups=1.51, wpb=10464.2, bsz=368.4, num_updates=22100, lr=1.47375e-05, gnorm=0.838, train_wall=66, wall=0
2021-01-28 15:40:38 | INFO | train_inner | epoch 170:    321 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.121, nll_loss=0.901, ppl=1.87, wps=15844.7, ups=1.51, wpb=10524.5, bsz=365.5, num_updates=22200, lr=1.47043e-05, gnorm=0.817, train_wall=66, wall=0
2021-01-28 15:41:45 | INFO | train_inner | epoch 170:    421 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.095, nll_loss=0.884, ppl=1.85, wps=15624.2, ups=1.49, wpb=10480.1, bsz=382.2, num_updates=22300, lr=1.46713e-05, gnorm=0.828, train_wall=67, wall=0
2021-01-28 15:42:52 | INFO | train_inner | epoch 170:    521 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.116, nll_loss=0.898, ppl=1.86, wps=15588.2, ups=1.49, wpb=10490, bsz=355.8, num_updates=22400, lr=1.46385e-05, gnorm=0.832, train_wall=67, wall=0
2021-01-28 15:43:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 15:43:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:43:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:43:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:43:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:43:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:43:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:43:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:43:49 | INFO | valid | epoch 170 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.228 | nll_loss 3.714 | ppl 13.12 | bleu 23.16 | wps 3043.5 | wpb 7508.5 | bsz 272.7 | num_updates 22440 | best_bleu 23.39
2021-01-28 15:43:49 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 15:43:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:43:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:43:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:43:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:43:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 170 @ 22440 updates, score 23.16) (writing took 3.5857524150051177 seconds)
2021-01-28 15:43:53 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2021-01-28 15:43:53 | INFO | train | epoch 170 | symm_kl 0.302 | self_kl 0 | self_cv 0 | loss 3.114 | nll_loss 0.897 | ppl 1.86 | wps 14341.1 | ups 1.37 | wpb 10483.4 | bsz 369.6 | num_updates 22440 | lr 1.46254e-05 | gnorm 0.827 | train_wall 370 | wall 0
2021-01-28 15:43:53 | INFO | fairseq.trainer | begin training epoch 171
2021-01-28 15:43:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:43:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:44:37 | INFO | train_inner | epoch 171:     60 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.118, nll_loss=0.899, ppl=1.87, wps=9986.5, ups=0.95, wpb=10511.4, bsz=372, num_updates=22500, lr=1.46059e-05, gnorm=0.835, train_wall=66, wall=0
2021-01-28 15:45:42 | INFO | train_inner | epoch 171:    160 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.103, nll_loss=0.892, ppl=1.86, wps=16207.6, ups=1.54, wpb=10547.5, bsz=378.6, num_updates=22600, lr=1.45736e-05, gnorm=0.814, train_wall=65, wall=0
2021-01-28 15:46:48 | INFO | train_inner | epoch 171:    260 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.12, nll_loss=0.9, ppl=1.87, wps=15866, ups=1.52, wpb=10404.7, bsz=354.6, num_updates=22700, lr=1.45414e-05, gnorm=0.847, train_wall=65, wall=0
2021-01-28 15:47:53 | INFO | train_inner | epoch 171:    360 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.097, nll_loss=0.883, ppl=1.84, wps=16225.3, ups=1.54, wpb=10526.5, bsz=379.4, num_updates=22800, lr=1.45095e-05, gnorm=0.817, train_wall=64, wall=0
2021-01-28 15:48:59 | INFO | train_inner | epoch 171:    460 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.125, nll_loss=0.906, ppl=1.87, wps=15715.1, ups=1.5, wpb=10442.4, bsz=377.6, num_updates=22900, lr=1.44778e-05, gnorm=0.826, train_wall=66, wall=0
2021-01-28 15:50:05 | INFO | train_inner | epoch 171:    560 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.112, nll_loss=0.898, ppl=1.86, wps=15844.4, ups=1.51, wpb=10496.1, bsz=362, num_updates=23000, lr=1.44463e-05, gnorm=0.826, train_wall=66, wall=0
2021-01-28 15:50:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 15:50:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:50:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:50:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:50:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:50:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:50:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:50:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:50:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:50:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:50:38 | INFO | valid | epoch 171 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.229 | nll_loss 3.714 | ppl 13.12 | bleu 23.13 | wps 2951.1 | wpb 7508.5 | bsz 272.7 | num_updates 23001 | best_bleu 23.39
2021-01-28 15:50:38 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 15:50:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:50:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:50:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:50:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:50:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 171 @ 23001 updates, score 23.13) (writing took 3.1694854850065894 seconds)
2021-01-28 15:50:42 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2021-01-28 15:50:42 | INFO | train | epoch 171 | symm_kl 0.302 | self_kl 0 | self_cv 0 | loss 3.112 | nll_loss 0.896 | ppl 1.86 | wps 14392.8 | ups 1.37 | wpb 10483.4 | bsz 369.6 | num_updates 23001 | lr 1.4446e-05 | gnorm 0.826 | train_wall 366 | wall 0
2021-01-28 15:50:42 | INFO | fairseq.trainer | begin training epoch 172
2021-01-28 15:50:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:50:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:51:51 | INFO | train_inner | epoch 172:     99 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.115, nll_loss=0.897, ppl=1.86, wps=9749.3, ups=0.95, wpb=10273.8, bsz=351.8, num_updates=23100, lr=1.4415e-05, gnorm=0.835, train_wall=65, wall=0
2021-01-28 15:52:57 | INFO | train_inner | epoch 172:    199 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.106, nll_loss=0.892, ppl=1.86, wps=15791.2, ups=1.51, wpb=10442.6, bsz=376.6, num_updates=23200, lr=1.43839e-05, gnorm=0.825, train_wall=66, wall=0
2021-01-28 15:54:03 | INFO | train_inner | epoch 172:    299 / 561 symm_kl=0.305, self_kl=0, self_cv=0, loss=3.122, nll_loss=0.9, ppl=1.87, wps=16127.5, ups=1.52, wpb=10588.9, bsz=365.8, num_updates=23300, lr=1.4353e-05, gnorm=0.833, train_wall=65, wall=0
2021-01-28 15:55:08 | INFO | train_inner | epoch 172:    399 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.103, nll_loss=0.89, ppl=1.85, wps=16054.4, ups=1.52, wpb=10558.5, bsz=380.1, num_updates=23400, lr=1.43223e-05, gnorm=0.82, train_wall=65, wall=0
2021-01-28 15:56:13 | INFO | train_inner | epoch 172:    499 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.115, nll_loss=0.899, ppl=1.87, wps=16386.3, ups=1.55, wpb=10568.6, bsz=378.1, num_updates=23500, lr=1.42918e-05, gnorm=0.826, train_wall=64, wall=0
2021-01-28 15:56:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 15:56:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:56:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:56:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:56:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:56:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:56:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:56:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:56:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:56:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:56:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:56:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:56:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:57:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:57:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:57:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:57:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:57:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:57:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:57:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:57:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:57:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:57:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:57:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:57:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:57:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:57:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:57:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:57:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:57:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:57:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:57:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:57:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:57:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:57:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:57:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:57:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:57:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:57:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:57:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:57:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:57:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:57:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:57:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 15:57:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 15:57:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 15:57:22 | INFO | valid | epoch 172 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.226 | nll_loss 3.713 | ppl 13.12 | bleu 23.19 | wps 3307.1 | wpb 7508.5 | bsz 272.7 | num_updates 23562 | best_bleu 23.39
2021-01-28 15:57:22 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 15:57:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:57:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:57:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:57:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:57:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 172 @ 23562 updates, score 23.19) (writing took 3.351965328969527 seconds)
2021-01-28 15:57:26 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2021-01-28 15:57:26 | INFO | train | epoch 172 | symm_kl 0.302 | self_kl 0 | self_cv 0 | loss 3.111 | nll_loss 0.895 | ppl 1.86 | wps 14549.4 | ups 1.39 | wpb 10483.4 | bsz 369.6 | num_updates 23562 | lr 1.4273e-05 | gnorm 0.828 | train_wall 364 | wall 0
2021-01-28 15:57:26 | INFO | fairseq.trainer | begin training epoch 173
2021-01-28 15:57:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 15:57:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 15:57:52 | INFO | train_inner | epoch 173:     38 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.107, nll_loss=0.89, ppl=1.85, wps=10485, ups=1.01, wpb=10356.3, bsz=357.6, num_updates=23600, lr=1.42615e-05, gnorm=0.83, train_wall=62, wall=0
2021-01-28 15:58:50 | INFO | train_inner | epoch 173:    138 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.112, nll_loss=0.894, ppl=1.86, wps=17741.2, ups=1.7, wpb=10425.5, bsz=349.2, num_updates=23700, lr=1.42314e-05, gnorm=0.826, train_wall=59, wall=0
2021-01-28 15:59:50 | INFO | train_inner | epoch 173:    238 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.112, nll_loss=0.896, ppl=1.86, wps=17746.4, ups=1.68, wpb=10574.1, bsz=377.6, num_updates=23800, lr=1.42014e-05, gnorm=0.817, train_wall=59, wall=0
2021-01-28 16:00:50 | INFO | train_inner | epoch 173:    338 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.12, nll_loss=0.902, ppl=1.87, wps=17499, ups=1.67, wpb=10461.4, bsz=379.8, num_updates=23900, lr=1.41717e-05, gnorm=0.836, train_wall=60, wall=0
2021-01-28 16:01:50 | INFO | train_inner | epoch 173:    438 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.107, nll_loss=0.895, ppl=1.86, wps=17511.2, ups=1.66, wpb=10525.6, bsz=371.7, num_updates=24000, lr=1.41421e-05, gnorm=0.821, train_wall=60, wall=0
2021-01-28 16:02:58 | INFO | train_inner | epoch 173:    538 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.105, nll_loss=0.89, ppl=1.85, wps=15626.7, ups=1.48, wpb=10559.4, bsz=375.9, num_updates=24100, lr=1.41128e-05, gnorm=0.83, train_wall=67, wall=0
2021-01-28 16:03:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 16:03:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:03:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:03:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:03:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:03:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:03:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:03:45 | INFO | valid | epoch 173 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.232 | nll_loss 3.716 | ppl 13.14 | bleu 23.2 | wps 3099.7 | wpb 7508.5 | bsz 272.7 | num_updates 24123 | best_bleu 23.39
2021-01-28 16:03:45 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 16:03:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:03:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:03:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 173 @ 24123 updates, score 23.2) (writing took 3.282656883005984 seconds)
2021-01-28 16:03:48 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2021-01-28 16:03:48 | INFO | train | epoch 173 | symm_kl 0.301 | self_kl 0 | self_cv 0 | loss 3.11 | nll_loss 0.895 | ppl 1.86 | wps 15381.6 | ups 1.47 | wpb 10483.4 | bsz 369.6 | num_updates 24123 | lr 1.4106e-05 | gnorm 0.826 | train_wall 342 | wall 0
2021-01-28 16:03:48 | INFO | fairseq.trainer | begin training epoch 174
2021-01-28 16:03:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:03:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:03:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:03:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:04:43 | INFO | train_inner | epoch 174:     77 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.107, nll_loss=0.891, ppl=1.85, wps=9836.9, ups=0.95, wpb=10338.5, bsz=372, num_updates=24200, lr=1.40836e-05, gnorm=0.823, train_wall=65, wall=0
2021-01-28 16:05:49 | INFO | train_inner | epoch 174:    177 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.117, nll_loss=0.9, ppl=1.87, wps=15782.3, ups=1.52, wpb=10410.1, bsz=380.6, num_updates=24300, lr=1.40546e-05, gnorm=0.824, train_wall=65, wall=0
2021-01-28 16:06:55 | INFO | train_inner | epoch 174:    277 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.101, nll_loss=0.89, ppl=1.85, wps=16025.9, ups=1.51, wpb=10613.8, bsz=377.8, num_updates=24400, lr=1.40257e-05, gnorm=0.812, train_wall=66, wall=0
2021-01-28 16:08:02 | INFO | train_inner | epoch 174:    377 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.096, nll_loss=0.884, ppl=1.84, wps=15747.4, ups=1.49, wpb=10539.8, bsz=368.8, num_updates=24500, lr=1.39971e-05, gnorm=0.822, train_wall=67, wall=0
2021-01-28 16:09:09 | INFO | train_inner | epoch 174:    477 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.113, nll_loss=0.896, ppl=1.86, wps=15528.3, ups=1.49, wpb=10449.3, bsz=369.5, num_updates=24600, lr=1.39686e-05, gnorm=0.831, train_wall=67, wall=0
2021-01-28 16:10:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 16:10:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:10:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:10:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:10:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:10:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:10:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:10:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:10:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:10:37 | INFO | valid | epoch 174 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.23 | nll_loss 3.716 | ppl 13.14 | bleu 23.13 | wps 2876.8 | wpb 7508.5 | bsz 272.7 | num_updates 24684 | best_bleu 23.39
2021-01-28 16:10:37 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 16:10:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:10:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:10:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 174 @ 24684 updates, score 23.13) (writing took 4.194536212016828 seconds)
2021-01-28 16:10:42 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2021-01-28 16:10:42 | INFO | train | epoch 174 | symm_kl 0.301 | self_kl 0 | self_cv 0 | loss 3.11 | nll_loss 0.894 | ppl 1.86 | wps 14225.9 | ups 1.36 | wpb 10483.4 | bsz 369.6 | num_updates 24684 | lr 1.39448e-05 | gnorm 0.825 | train_wall 370 | wall 0
2021-01-28 16:10:42 | INFO | fairseq.trainer | begin training epoch 175
2021-01-28 16:10:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:10:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:10:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:10:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:10:56 | INFO | train_inner | epoch 175:     16 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.125, nll_loss=0.903, ppl=1.87, wps=9793, ups=0.94, wpb=10429.7, bsz=341.7, num_updates=24700, lr=1.39403e-05, gnorm=0.842, train_wall=66, wall=0
2021-01-28 16:11:58 | INFO | train_inner | epoch 175:    116 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.098, nll_loss=0.883, ppl=1.84, wps=16663.9, ups=1.59, wpb=10467.8, bsz=372, num_updates=24800, lr=1.39122e-05, gnorm=0.827, train_wall=62, wall=0
2021-01-28 16:13:05 | INFO | train_inner | epoch 175:    216 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.102, nll_loss=0.89, ppl=1.85, wps=15913.6, ups=1.5, wpb=10617.7, bsz=380.3, num_updates=24900, lr=1.38842e-05, gnorm=0.818, train_wall=66, wall=0
2021-01-28 16:14:08 | INFO | train_inner | epoch 175:    316 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.113, nll_loss=0.899, ppl=1.86, wps=16500.8, ups=1.58, wpb=10458.2, bsz=390.2, num_updates=25000, lr=1.38564e-05, gnorm=0.823, train_wall=63, wall=0
2021-01-28 16:15:15 | INFO | train_inner | epoch 175:    416 / 561 symm_kl=0.307, self_kl=0, self_cv=0, loss=3.135, nll_loss=0.914, ppl=1.88, wps=15652, ups=1.5, wpb=10452.7, bsz=354.8, num_updates=25100, lr=1.38288e-05, gnorm=0.837, train_wall=66, wall=0
2021-01-28 16:16:21 | INFO | train_inner | epoch 175:    516 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.11, nll_loss=0.894, ppl=1.86, wps=15874, ups=1.52, wpb=10439.9, bsz=362, num_updates=25200, lr=1.38013e-05, gnorm=0.833, train_wall=65, wall=0
2021-01-28 16:16:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 16:16:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:16:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:16:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:16:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:16:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:16:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:16:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:16:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:16:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:16:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:16:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:16:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:16:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:16:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:16:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:16:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:16:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:16:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:16:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:16:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:16:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:16:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:16:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:16:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:16:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:16:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:16:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:16:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:16:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:16:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:16:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:16:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:16:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:16:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:16:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:16:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:16:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:16:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:16:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:17:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:17:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:17:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:17:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:17:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:17:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:17:09 | INFO | valid | epoch 175 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.225 | nll_loss 3.712 | ppl 13.11 | bleu 23.16 | wps 4455.1 | wpb 7508.5 | bsz 272.7 | num_updates 25245 | best_bleu 23.39
2021-01-28 16:17:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 16:17:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:17:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:17:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:17:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:17:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 175 @ 25245 updates, score 23.16) (writing took 3.1609857580042444 seconds)
2021-01-28 16:17:13 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2021-01-28 16:17:13 | INFO | train | epoch 175 | symm_kl 0.301 | self_kl 0 | self_cv 0 | loss 3.11 | nll_loss 0.895 | ppl 1.86 | wps 15037.5 | ups 1.43 | wpb 10483.4 | bsz 369.6 | num_updates 25245 | lr 1.3789e-05 | gnorm 0.828 | train_wall 360 | wall 0
2021-01-28 16:17:13 | INFO | fairseq.trainer | begin training epoch 176
2021-01-28 16:17:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:17:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:17:51 | INFO | train_inner | epoch 176:     55 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.1, nll_loss=0.888, ppl=1.85, wps=11670, ups=1.11, wpb=10533.1, bsz=363.1, num_updates=25300, lr=1.3774e-05, gnorm=0.817, train_wall=62, wall=0
2021-01-28 16:18:56 | INFO | train_inner | epoch 176:    155 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.119, nll_loss=0.901, ppl=1.87, wps=16183.5, ups=1.54, wpb=10515.3, bsz=361.3, num_updates=25400, lr=1.37469e-05, gnorm=0.826, train_wall=65, wall=0
2021-01-28 16:20:01 | INFO | train_inner | epoch 176:    255 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.103, nll_loss=0.89, ppl=1.85, wps=16120.9, ups=1.54, wpb=10478.8, bsz=381, num_updates=25500, lr=1.37199e-05, gnorm=0.821, train_wall=65, wall=0
2021-01-28 16:21:03 | INFO | train_inner | epoch 176:    355 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.113, nll_loss=0.898, ppl=1.86, wps=16804.7, ups=1.62, wpb=10402.3, bsz=385.7, num_updates=25600, lr=1.36931e-05, gnorm=0.841, train_wall=62, wall=0
2021-01-28 16:22:03 | INFO | train_inner | epoch 176:    455 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.104, nll_loss=0.889, ppl=1.85, wps=17723, ups=1.68, wpb=10526.4, bsz=359, num_updates=25700, lr=1.36664e-05, gnorm=0.817, train_wall=59, wall=0
2021-01-28 16:23:02 | INFO | train_inner | epoch 176:    555 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.103, nll_loss=0.889, ppl=1.85, wps=17680.5, ups=1.68, wpb=10530.7, bsz=363.7, num_updates=25800, lr=1.36399e-05, gnorm=0.832, train_wall=59, wall=0
2021-01-28 16:23:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 16:23:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:23:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:23:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:23:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:23:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:23:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:23:27 | INFO | valid | epoch 176 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.227 | nll_loss 3.713 | ppl 13.11 | bleu 23.14 | wps 4582.3 | wpb 7508.5 | bsz 272.7 | num_updates 25806 | best_bleu 23.39
2021-01-28 16:23:27 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 16:23:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:23:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:23:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:23:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:23:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 176 @ 25806 updates, score 23.14) (writing took 3.229459701979067 seconds)
2021-01-28 16:23:30 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2021-01-28 16:23:30 | INFO | train | epoch 176 | symm_kl 0.3 | self_kl 0 | self_cv 0 | loss 3.108 | nll_loss 0.893 | ppl 1.86 | wps 15580 | ups 1.49 | wpb 10483.4 | bsz 369.6 | num_updates 25806 | lr 1.36383e-05 | gnorm 0.826 | train_wall 348 | wall 0
2021-01-28 16:23:30 | INFO | fairseq.trainer | begin training epoch 177
2021-01-28 16:23:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:23:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:24:36 | INFO | train_inner | epoch 177:     94 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.102, nll_loss=0.892, ppl=1.86, wps=11128.5, ups=1.07, wpb=10440.3, bsz=375.5, num_updates=25900, lr=1.36135e-05, gnorm=0.819, train_wall=66, wall=0
2021-01-28 16:25:43 | INFO | train_inner | epoch 177:    194 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.11, nll_loss=0.891, ppl=1.85, wps=15636.3, ups=1.48, wpb=10551.5, bsz=367.8, num_updates=26000, lr=1.35873e-05, gnorm=0.833, train_wall=67, wall=0
2021-01-28 16:26:48 | INFO | train_inner | epoch 177:    294 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.112, nll_loss=0.898, ppl=1.86, wps=15845.5, ups=1.55, wpb=10246.9, bsz=373.6, num_updates=26100, lr=1.35613e-05, gnorm=0.832, train_wall=64, wall=0
2021-01-28 16:27:54 | INFO | train_inner | epoch 177:    394 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.099, nll_loss=0.888, ppl=1.85, wps=15921.7, ups=1.51, wpb=10533.5, bsz=371.4, num_updates=26200, lr=1.35354e-05, gnorm=0.816, train_wall=66, wall=0
2021-01-28 16:28:58 | INFO | train_inner | epoch 177:    494 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.105, nll_loss=0.888, ppl=1.85, wps=16503.2, ups=1.56, wpb=10599, bsz=354.1, num_updates=26300, lr=1.35096e-05, gnorm=0.826, train_wall=64, wall=0
2021-01-28 16:29:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 16:29:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:29:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:29:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:29:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:29:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:29:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:29:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:29:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:29:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:29:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:29:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:29:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:29:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:29:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:29:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:29:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:29:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:29:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:29:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:30:00 | INFO | valid | epoch 177 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.226 | nll_loss 3.711 | ppl 13.09 | bleu 23.24 | wps 4634.7 | wpb 7508.5 | bsz 272.7 | num_updates 26367 | best_bleu 23.39
2021-01-28 16:30:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 16:30:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:30:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:30:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:30:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:30:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 177 @ 26367 updates, score 23.24) (writing took 3.045590182999149 seconds)
2021-01-28 16:30:03 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2021-01-28 16:30:03 | INFO | train | epoch 177 | symm_kl 0.301 | self_kl 0 | self_cv 0 | loss 3.108 | nll_loss 0.892 | ppl 1.86 | wps 14971.9 | ups 1.43 | wpb 10483.4 | bsz 369.6 | num_updates 26367 | lr 1.34924e-05 | gnorm 0.825 | train_wall 362 | wall 0
2021-01-28 16:30:03 | INFO | fairseq.trainer | begin training epoch 178
2021-01-28 16:30:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:30:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:30:25 | INFO | train_inner | epoch 178:     33 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.115, nll_loss=0.898, ppl=1.86, wps=12063.7, ups=1.15, wpb=10481.2, bsz=377.4, num_updates=26400, lr=1.3484e-05, gnorm=0.828, train_wall=58, wall=0
2021-01-28 16:31:24 | INFO | train_inner | epoch 178:    133 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.107, nll_loss=0.895, ppl=1.86, wps=17913.1, ups=1.71, wpb=10494.5, bsz=382.6, num_updates=26500, lr=1.34585e-05, gnorm=0.816, train_wall=58, wall=0
2021-01-28 16:32:23 | INFO | train_inner | epoch 178:    233 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.117, nll_loss=0.901, ppl=1.87, wps=17773.7, ups=1.68, wpb=10566.7, bsz=368.2, num_updates=26600, lr=1.34332e-05, gnorm=0.823, train_wall=59, wall=0
2021-01-28 16:33:23 | INFO | train_inner | epoch 178:    333 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.11, nll_loss=0.893, ppl=1.86, wps=17569.8, ups=1.67, wpb=10547.9, bsz=371.6, num_updates=26700, lr=1.3408e-05, gnorm=0.827, train_wall=60, wall=0
2021-01-28 16:34:23 | INFO | train_inner | epoch 178:    433 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.099, nll_loss=0.884, ppl=1.85, wps=17632.9, ups=1.68, wpb=10525.3, bsz=371.8, num_updates=26800, lr=1.3383e-05, gnorm=0.817, train_wall=60, wall=0
2021-01-28 16:35:23 | INFO | train_inner | epoch 178:    533 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.115, nll_loss=0.898, ppl=1.86, wps=17367.9, ups=1.68, wpb=10313.5, bsz=354.9, num_updates=26900, lr=1.33581e-05, gnorm=0.842, train_wall=59, wall=0
2021-01-28 16:35:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 16:35:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:35:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:35:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:35:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:35:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:35:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:35:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:35:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:35:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:35:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:35:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:35:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:35:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:36:00 | INFO | valid | epoch 178 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.226 | nll_loss 3.712 | ppl 13.11 | bleu 23.16 | wps 4652.1 | wpb 7508.5 | bsz 272.7 | num_updates 26928 | best_bleu 23.39
2021-01-28 16:36:00 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 16:36:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:36:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:36:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:36:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:36:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 178 @ 26928 updates, score 23.16) (writing took 3.1209951019845903 seconds)
2021-01-28 16:36:03 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2021-01-28 16:36:03 | INFO | train | epoch 178 | symm_kl 0.3 | self_kl 0 | self_cv 0 | loss 3.107 | nll_loss 0.892 | ppl 1.86 | wps 16326.2 | ups 1.56 | wpb 10483.4 | bsz 369.6 | num_updates 26928 | lr 1.33511e-05 | gnorm 0.826 | train_wall 332 | wall 0
2021-01-28 16:36:03 | INFO | fairseq.trainer | begin training epoch 179
2021-01-28 16:36:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:36:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:36:48 | INFO | train_inner | epoch 179:     72 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.102, nll_loss=0.887, ppl=1.85, wps=12223, ups=1.17, wpb=10465.5, bsz=359.2, num_updates=27000, lr=1.33333e-05, gnorm=0.823, train_wall=58, wall=0
2021-01-28 16:37:48 | INFO | train_inner | epoch 179:    172 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.106, nll_loss=0.891, ppl=1.85, wps=17612, ups=1.67, wpb=10564.1, bsz=359.1, num_updates=27100, lr=1.33087e-05, gnorm=0.834, train_wall=60, wall=0
2021-01-28 16:38:48 | INFO | train_inner | epoch 179:    272 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.11, nll_loss=0.897, ppl=1.86, wps=17682, ups=1.68, wpb=10499.7, bsz=377.4, num_updates=27200, lr=1.32842e-05, gnorm=0.817, train_wall=59, wall=0
2021-01-28 16:39:48 | INFO | train_inner | epoch 179:    372 / 561 symm_kl=0.294, self_kl=0, self_cv=0, loss=3.08, nll_loss=0.874, ppl=1.83, wps=17462.5, ups=1.67, wpb=10477, bsz=384.8, num_updates=27300, lr=1.32599e-05, gnorm=0.817, train_wall=60, wall=0
2021-01-28 16:40:47 | INFO | train_inner | epoch 179:    472 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.118, nll_loss=0.903, ppl=1.87, wps=17591, ups=1.68, wpb=10472.7, bsz=371, num_updates=27400, lr=1.32357e-05, gnorm=0.825, train_wall=59, wall=0
2021-01-28 16:41:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 16:41:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:41:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:41:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:41:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:41:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:41:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:41:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:41:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:41:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:41:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:41:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:41:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:41:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:41:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:41:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:41:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:41:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:41:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:41:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:41:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:41:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:42:01 | INFO | valid | epoch 179 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.228 | nll_loss 3.712 | ppl 13.11 | bleu 23.16 | wps 4611.2 | wpb 7508.5 | bsz 272.7 | num_updates 27489 | best_bleu 23.39
2021-01-28 16:42:01 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 16:42:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:42:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:42:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:42:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:42:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 179 @ 27489 updates, score 23.16) (writing took 3.0949652660056017 seconds)
2021-01-28 16:42:04 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2021-01-28 16:42:04 | INFO | train | epoch 179 | symm_kl 0.3 | self_kl 0 | self_cv 0 | loss 3.106 | nll_loss 0.891 | ppl 1.86 | wps 16297.5 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 27489 | lr 1.32142e-05 | gnorm 0.826 | train_wall 332 | wall 0
2021-01-28 16:42:04 | INFO | fairseq.trainer | begin training epoch 180
2021-01-28 16:42:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:42:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:42:14 | INFO | train_inner | epoch 180:     11 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.109, nll_loss=0.89, ppl=1.85, wps=11997.1, ups=1.15, wpb=10402.7, bsz=359, num_updates=27500, lr=1.32116e-05, gnorm=0.845, train_wall=59, wall=0
2021-01-28 16:43:13 | INFO | train_inner | epoch 180:    111 / 561 symm_kl=0.292, self_kl=0, self_cv=0, loss=3.077, nll_loss=0.872, ppl=1.83, wps=17990.2, ups=1.7, wpb=10607.8, bsz=380, num_updates=27600, lr=1.31876e-05, gnorm=0.805, train_wall=59, wall=0
2021-01-28 16:44:13 | INFO | train_inner | epoch 180:    211 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.108, nll_loss=0.893, ppl=1.86, wps=17614.8, ups=1.67, wpb=10532.3, bsz=362.9, num_updates=27700, lr=1.31638e-05, gnorm=0.825, train_wall=60, wall=0
2021-01-28 16:45:12 | INFO | train_inner | epoch 180:    311 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.111, nll_loss=0.896, ppl=1.86, wps=17783.3, ups=1.69, wpb=10535.4, bsz=371.6, num_updates=27800, lr=1.31401e-05, gnorm=0.818, train_wall=59, wall=0
2021-01-28 16:46:11 | INFO | train_inner | epoch 180:    411 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.105, nll_loss=0.892, ppl=1.86, wps=17473.9, ups=1.68, wpb=10372.4, bsz=375, num_updates=27900, lr=1.31165e-05, gnorm=0.831, train_wall=59, wall=0
2021-01-28 16:47:10 | INFO | train_inner | epoch 180:    511 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.114, nll_loss=0.895, ppl=1.86, wps=17507.2, ups=1.69, wpb=10370.3, bsz=365, num_updates=28000, lr=1.30931e-05, gnorm=0.843, train_wall=59, wall=0
2021-01-28 16:47:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 16:47:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:47:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:47:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:47:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:47:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:47:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:47:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:47:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:47:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:47:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:47:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:47:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:47:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:47:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:47:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:47:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:47:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:47:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:47:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:47:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:47:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:47:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:47:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:47:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:47:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:47:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:47:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:47:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:47:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:47:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:47:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:47:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:47:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:47:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:47:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:47:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:47:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:47:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:47:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:48:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:48:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:48:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:48:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:48:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:48:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:48:05 | INFO | valid | epoch 180 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.228 | nll_loss 3.713 | ppl 13.11 | bleu 23.09 | wps 4094.5 | wpb 7508.5 | bsz 272.7 | num_updates 28050 | best_bleu 23.39
2021-01-28 16:48:05 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 16:48:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:48:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:48:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:48:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:48:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 180 @ 28050 updates, score 23.09) (writing took 3.1182064039749093 seconds)
2021-01-28 16:48:08 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2021-01-28 16:48:08 | INFO | train | epoch 180 | symm_kl 0.3 | self_kl 0 | self_cv 0 | loss 3.105 | nll_loss 0.891 | ppl 1.85 | wps 16162.7 | ups 1.54 | wpb 10483.4 | bsz 369.6 | num_updates 28050 | lr 1.30814e-05 | gnorm 0.826 | train_wall 332 | wall 0
2021-01-28 16:48:08 | INFO | fairseq.trainer | begin training epoch 181
2021-01-28 16:48:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:48:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:48:42 | INFO | train_inner | epoch 181:     50 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.106, nll_loss=0.891, ppl=1.85, wps=11389.8, ups=1.1, wpb=10388.8, bsz=369.8, num_updates=28100, lr=1.30698e-05, gnorm=0.838, train_wall=58, wall=0
2021-01-28 16:49:41 | INFO | train_inner | epoch 181:    150 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.109, nll_loss=0.892, ppl=1.86, wps=17663.5, ups=1.68, wpb=10502.5, bsz=371.9, num_updates=28200, lr=1.30466e-05, gnorm=0.829, train_wall=59, wall=0
2021-01-28 16:50:41 | INFO | train_inner | epoch 181:    250 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.117, nll_loss=0.899, ppl=1.87, wps=17673, ups=1.68, wpb=10524.5, bsz=363.7, num_updates=28300, lr=1.30235e-05, gnorm=0.833, train_wall=59, wall=0
2021-01-28 16:51:40 | INFO | train_inner | epoch 181:    350 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.103, nll_loss=0.89, ppl=1.85, wps=17810.2, ups=1.68, wpb=10625.4, bsz=365.4, num_updates=28400, lr=1.30005e-05, gnorm=0.82, train_wall=59, wall=0
2021-01-28 16:52:40 | INFO | train_inner | epoch 181:    450 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.096, nll_loss=0.886, ppl=1.85, wps=17679.8, ups=1.68, wpb=10541.8, bsz=382.4, num_updates=28500, lr=1.29777e-05, gnorm=0.832, train_wall=59, wall=0
2021-01-28 16:53:39 | INFO | train_inner | epoch 181:    550 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.103, nll_loss=0.891, ppl=1.85, wps=17532.7, ups=1.7, wpb=10318, bsz=364.3, num_updates=28600, lr=1.2955e-05, gnorm=0.829, train_wall=59, wall=0
2021-01-28 16:53:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 16:53:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:53:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:53:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:53:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:53:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:53:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:53:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:53:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:53:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:53:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:53:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:53:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:53:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:53:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:53:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:53:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:53:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:53:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:53:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:53:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:53:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:53:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:53:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:53:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:53:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:53:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:53:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:53:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:53:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:53:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:53:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:53:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:53:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:53:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:53:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:53:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:53:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:53:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:53:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:54:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:54:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:54:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:54:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:54:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:54:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:54:06 | INFO | valid | epoch 181 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.229 | nll_loss 3.715 | ppl 13.13 | bleu 23.11 | wps 4597.8 | wpb 7508.5 | bsz 272.7 | num_updates 28611 | best_bleu 23.39
2021-01-28 16:54:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 16:54:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:54:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:54:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:54:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:54:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 181 @ 28611 updates, score 23.11) (writing took 3.136600872967392 seconds)
2021-01-28 16:54:10 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2021-01-28 16:54:10 | INFO | train | epoch 181 | symm_kl 0.3 | self_kl 0 | self_cv 0 | loss 3.104 | nll_loss 0.891 | ppl 1.85 | wps 16261.7 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 28611 | lr 1.29525e-05 | gnorm 0.831 | train_wall 331 | wall 0
2021-01-28 16:54:10 | INFO | fairseq.trainer | begin training epoch 182
2021-01-28 16:54:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:54:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:55:05 | INFO | train_inner | epoch 182:     89 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.107, nll_loss=0.893, ppl=1.86, wps=12060.2, ups=1.16, wpb=10361.6, bsz=357.8, num_updates=28700, lr=1.29324e-05, gnorm=0.838, train_wall=58, wall=0
2021-01-28 16:56:04 | INFO | train_inner | epoch 182:    189 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.097, nll_loss=0.886, ppl=1.85, wps=17614.3, ups=1.68, wpb=10470.5, bsz=376.4, num_updates=28800, lr=1.29099e-05, gnorm=0.827, train_wall=59, wall=0
2021-01-28 16:57:04 | INFO | train_inner | epoch 182:    289 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.101, nll_loss=0.889, ppl=1.85, wps=17631.5, ups=1.68, wpb=10501.4, bsz=377.6, num_updates=28900, lr=1.28876e-05, gnorm=0.82, train_wall=59, wall=0
2021-01-28 16:58:03 | INFO | train_inner | epoch 182:    389 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.106, nll_loss=0.889, ppl=1.85, wps=17604.6, ups=1.68, wpb=10488.2, bsz=359.7, num_updates=29000, lr=1.28654e-05, gnorm=0.839, train_wall=59, wall=0
2021-01-28 16:59:03 | INFO | train_inner | epoch 182:    489 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.106, nll_loss=0.891, ppl=1.85, wps=17902.9, ups=1.68, wpb=10652.9, bsz=371.3, num_updates=29100, lr=1.28432e-05, gnorm=0.818, train_wall=59, wall=0
2021-01-28 16:59:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 16:59:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:59:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:59:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 16:59:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:59:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:59:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 16:59:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:59:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:59:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:59:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:59:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:59:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:59:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:59:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:59:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:59:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:59:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:59:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:59:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:59:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:59:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:59:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:59:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:59:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:59:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:59:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:59:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:59:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:59:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:59:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:59:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:59:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:59:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:59:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:59:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:59:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 16:59:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 16:59:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 16:59:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:00:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:00:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:00:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:00:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:00:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:00:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:00:06 | INFO | valid | epoch 182 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.225 | nll_loss 3.712 | ppl 13.11 | bleu 23.2 | wps 4592 | wpb 7508.5 | bsz 272.7 | num_updates 29172 | best_bleu 23.39
2021-01-28 17:00:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 17:00:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:00:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:00:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:00:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:00:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 182 @ 29172 updates, score 23.2) (writing took 3.110033516015392 seconds)
2021-01-28 17:00:10 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2021-01-28 17:00:10 | INFO | train | epoch 182 | symm_kl 0.299 | self_kl 0 | self_cv 0 | loss 3.103 | nll_loss 0.89 | ppl 1.85 | wps 16334.5 | ups 1.56 | wpb 10483.4 | bsz 369.6 | num_updates 29172 | lr 1.28274e-05 | gnorm 0.828 | train_wall 332 | wall 0
2021-01-28 17:00:10 | INFO | fairseq.trainer | begin training epoch 183
2021-01-28 17:00:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:00:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:00:29 | INFO | train_inner | epoch 183:     28 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.108, nll_loss=0.895, ppl=1.86, wps=12061.2, ups=1.16, wpb=10393.1, bsz=375, num_updates=29200, lr=1.28212e-05, gnorm=0.835, train_wall=58, wall=0
2021-01-28 17:01:28 | INFO | train_inner | epoch 183:    128 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.105, nll_loss=0.892, ppl=1.86, wps=17902.8, ups=1.69, wpb=10581.2, bsz=381.7, num_updates=29300, lr=1.27993e-05, gnorm=0.818, train_wall=59, wall=0
2021-01-28 17:02:28 | INFO | train_inner | epoch 183:    228 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.104, nll_loss=0.889, ppl=1.85, wps=17518.2, ups=1.67, wpb=10497.3, bsz=361, num_updates=29400, lr=1.27775e-05, gnorm=0.827, train_wall=60, wall=0
2021-01-28 17:03:27 | INFO | train_inner | epoch 183:    328 / 561 symm_kl=0.296, self_kl=0, self_cv=0, loss=3.097, nll_loss=0.889, ppl=1.85, wps=17554.2, ups=1.68, wpb=10437.3, bsz=391.4, num_updates=29500, lr=1.27559e-05, gnorm=0.821, train_wall=59, wall=0
2021-01-28 17:04:27 | INFO | train_inner | epoch 183:    428 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.1, nll_loss=0.885, ppl=1.85, wps=17695.5, ups=1.68, wpb=10517, bsz=350.9, num_updates=29600, lr=1.27343e-05, gnorm=0.838, train_wall=59, wall=0
2021-01-28 17:05:26 | INFO | train_inner | epoch 183:    528 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.094, nll_loss=0.882, ppl=1.84, wps=17688.2, ups=1.68, wpb=10499.1, bsz=367.2, num_updates=29700, lr=1.27128e-05, gnorm=0.826, train_wall=59, wall=0
2021-01-28 17:05:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 17:05:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:05:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:05:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:05:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:05:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:05:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:05:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:05:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:05:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:05:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:05:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:05:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:05:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:05:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:05:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:05:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:05:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:05:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:05:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:05:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:05:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:05:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:05:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:05:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:05:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:05:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:05:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:05:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:05:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:05:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:05:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:05:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:05:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:05:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:05:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:05:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:05:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:05:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:05:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:06:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:06:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:06:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:06:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:06:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:06:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:06:07 | INFO | valid | epoch 183 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.229 | nll_loss 3.713 | ppl 13.11 | bleu 23.35 | wps 4625.2 | wpb 7508.5 | bsz 272.7 | num_updates 29733 | best_bleu 23.39
2021-01-28 17:06:07 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 17:06:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:06:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:06:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:06:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:06:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 183 @ 29733 updates, score 23.35) (writing took 3.030028465029318 seconds)
2021-01-28 17:06:10 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2021-01-28 17:06:10 | INFO | train | epoch 183 | symm_kl 0.299 | self_kl 0 | self_cv 0 | loss 3.103 | nll_loss 0.89 | ppl 1.85 | wps 16337.6 | ups 1.56 | wpb 10483.4 | bsz 369.6 | num_updates 29733 | lr 1.27058e-05 | gnorm 0.829 | train_wall 332 | wall 0
2021-01-28 17:06:10 | INFO | fairseq.trainer | begin training epoch 184
2021-01-28 17:06:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:06:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:06:52 | INFO | train_inner | epoch 184:     67 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.118, nll_loss=0.899, ppl=1.86, wps=12030, ups=1.17, wpb=10314.1, bsz=365.2, num_updates=29800, lr=1.26915e-05, gnorm=0.851, train_wall=58, wall=0
2021-01-28 17:07:51 | INFO | train_inner | epoch 184:    167 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.107, nll_loss=0.895, ppl=1.86, wps=17550.7, ups=1.68, wpb=10448.3, bsz=380.8, num_updates=29900, lr=1.26702e-05, gnorm=0.83, train_wall=59, wall=0
2021-01-28 17:08:52 | INFO | train_inner | epoch 184:    267 / 561 symm_kl=0.296, self_kl=0, self_cv=0, loss=3.095, nll_loss=0.886, ppl=1.85, wps=17520.4, ups=1.66, wpb=10576, bsz=370.1, num_updates=30000, lr=1.26491e-05, gnorm=0.813, train_wall=60, wall=0
2021-01-28 17:09:51 | INFO | train_inner | epoch 184:    367 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.106, nll_loss=0.886, ppl=1.85, wps=17410.6, ups=1.69, wpb=10314.7, bsz=369.1, num_updates=30100, lr=1.26281e-05, gnorm=0.854, train_wall=59, wall=0
2021-01-28 17:10:51 | INFO | train_inner | epoch 184:    467 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.097, nll_loss=0.887, ppl=1.85, wps=17802.9, ups=1.67, wpb=10644.3, bsz=379.8, num_updates=30200, lr=1.26072e-05, gnorm=0.816, train_wall=60, wall=0
2021-01-28 17:11:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 17:11:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:11:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:11:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:11:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:11:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:11:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:11:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:11:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:11:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:11:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:11:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:11:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:11:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:11:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:11:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:11:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:11:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:11:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:11:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:11:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:11:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:11:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:11:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:11:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:11:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:11:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:11:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:11:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:11:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:11:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:11:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:11:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:11:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:11:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:11:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:11:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:11:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:11:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:11:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:12:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:12:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:12:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:12:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:12:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:12:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:12:09 | INFO | valid | epoch 184 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.231 | nll_loss 3.718 | ppl 13.16 | bleu 23.08 | wps 4496 | wpb 7508.5 | bsz 272.7 | num_updates 30294 | best_bleu 23.39
2021-01-28 17:12:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 17:12:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:12:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:12:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:12:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:12:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 184 @ 30294 updates, score 23.08) (writing took 3.0712682479643263 seconds)
2021-01-28 17:12:12 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2021-01-28 17:12:12 | INFO | train | epoch 184 | symm_kl 0.299 | self_kl 0 | self_cv 0 | loss 3.102 | nll_loss 0.888 | ppl 1.85 | wps 16240.2 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 30294 | lr 1.25876e-05 | gnorm 0.83 | train_wall 333 | wall 0
2021-01-28 17:12:12 | INFO | fairseq.trainer | begin training epoch 185
2021-01-28 17:12:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:12:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:12:19 | INFO | train_inner | epoch 185:      6 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.101, nll_loss=0.888, ppl=1.85, wps=11942.6, ups=1.14, wpb=10481.4, bsz=347.3, num_updates=30300, lr=1.25863e-05, gnorm=0.833, train_wall=60, wall=0
2021-01-28 17:13:17 | INFO | train_inner | epoch 185:    106 / 561 symm_kl=0.294, self_kl=0, self_cv=0, loss=3.085, nll_loss=0.878, ppl=1.84, wps=17721, ups=1.7, wpb=10413.6, bsz=390.1, num_updates=30400, lr=1.25656e-05, gnorm=0.818, train_wall=59, wall=0
2021-01-28 17:14:17 | INFO | train_inner | epoch 185:    206 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.104, nll_loss=0.889, ppl=1.85, wps=17471.8, ups=1.67, wpb=10451.6, bsz=364.2, num_updates=30500, lr=1.2545e-05, gnorm=0.823, train_wall=60, wall=0
2021-01-28 17:15:17 | INFO | train_inner | epoch 185:    306 / 561 symm_kl=0.296, self_kl=0, self_cv=0, loss=3.092, nll_loss=0.883, ppl=1.84, wps=17732.3, ups=1.68, wpb=10553.8, bsz=378.3, num_updates=30600, lr=1.25245e-05, gnorm=0.822, train_wall=59, wall=0
2021-01-28 17:16:17 | INFO | train_inner | epoch 185:    406 / 561 symm_kl=0.304, self_kl=0, self_cv=0, loss=3.117, nll_loss=0.898, ppl=1.86, wps=17504.1, ups=1.67, wpb=10503.2, bsz=350.6, num_updates=30700, lr=1.25041e-05, gnorm=0.838, train_wall=60, wall=0
2021-01-28 17:17:16 | INFO | train_inner | epoch 185:    506 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.104, nll_loss=0.891, ppl=1.85, wps=17685, ups=1.67, wpb=10561.4, bsz=372, num_updates=30800, lr=1.24838e-05, gnorm=0.826, train_wall=60, wall=0
2021-01-28 17:17:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 17:17:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:17:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:17:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:17:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:17:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:17:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:17:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:17:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:17:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:17:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:17:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:17:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:17:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:17:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:17:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:17:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:17:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:17:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:17:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:17:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:17:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:17:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:17:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:17:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:17:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:17:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:17:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:17:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:17:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:17:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:17:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:17:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:17:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:17:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:17:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:17:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:17:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:17:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:17:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:18:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:18:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:18:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:18:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:18:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:18:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:18:10 | INFO | valid | epoch 185 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.225 | nll_loss 3.713 | ppl 13.11 | bleu 23.05 | wps 4600.2 | wpb 7508.5 | bsz 272.7 | num_updates 30855 | best_bleu 23.39
2021-01-28 17:18:10 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 17:18:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:18:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:18:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:18:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:18:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 185 @ 30855 updates, score 23.05) (writing took 3.0429854299873114 seconds)
2021-01-28 17:18:13 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2021-01-28 17:18:13 | INFO | train | epoch 185 | symm_kl 0.299 | self_kl 0 | self_cv 0 | loss 3.102 | nll_loss 0.889 | ppl 1.85 | wps 16261.1 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 30855 | lr 1.24726e-05 | gnorm 0.827 | train_wall 333 | wall 0
2021-01-28 17:18:13 | INFO | fairseq.trainer | begin training epoch 186
2021-01-28 17:18:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:18:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:18:43 | INFO | train_inner | epoch 186:     45 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.107, nll_loss=0.891, ppl=1.85, wps=12091.4, ups=1.16, wpb=10418, bsz=353.8, num_updates=30900, lr=1.24635e-05, gnorm=0.833, train_wall=58, wall=0
2021-01-28 17:19:42 | INFO | train_inner | epoch 186:    145 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.096, nll_loss=0.882, ppl=1.84, wps=17845.5, ups=1.69, wpb=10588.5, bsz=360.5, num_updates=31000, lr=1.24434e-05, gnorm=0.819, train_wall=59, wall=0
2021-01-28 17:20:42 | INFO | train_inner | epoch 186:    245 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.106, nll_loss=0.894, ppl=1.86, wps=17411.2, ups=1.67, wpb=10421.8, bsz=369.2, num_updates=31100, lr=1.24234e-05, gnorm=0.83, train_wall=60, wall=0
2021-01-28 17:21:41 | INFO | train_inner | epoch 186:    345 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.117, nll_loss=0.898, ppl=1.86, wps=17519.7, ups=1.67, wpb=10461.7, bsz=364.6, num_updates=31200, lr=1.24035e-05, gnorm=0.833, train_wall=60, wall=0
2021-01-28 17:22:41 | INFO | train_inner | epoch 186:    445 / 561 symm_kl=0.291, self_kl=0, self_cv=0, loss=3.074, nll_loss=0.87, ppl=1.83, wps=17630, ups=1.67, wpb=10549.4, bsz=395.9, num_updates=31300, lr=1.23836e-05, gnorm=0.807, train_wall=60, wall=0
2021-01-28 17:23:41 | INFO | train_inner | epoch 186:    545 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.109, nll_loss=0.896, ppl=1.86, wps=17499.3, ups=1.67, wpb=10502.9, bsz=371, num_updates=31400, lr=1.23639e-05, gnorm=0.826, train_wall=60, wall=0
2021-01-28 17:23:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 17:23:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:23:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:23:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:23:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:23:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:23:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:23:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:23:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:23:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:23:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:23:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:23:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:23:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:23:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:23:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:23:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:23:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:23:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:23:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:23:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:23:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:23:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:23:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:23:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:23:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:23:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:23:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:23:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:23:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:23:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:23:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:23:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:23:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:23:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:23:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:23:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:24:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:24:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:24:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:24:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:24:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:24:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:24:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:24:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:24:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:24:12 | INFO | valid | epoch 186 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.227 | nll_loss 3.712 | ppl 13.11 | bleu 23.19 | wps 4636.5 | wpb 7508.5 | bsz 272.7 | num_updates 31416 | best_bleu 23.39
2021-01-28 17:24:12 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 17:24:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:24:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:24:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:24:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:24:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 186 @ 31416 updates, score 23.19) (writing took 3.0290930519695394 seconds)
2021-01-28 17:24:15 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2021-01-28 17:24:15 | INFO | train | epoch 186 | symm_kl 0.299 | self_kl 0 | self_cv 0 | loss 3.1 | nll_loss 0.888 | ppl 1.85 | wps 16260.3 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 31416 | lr 1.23608e-05 | gnorm 0.826 | train_wall 333 | wall 0
2021-01-28 17:24:15 | INFO | fairseq.trainer | begin training epoch 187
2021-01-28 17:24:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:24:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:25:08 | INFO | train_inner | epoch 187:     84 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.098, nll_loss=0.882, ppl=1.84, wps=12076.1, ups=1.16, wpb=10434.9, bsz=351.8, num_updates=31500, lr=1.23443e-05, gnorm=0.849, train_wall=59, wall=0
2021-01-28 17:26:08 | INFO | train_inner | epoch 187:    184 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.091, nll_loss=0.877, ppl=1.84, wps=17535.8, ups=1.66, wpb=10545.8, bsz=376.2, num_updates=31600, lr=1.23247e-05, gnorm=0.818, train_wall=60, wall=0
2021-01-28 17:27:08 | INFO | train_inner | epoch 187:    284 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.11, nll_loss=0.896, ppl=1.86, wps=17329.3, ups=1.67, wpb=10385.3, bsz=376.6, num_updates=31700, lr=1.23053e-05, gnorm=0.833, train_wall=60, wall=0
2021-01-28 17:28:08 | INFO | train_inner | epoch 187:    384 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.1, nll_loss=0.888, ppl=1.85, wps=17516.2, ups=1.67, wpb=10476.3, bsz=367.7, num_updates=31800, lr=1.22859e-05, gnorm=0.836, train_wall=60, wall=0
2021-01-28 17:29:07 | INFO | train_inner | epoch 187:    484 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.102, nll_loss=0.888, ppl=1.85, wps=17542.9, ups=1.67, wpb=10491.7, bsz=375.8, num_updates=31900, lr=1.22666e-05, gnorm=0.829, train_wall=60, wall=0
2021-01-28 17:29:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 17:29:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:29:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:29:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:29:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:29:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:29:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:29:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:29:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:29:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:29:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:29:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:29:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:29:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:29:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:29:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:29:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:29:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:29:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:29:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:29:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:29:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:29:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:29:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:29:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:30:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:30:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:30:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:30:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:30:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:30:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:30:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:30:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:30:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:30:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:30:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:30:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:30:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:30:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:30:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:30:14 | INFO | valid | epoch 187 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.227 | nll_loss 3.714 | ppl 13.12 | bleu 23.1 | wps 4613.2 | wpb 7508.5 | bsz 272.7 | num_updates 31977 | best_bleu 23.39
2021-01-28 17:30:14 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 17:30:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:30:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:30:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:30:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:30:17 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 187 @ 31977 updates, score 23.1) (writing took 3.0932319560088217 seconds)
2021-01-28 17:30:17 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2021-01-28 17:30:17 | INFO | train | epoch 187 | symm_kl 0.299 | self_kl 0 | self_cv 0 | loss 3.1 | nll_loss 0.887 | ppl 1.85 | wps 16235.9 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 31977 | lr 1.22519e-05 | gnorm 0.832 | train_wall 334 | wall 0
2021-01-28 17:30:17 | INFO | fairseq.trainer | begin training epoch 188
2021-01-28 17:30:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:30:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:30:34 | INFO | train_inner | epoch 188:     23 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.109, nll_loss=0.897, ppl=1.86, wps=12086.3, ups=1.16, wpb=10428.1, bsz=366.4, num_updates=32000, lr=1.22474e-05, gnorm=0.846, train_wall=59, wall=0
2021-01-28 17:31:32 | INFO | train_inner | epoch 188:    123 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.09, nll_loss=0.875, ppl=1.83, wps=17836.3, ups=1.71, wpb=10456.2, bsz=358.3, num_updates=32100, lr=1.22284e-05, gnorm=0.825, train_wall=58, wall=0
2021-01-28 17:32:32 | INFO | train_inner | epoch 188:    223 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.115, nll_loss=0.898, ppl=1.86, wps=17598.1, ups=1.67, wpb=10524.8, bsz=367.2, num_updates=32200, lr=1.22094e-05, gnorm=0.831, train_wall=60, wall=0
2021-01-28 17:33:31 | INFO | train_inner | epoch 188:    323 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.11, nll_loss=0.897, ppl=1.86, wps=17547.1, ups=1.69, wpb=10398.2, bsz=368.6, num_updates=32300, lr=1.21904e-05, gnorm=0.827, train_wall=59, wall=0
2021-01-28 17:34:32 | INFO | train_inner | epoch 188:    423 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.097, nll_loss=0.886, ppl=1.85, wps=17210.1, ups=1.64, wpb=10465.1, bsz=379.4, num_updates=32400, lr=1.21716e-05, gnorm=0.814, train_wall=61, wall=0
2021-01-28 17:35:39 | INFO | train_inner | epoch 188:    523 / 561 symm_kl=0.293, self_kl=0, self_cv=0, loss=3.08, nll_loss=0.875, ppl=1.83, wps=16064.8, ups=1.5, wpb=10687.8, bsz=372, num_updates=32500, lr=1.21529e-05, gnorm=0.81, train_wall=66, wall=0
2021-01-28 17:36:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 17:36:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:36:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:36:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:36:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:36:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:36:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:36:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:36:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:36:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:36:35 | INFO | valid | epoch 188 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.227 | nll_loss 3.715 | ppl 13.13 | bleu 23.17 | wps 3140.8 | wpb 7508.5 | bsz 272.7 | num_updates 32538 | best_bleu 23.39
2021-01-28 17:36:35 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 17:36:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:36:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:36:38 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 188 @ 32538 updates, score 23.17) (writing took 3.3315552219864912 seconds)
2021-01-28 17:36:38 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2021-01-28 17:36:38 | INFO | train | epoch 188 | symm_kl 0.298 | self_kl 0 | self_cv 0 | loss 3.1 | nll_loss 0.888 | ppl 1.85 | wps 15444.4 | ups 1.47 | wpb 10483.4 | bsz 369.6 | num_updates 32538 | lr 1.21458e-05 | gnorm 0.823 | train_wall 341 | wall 0
2021-01-28 17:36:38 | INFO | fairseq.trainer | begin training epoch 189
2021-01-28 17:36:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:36:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:36:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:36:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:37:26 | INFO | train_inner | epoch 189:     62 / 561 symm_kl=0.296, self_kl=0, self_cv=0, loss=3.099, nll_loss=0.89, ppl=1.85, wps=9733.8, ups=0.94, wpb=10400.3, bsz=384.5, num_updates=32600, lr=1.21342e-05, gnorm=0.825, train_wall=65, wall=0
2021-01-28 17:38:32 | INFO | train_inner | epoch 189:    162 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.117, nll_loss=0.901, ppl=1.87, wps=15864, ups=1.51, wpb=10479.4, bsz=371.4, num_updates=32700, lr=1.21157e-05, gnorm=0.827, train_wall=66, wall=0
2021-01-28 17:39:36 | INFO | train_inner | epoch 189:    262 / 561 symm_kl=0.296, self_kl=0, self_cv=0, loss=3.088, nll_loss=0.877, ppl=1.84, wps=16089.5, ups=1.55, wpb=10410.7, bsz=363, num_updates=32800, lr=1.20972e-05, gnorm=0.827, train_wall=64, wall=0
2021-01-28 17:40:43 | INFO | train_inner | epoch 189:    362 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.097, nll_loss=0.885, ppl=1.85, wps=15895.5, ups=1.5, wpb=10605.8, bsz=363.4, num_updates=32900, lr=1.20788e-05, gnorm=0.825, train_wall=66, wall=0
2021-01-28 17:41:51 | INFO | train_inner | epoch 189:    462 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.095, nll_loss=0.884, ppl=1.85, wps=15627.8, ups=1.48, wpb=10566.6, bsz=381.2, num_updates=33000, lr=1.20605e-05, gnorm=0.821, train_wall=67, wall=0
2021-01-28 17:42:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 17:42:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:42:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:42:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:43:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:43:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:43:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:43:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:43:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:43:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:43:28 | INFO | valid | epoch 189 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.228 | nll_loss 3.715 | ppl 13.13 | bleu 23.16 | wps 3098.4 | wpb 7508.5 | bsz 272.7 | num_updates 33099 | best_bleu 23.39
2021-01-28 17:43:28 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 17:43:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:43:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:43:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 189 @ 33099 updates, score 23.16) (writing took 3.568955812952481 seconds)
2021-01-28 17:43:31 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2021-01-28 17:43:31 | INFO | train | epoch 189 | symm_kl 0.298 | self_kl 0 | self_cv 0 | loss 3.099 | nll_loss 0.887 | ppl 1.85 | wps 14233.4 | ups 1.36 | wpb 10483.4 | bsz 369.6 | num_updates 33099 | lr 1.20424e-05 | gnorm 0.826 | train_wall 368 | wall 0
2021-01-28 17:43:31 | INFO | fairseq.trainer | begin training epoch 190
2021-01-28 17:43:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:43:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:43:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:43:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:43:39 | INFO | train_inner | epoch 190:      1 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.102, nll_loss=0.887, ppl=1.85, wps=9548.2, ups=0.92, wpb=10355, bsz=356.9, num_updates=33100, lr=1.20422e-05, gnorm=0.838, train_wall=65, wall=0
2021-01-28 17:44:45 | INFO | train_inner | epoch 190:    101 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.092, nll_loss=0.879, ppl=1.84, wps=16102.1, ups=1.52, wpb=10572.9, bsz=360.8, num_updates=33200, lr=1.20241e-05, gnorm=0.823, train_wall=65, wall=0
2021-01-28 17:45:50 | INFO | train_inner | epoch 190:    201 / 561 symm_kl=0.29, self_kl=0, self_cv=0, loss=3.069, nll_loss=0.866, ppl=1.82, wps=16036, ups=1.53, wpb=10481.1, bsz=400.1, num_updates=33300, lr=1.2006e-05, gnorm=0.809, train_wall=65, wall=0
2021-01-28 17:46:56 | INFO | train_inner | epoch 190:    301 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.098, nll_loss=0.886, ppl=1.85, wps=15868.2, ups=1.51, wpb=10484.7, bsz=378.3, num_updates=33400, lr=1.1988e-05, gnorm=0.828, train_wall=65, wall=0
2021-01-28 17:48:02 | INFO | train_inner | epoch 190:    401 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.116, nll_loss=0.903, ppl=1.87, wps=16071.8, ups=1.52, wpb=10567.2, bsz=368, num_updates=33500, lr=1.19701e-05, gnorm=0.825, train_wall=65, wall=0
2021-01-28 17:49:08 | INFO | train_inner | epoch 190:    501 / 561 symm_kl=0.303, self_kl=0, self_cv=0, loss=3.114, nll_loss=0.896, ppl=1.86, wps=15815.6, ups=1.52, wpb=10433.5, bsz=350.8, num_updates=33600, lr=1.19523e-05, gnorm=0.834, train_wall=66, wall=0
2021-01-28 17:49:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 17:49:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:49:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:49:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:49:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:49:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:49:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:49:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:49:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:49:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:49:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:49:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:49:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:49:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:49:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:49:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:49:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:49:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:49:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:49:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:49:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:49:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:49:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:49:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:49:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:49:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:49:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:49:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:49:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:49:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:49:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:49:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:49:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:49:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:49:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:49:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:49:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:50:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:50:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:50:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:50:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:50:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:50:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:50:21 | INFO | valid | epoch 190 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.231 | nll_loss 3.717 | ppl 13.15 | bleu 23.2 | wps 2671.1 | wpb 7508.5 | bsz 272.7 | num_updates 33660 | best_bleu 23.39
2021-01-28 17:50:21 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 17:50:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:50:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:50:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:50:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:50:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 190 @ 33660 updates, score 23.2) (writing took 4.271264742012136 seconds)
2021-01-28 17:50:26 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2021-01-28 17:50:26 | INFO | train | epoch 190 | symm_kl 0.298 | self_kl 0 | self_cv 0 | loss 3.098 | nll_loss 0.886 | ppl 1.85 | wps 14197.4 | ups 1.35 | wpb 10483.4 | bsz 369.6 | num_updates 33660 | lr 1.19416e-05 | gnorm 0.827 | train_wall 366 | wall 0
2021-01-28 17:50:26 | INFO | fairseq.trainer | begin training epoch 191
2021-01-28 17:50:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:50:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:50:57 | INFO | train_inner | epoch 191:     40 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.1, nll_loss=0.889, ppl=1.85, wps=9539.1, ups=0.92, wpb=10376.9, bsz=374.2, num_updates=33700, lr=1.19345e-05, gnorm=0.838, train_wall=65, wall=0
2021-01-28 17:52:01 | INFO | train_inner | epoch 191:    140 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.107, nll_loss=0.894, ppl=1.86, wps=16218.2, ups=1.55, wpb=10497.2, bsz=380.1, num_updates=33800, lr=1.19169e-05, gnorm=0.83, train_wall=64, wall=0
2021-01-28 17:53:08 | INFO | train_inner | epoch 191:    240 / 561 symm_kl=0.295, self_kl=0, self_cv=0, loss=3.083, nll_loss=0.872, ppl=1.83, wps=15655.1, ups=1.5, wpb=10442.8, bsz=359.7, num_updates=33900, lr=1.18993e-05, gnorm=0.823, train_wall=66, wall=0
2021-01-28 17:54:15 | INFO | train_inner | epoch 191:    340 / 561 symm_kl=0.295, self_kl=0, self_cv=0, loss=3.089, nll_loss=0.88, ppl=1.84, wps=15949.5, ups=1.5, wpb=10636.8, bsz=366.9, num_updates=34000, lr=1.18818e-05, gnorm=0.809, train_wall=66, wall=0
2021-01-28 17:55:17 | INFO | train_inner | epoch 191:    440 / 561 symm_kl=0.295, self_kl=0, self_cv=0, loss=3.088, nll_loss=0.881, ppl=1.84, wps=16851.5, ups=1.61, wpb=10489.2, bsz=375.3, num_updates=34100, lr=1.18643e-05, gnorm=0.818, train_wall=62, wall=0
2021-01-28 17:56:17 | INFO | train_inner | epoch 191:    540 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.112, nll_loss=0.896, ppl=1.86, wps=17477, ups=1.68, wpb=10433.1, bsz=361.3, num_updates=34200, lr=1.1847e-05, gnorm=0.83, train_wall=59, wall=0
2021-01-28 17:56:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 17:56:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:56:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:56:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:56:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:56:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:56:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:56:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 17:56:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 17:56:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 17:56:50 | INFO | valid | epoch 191 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.226 | nll_loss 3.715 | ppl 13.13 | bleu 22.98 | wps 4646.2 | wpb 7508.5 | bsz 272.7 | num_updates 34221 | best_bleu 23.39
2021-01-28 17:56:50 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 17:56:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:56:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:56:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:56:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:56:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 191 @ 34221 updates, score 22.98) (writing took 3.1036046069930308 seconds)
2021-01-28 17:56:53 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2021-01-28 17:56:53 | INFO | train | epoch 191 | symm_kl 0.298 | self_kl 0 | self_cv 0 | loss 3.098 | nll_loss 0.886 | ppl 1.85 | wps 15164.9 | ups 1.45 | wpb 10483.4 | bsz 369.6 | num_updates 34221 | lr 1.18433e-05 | gnorm 0.824 | train_wall 357 | wall 0
2021-01-28 17:56:53 | INFO | fairseq.trainer | begin training epoch 192
2021-01-28 17:56:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 17:56:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 17:57:43 | INFO | train_inner | epoch 192:     79 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.098, nll_loss=0.886, ppl=1.85, wps=12146.8, ups=1.16, wpb=10476.9, bsz=365.5, num_updates=34300, lr=1.18297e-05, gnorm=0.827, train_wall=59, wall=0
2021-01-28 17:58:42 | INFO | train_inner | epoch 192:    179 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.098, nll_loss=0.884, ppl=1.85, wps=17733, ups=1.69, wpb=10471.5, bsz=363.4, num_updates=34400, lr=1.18125e-05, gnorm=0.836, train_wall=59, wall=0
2021-01-28 17:59:42 | INFO | train_inner | epoch 192:    279 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.112, nll_loss=0.895, ppl=1.86, wps=17529.3, ups=1.67, wpb=10476.2, bsz=361.8, num_updates=34500, lr=1.17954e-05, gnorm=0.826, train_wall=60, wall=0
2021-01-28 18:00:41 | INFO | train_inner | epoch 192:    379 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.099, nll_loss=0.889, ppl=1.85, wps=17698.5, ups=1.68, wpb=10507.7, bsz=370.6, num_updates=34600, lr=1.17783e-05, gnorm=0.82, train_wall=59, wall=0
2021-01-28 18:01:41 | INFO | train_inner | epoch 192:    479 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.108, nll_loss=0.891, ppl=1.85, wps=17362.5, ups=1.67, wpb=10371.7, bsz=373.9, num_updates=34700, lr=1.17613e-05, gnorm=0.846, train_wall=60, wall=0
2021-01-28 18:02:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 18:02:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:02:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:02:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:02:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:02:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:02:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:02:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:02:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:02:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:02:51 | INFO | valid | epoch 192 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.227 | nll_loss 3.715 | ppl 13.13 | bleu 23.2 | wps 4494.2 | wpb 7508.5 | bsz 272.7 | num_updates 34782 | best_bleu 23.39
2021-01-28 18:02:51 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 18:02:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:02:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:02:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:02:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:02:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 192 @ 34782 updates, score 23.2) (writing took 3.0896684050094336 seconds)
2021-01-28 18:02:55 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2021-01-28 18:02:55 | INFO | train | epoch 192 | symm_kl 0.298 | self_kl 0 | self_cv 0 | loss 3.097 | nll_loss 0.885 | ppl 1.85 | wps 16281.3 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 34782 | lr 1.17474e-05 | gnorm 0.827 | train_wall 332 | wall 0
2021-01-28 18:02:55 | INFO | fairseq.trainer | begin training epoch 193
2021-01-28 18:02:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:02:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:03:08 | INFO | train_inner | epoch 193:     18 / 561 symm_kl=0.29, self_kl=0, self_cv=0, loss=3.069, nll_loss=0.867, ppl=1.82, wps=12051.8, ups=1.14, wpb=10542.9, bsz=382.6, num_updates=34800, lr=1.17444e-05, gnorm=0.814, train_wall=59, wall=0
2021-01-28 18:04:07 | INFO | train_inner | epoch 193:    118 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.104, nll_loss=0.89, ppl=1.85, wps=17726.4, ups=1.7, wpb=10409.3, bsz=373, num_updates=34900, lr=1.17276e-05, gnorm=0.828, train_wall=59, wall=0
2021-01-28 18:05:07 | INFO | train_inner | epoch 193:    218 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.105, nll_loss=0.89, ppl=1.85, wps=17418.4, ups=1.66, wpb=10462.2, bsz=364.6, num_updates=35000, lr=1.17108e-05, gnorm=0.826, train_wall=60, wall=0
2021-01-28 18:06:07 | INFO | train_inner | epoch 193:    318 / 561 symm_kl=0.294, self_kl=0, self_cv=0, loss=3.08, nll_loss=0.872, ppl=1.83, wps=17295.5, ups=1.67, wpb=10386.5, bsz=372.9, num_updates=35100, lr=1.16941e-05, gnorm=0.83, train_wall=60, wall=0
2021-01-28 18:07:08 | INFO | train_inner | epoch 193:    418 / 561 symm_kl=0.292, self_kl=0, self_cv=0, loss=3.081, nll_loss=0.877, ppl=1.84, wps=17698.3, ups=1.66, wpb=10682.2, bsz=368.2, num_updates=35200, lr=1.16775e-05, gnorm=0.803, train_wall=60, wall=0
2021-01-28 18:08:07 | INFO | train_inner | epoch 193:    518 / 561 symm_kl=0.302, self_kl=0, self_cv=0, loss=3.113, nll_loss=0.897, ppl=1.86, wps=17524.6, ups=1.68, wpb=10441.8, bsz=370.4, num_updates=35300, lr=1.16609e-05, gnorm=0.839, train_wall=59, wall=0
2021-01-28 18:08:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 18:08:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:08:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:08:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:08:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:08:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:08:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:08:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:08:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:08:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:08:54 | INFO | valid | epoch 193 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.229 | nll_loss 3.715 | ppl 13.13 | bleu 23.11 | wps 4722.5 | wpb 7508.5 | bsz 272.7 | num_updates 35343 | best_bleu 23.39
2021-01-28 18:08:54 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 18:08:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:08:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:08:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:08:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:08:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 193 @ 35343 updates, score 23.11) (writing took 3.0520586899947375 seconds)
2021-01-28 18:08:57 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2021-01-28 18:08:57 | INFO | train | epoch 193 | symm_kl 0.297 | self_kl 0 | self_cv 0 | loss 3.096 | nll_loss 0.884 | ppl 1.85 | wps 16232.9 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 35343 | lr 1.16538e-05 | gnorm 0.825 | train_wall 334 | wall 0
2021-01-28 18:08:57 | INFO | fairseq.trainer | begin training epoch 194
2021-01-28 18:08:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:09:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:09:34 | INFO | train_inner | epoch 194:     57 / 561 symm_kl=0.295, self_kl=0, self_cv=0, loss=3.086, nll_loss=0.877, ppl=1.84, wps=12239.2, ups=1.16, wpb=10578.5, bsz=371.6, num_updates=35400, lr=1.16445e-05, gnorm=0.819, train_wall=59, wall=0
2021-01-28 18:10:33 | INFO | train_inner | epoch 194:    157 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.1, nll_loss=0.887, ppl=1.85, wps=17502.9, ups=1.68, wpb=10435.4, bsz=373.9, num_updates=35500, lr=1.1628e-05, gnorm=0.826, train_wall=59, wall=0
2021-01-28 18:11:33 | INFO | train_inner | epoch 194:    257 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.104, nll_loss=0.89, ppl=1.85, wps=17598.5, ups=1.68, wpb=10503.1, bsz=358.9, num_updates=35600, lr=1.16117e-05, gnorm=0.825, train_wall=59, wall=0
2021-01-28 18:12:33 | INFO | train_inner | epoch 194:    357 / 561 symm_kl=0.296, self_kl=0, self_cv=0, loss=3.093, nll_loss=0.883, ppl=1.84, wps=17460.1, ups=1.67, wpb=10443.5, bsz=362.2, num_updates=35700, lr=1.15954e-05, gnorm=0.848, train_wall=60, wall=0
2021-01-28 18:13:33 | INFO | train_inner | epoch 194:    457 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.113, nll_loss=0.898, ppl=1.86, wps=17587.8, ups=1.67, wpb=10500.5, bsz=368.1, num_updates=35800, lr=1.15792e-05, gnorm=0.833, train_wall=60, wall=0
2021-01-28 18:14:33 | INFO | train_inner | epoch 194:    557 / 561 symm_kl=0.295, self_kl=0, self_cv=0, loss=3.081, nll_loss=0.872, ppl=1.83, wps=17509.5, ups=1.67, wpb=10506.1, bsz=372.4, num_updates=35900, lr=1.15631e-05, gnorm=0.827, train_wall=60, wall=0
2021-01-28 18:14:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 18:14:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:14:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:14:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:14:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:14:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:14:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:14:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:14:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:14:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:14:56 | INFO | valid | epoch 194 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.23 | nll_loss 3.715 | ppl 13.13 | bleu 23.11 | wps 4545.1 | wpb 7508.5 | bsz 272.7 | num_updates 35904 | best_bleu 23.39
2021-01-28 18:14:56 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 18:14:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:14:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:14:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:14:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:14:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 194 @ 35904 updates, score 23.11) (writing took 3.1040045440313406 seconds)
2021-01-28 18:14:59 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2021-01-28 18:14:59 | INFO | train | epoch 194 | symm_kl 0.297 | self_kl 0 | self_cv 0 | loss 3.095 | nll_loss 0.884 | ppl 1.85 | wps 16233.6 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 35904 | lr 1.15624e-05 | gnorm 0.83 | train_wall 334 | wall 0
2021-01-28 18:14:59 | INFO | fairseq.trainer | begin training epoch 195
2021-01-28 18:15:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:15:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:16:00 | INFO | train_inner | epoch 195:     96 / 561 symm_kl=0.294, self_kl=0, self_cv=0, loss=3.087, nll_loss=0.88, ppl=1.84, wps=11908.2, ups=1.14, wpb=10454.2, bsz=369.5, num_updates=36000, lr=1.1547e-05, gnorm=0.826, train_wall=59, wall=0
2021-01-28 18:17:01 | INFO | train_inner | epoch 195:    196 / 561 symm_kl=0.294, self_kl=0, self_cv=0, loss=3.084, nll_loss=0.877, ppl=1.84, wps=17370.3, ups=1.65, wpb=10506, bsz=366.5, num_updates=36100, lr=1.1531e-05, gnorm=0.817, train_wall=60, wall=0
2021-01-28 18:18:00 | INFO | train_inner | epoch 195:    296 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.097, nll_loss=0.885, ppl=1.85, wps=17591.6, ups=1.68, wpb=10490.3, bsz=360.6, num_updates=36200, lr=1.15151e-05, gnorm=0.826, train_wall=59, wall=0
2021-01-28 18:19:00 | INFO | train_inner | epoch 195:    396 / 561 symm_kl=0.296, self_kl=0, self_cv=0, loss=3.091, nll_loss=0.881, ppl=1.84, wps=17548.4, ups=1.67, wpb=10503.9, bsz=392.2, num_updates=36300, lr=1.14992e-05, gnorm=0.818, train_wall=60, wall=0
2021-01-28 18:20:00 | INFO | train_inner | epoch 195:    496 / 561 symm_kl=0.306, self_kl=0, self_cv=0, loss=3.123, nll_loss=0.902, ppl=1.87, wps=17458, ups=1.67, wpb=10426.6, bsz=354.3, num_updates=36400, lr=1.14834e-05, gnorm=0.858, train_wall=60, wall=0
2021-01-28 18:20:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 18:20:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:20:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:20:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:20:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:20:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:20:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:20:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:20:59 | INFO | valid | epoch 195 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.226 | nll_loss 3.714 | ppl 13.12 | bleu 23.13 | wps 4698.9 | wpb 7508.5 | bsz 272.7 | num_updates 36465 | best_bleu 23.39
2021-01-28 18:20:59 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 18:21:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:21:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:21:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:21:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:21:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 195 @ 36465 updates, score 23.13) (writing took 3.059251511003822 seconds)
2021-01-28 18:21:03 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2021-01-28 18:21:03 | INFO | train | epoch 195 | symm_kl 0.297 | self_kl 0 | self_cv 0 | loss 3.096 | nll_loss 0.884 | ppl 1.85 | wps 16183.9 | ups 1.54 | wpb 10483.4 | bsz 369.6 | num_updates 36465 | lr 1.14731e-05 | gnorm 0.829 | train_wall 334 | wall 0
2021-01-28 18:21:03 | INFO | fairseq.trainer | begin training epoch 196
2021-01-28 18:21:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:21:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:21:26 | INFO | train_inner | epoch 196:     35 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.098, nll_loss=0.888, ppl=1.85, wps=12163.2, ups=1.16, wpb=10513.1, bsz=377, num_updates=36500, lr=1.14676e-05, gnorm=0.829, train_wall=59, wall=0
2021-01-28 18:22:26 | INFO | train_inner | epoch 196:    135 / 561 symm_kl=0.296, self_kl=0, self_cv=0, loss=3.098, nll_loss=0.888, ppl=1.85, wps=17865.5, ups=1.68, wpb=10610.2, bsz=379.6, num_updates=36600, lr=1.1452e-05, gnorm=0.822, train_wall=59, wall=0
2021-01-28 18:23:26 | INFO | train_inner | epoch 196:    235 / 561 symm_kl=0.3, self_kl=0, self_cv=0, loss=3.103, nll_loss=0.888, ppl=1.85, wps=17185.1, ups=1.67, wpb=10276, bsz=369, num_updates=36700, lr=1.14364e-05, gnorm=0.84, train_wall=60, wall=0
2021-01-28 18:24:26 | INFO | train_inner | epoch 196:    335 / 561 symm_kl=0.293, self_kl=0, self_cv=0, loss=3.076, nll_loss=0.869, ppl=1.83, wps=17548.4, ups=1.67, wpb=10537.8, bsz=378.7, num_updates=36800, lr=1.14208e-05, gnorm=0.827, train_wall=60, wall=0
2021-01-28 18:25:25 | INFO | train_inner | epoch 196:    435 / 561 symm_kl=0.301, self_kl=0, self_cv=0, loss=3.111, nll_loss=0.896, ppl=1.86, wps=17460.8, ups=1.68, wpb=10374.2, bsz=357.6, num_updates=36900, lr=1.14053e-05, gnorm=0.838, train_wall=59, wall=0
2021-01-28 18:26:25 | INFO | train_inner | epoch 196:    535 / 561 symm_kl=0.295, self_kl=0, self_cv=0, loss=3.085, nll_loss=0.876, ppl=1.84, wps=17709, ups=1.67, wpb=10626, bsz=365.8, num_updates=37000, lr=1.13899e-05, gnorm=0.817, train_wall=60, wall=0
2021-01-28 18:26:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 18:26:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:26:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:26:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:26:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:26:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:26:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:26:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:26:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:26:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:26:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:26:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:26:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:26:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:26:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:26:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:26:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:26:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:26:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:26:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:26:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:26:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:27:02 | INFO | valid | epoch 196 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.228 | nll_loss 3.714 | ppl 13.13 | bleu 23.31 | wps 4690.9 | wpb 7508.5 | bsz 272.7 | num_updates 37026 | best_bleu 23.39
2021-01-28 18:27:02 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 18:27:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:27:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:27:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:27:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:27:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 196 @ 37026 updates, score 23.31) (writing took 3.082061768975109 seconds)
2021-01-28 18:27:05 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2021-01-28 18:27:05 | INFO | train | epoch 196 | symm_kl 0.297 | self_kl 0 | self_cv 0 | loss 3.095 | nll_loss 0.883 | ppl 1.84 | wps 16239.6 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 37026 | lr 1.13859e-05 | gnorm 0.829 | train_wall 334 | wall 0
2021-01-28 18:27:05 | INFO | fairseq.trainer | begin training epoch 197
2021-01-28 18:27:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:27:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:27:51 | INFO | train_inner | epoch 197:     74 / 561 symm_kl=0.291, self_kl=0, self_cv=0, loss=3.076, nll_loss=0.873, ppl=1.83, wps=12099.3, ups=1.16, wpb=10449.7, bsz=385.7, num_updates=37100, lr=1.13745e-05, gnorm=0.814, train_wall=59, wall=0
2021-01-28 18:28:52 | INFO | train_inner | epoch 197:    174 / 561 symm_kl=0.295, self_kl=0, self_cv=0, loss=3.076, nll_loss=0.865, ppl=1.82, wps=17432.3, ups=1.67, wpb=10467.9, bsz=372.2, num_updates=37200, lr=1.13592e-05, gnorm=0.829, train_wall=60, wall=0
2021-01-28 18:29:51 | INFO | train_inner | epoch 197:    274 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.107, nll_loss=0.894, ppl=1.86, wps=17506.7, ups=1.68, wpb=10436.1, bsz=365, num_updates=37300, lr=1.1344e-05, gnorm=0.827, train_wall=59, wall=0
2021-01-28 18:30:51 | INFO | train_inner | epoch 197:    374 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.1, nll_loss=0.886, ppl=1.85, wps=17609.7, ups=1.68, wpb=10459.1, bsz=358.2, num_updates=37400, lr=1.13288e-05, gnorm=0.83, train_wall=59, wall=0
2021-01-28 18:31:51 | INFO | train_inner | epoch 197:    474 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.103, nll_loss=0.891, ppl=1.85, wps=17507.6, ups=1.66, wpb=10528.8, bsz=367, num_updates=37500, lr=1.13137e-05, gnorm=0.825, train_wall=60, wall=0
2021-01-28 18:32:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 18:32:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:32:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:32:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:32:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:32:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:32:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:32:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:32:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:32:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:32:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:32:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:32:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:32:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:32:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:32:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:32:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:32:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:32:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:32:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:32:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:32:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:32:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:32:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:32:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:32:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:32:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:32:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:32:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:32:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:32:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:32:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:32:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:32:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:32:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:32:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:32:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:32:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:32:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:32:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:33:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:33:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:33:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:33:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:33:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:33:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:33:04 | INFO | valid | epoch 197 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.229 | nll_loss 3.717 | ppl 13.15 | bleu 23.1 | wps 4629.6 | wpb 7508.5 | bsz 272.7 | num_updates 37587 | best_bleu 23.39
2021-01-28 18:33:04 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 18:33:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:33:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:33:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:33:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:33:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 197 @ 37587 updates, score 23.1) (writing took 3.083638369978871 seconds)
2021-01-28 18:33:07 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2021-01-28 18:33:07 | INFO | train | epoch 197 | symm_kl 0.297 | self_kl 0 | self_cv 0 | loss 3.094 | nll_loss 0.883 | ppl 1.84 | wps 16248.5 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 37587 | lr 1.13006e-05 | gnorm 0.824 | train_wall 333 | wall 0
2021-01-28 18:33:07 | INFO | fairseq.trainer | begin training epoch 198
2021-01-28 18:33:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:33:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:33:17 | INFO | train_inner | epoch 198:     13 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.099, nll_loss=0.888, ppl=1.85, wps=12089, ups=1.15, wpb=10486, bsz=361.8, num_updates=37600, lr=1.12987e-05, gnorm=0.825, train_wall=59, wall=0
2021-01-28 18:34:17 | INFO | train_inner | epoch 198:    113 / 561 symm_kl=0.296, self_kl=0, self_cv=0, loss=3.089, nll_loss=0.879, ppl=1.84, wps=17633, ups=1.69, wpb=10440.4, bsz=373, num_updates=37700, lr=1.12837e-05, gnorm=0.824, train_wall=59, wall=0
2021-01-28 18:35:17 | INFO | train_inner | epoch 198:    213 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.093, nll_loss=0.881, ppl=1.84, wps=17528.4, ups=1.66, wpb=10545, bsz=371.3, num_updates=37800, lr=1.12687e-05, gnorm=0.835, train_wall=60, wall=0
2021-01-28 18:36:17 | INFO | train_inner | epoch 198:    313 / 561 symm_kl=0.296, self_kl=0, self_cv=0, loss=3.087, nll_loss=0.877, ppl=1.84, wps=17573.6, ups=1.66, wpb=10575.2, bsz=374.2, num_updates=37900, lr=1.12538e-05, gnorm=0.819, train_wall=60, wall=0
2021-01-28 18:37:17 | INFO | train_inner | epoch 198:    413 / 561 symm_kl=0.296, self_kl=0, self_cv=0, loss=3.092, nll_loss=0.883, ppl=1.84, wps=17451.7, ups=1.67, wpb=10474.8, bsz=374.7, num_updates=38000, lr=1.1239e-05, gnorm=0.83, train_wall=60, wall=0
2021-01-28 18:38:17 | INFO | train_inner | epoch 198:    513 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.096, nll_loss=0.886, ppl=1.85, wps=17547.5, ups=1.68, wpb=10462.2, bsz=352.2, num_updates=38100, lr=1.12243e-05, gnorm=0.828, train_wall=59, wall=0
2021-01-28 18:38:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 18:38:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:38:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:38:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:38:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:38:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:38:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:38:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:38:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:38:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:38:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:38:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:38:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:38:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:38:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:38:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:38:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:38:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:38:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:38:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:38:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:38:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:38:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:38:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:38:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:38:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:38:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:38:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:38:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:38:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:38:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:38:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:38:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:38:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:38:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:38:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:38:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:38:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:38:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:38:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:39:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:39:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:39:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:39:06 | INFO | valid | epoch 198 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.232 | nll_loss 3.718 | ppl 13.16 | bleu 23.22 | wps 4591 | wpb 7508.5 | bsz 272.7 | num_updates 38148 | best_bleu 23.39
2021-01-28 18:39:06 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 18:39:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:39:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:39:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:39:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:39:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 198 @ 38148 updates, score 23.22) (writing took 3.0978321220027283 seconds)
2021-01-28 18:39:09 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2021-01-28 18:39:09 | INFO | train | epoch 198 | symm_kl 0.297 | self_kl 0 | self_cv 0 | loss 3.094 | nll_loss 0.883 | ppl 1.84 | wps 16212.6 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 38148 | lr 1.12172e-05 | gnorm 0.83 | train_wall 334 | wall 0
2021-01-28 18:39:09 | INFO | fairseq.trainer | begin training epoch 199
2021-01-28 18:39:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:39:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:39:43 | INFO | train_inner | epoch 199:     52 / 561 symm_kl=0.299, self_kl=0, self_cv=0, loss=3.101, nll_loss=0.888, ppl=1.85, wps=12035.4, ups=1.16, wpb=10392.1, bsz=366.5, num_updates=38200, lr=1.12096e-05, gnorm=0.843, train_wall=59, wall=0
2021-01-28 18:40:42 | INFO | train_inner | epoch 199:    152 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.096, nll_loss=0.885, ppl=1.85, wps=17538.4, ups=1.68, wpb=10418.3, bsz=378.9, num_updates=38300, lr=1.11949e-05, gnorm=0.837, train_wall=59, wall=0
2021-01-28 18:41:43 | INFO | train_inner | epoch 199:    252 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.098, nll_loss=0.888, ppl=1.85, wps=17444.2, ups=1.66, wpb=10504, bsz=374.8, num_updates=38400, lr=1.11803e-05, gnorm=0.82, train_wall=60, wall=0
2021-01-28 18:42:42 | INFO | train_inner | epoch 199:    352 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.094, nll_loss=0.883, ppl=1.84, wps=17597.1, ups=1.68, wpb=10488.4, bsz=371.7, num_updates=38500, lr=1.11658e-05, gnorm=0.83, train_wall=59, wall=0
2021-01-28 18:43:43 | INFO | train_inner | epoch 199:    452 / 561 symm_kl=0.294, self_kl=0, self_cv=0, loss=3.078, nll_loss=0.87, ppl=1.83, wps=17567.1, ups=1.66, wpb=10609.2, bsz=360.6, num_updates=38600, lr=1.11513e-05, gnorm=0.815, train_wall=60, wall=0
2021-01-28 18:44:42 | INFO | train_inner | epoch 199:    552 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.099, nll_loss=0.888, ppl=1.85, wps=17540, ups=1.67, wpb=10486.9, bsz=376.3, num_updates=38700, lr=1.11369e-05, gnorm=0.838, train_wall=60, wall=0
2021-01-28 18:44:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 18:44:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:44:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:44:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:44:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:44:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:44:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:44:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:44:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:44:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:44:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:44:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:44:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:44:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:44:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:44:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:44:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:44:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:44:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:44:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:44:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:44:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:44:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:44:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:44:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:44:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:44:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:44:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:44:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:44:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:44:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:44:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:44:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:44:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:44:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:44:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:44:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:44:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:44:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:44:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:45:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:45:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:45:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:45:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:45:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:45:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:45:09 | INFO | valid | epoch 199 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.231 | nll_loss 3.719 | ppl 13.17 | bleu 23.08 | wps 4658.2 | wpb 7508.5 | bsz 272.7 | num_updates 38709 | best_bleu 23.39
2021-01-28 18:45:09 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 18:45:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:45:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:45:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:45:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:45:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 199 @ 38709 updates, score 23.08) (writing took 3.1305090470123105 seconds)
2021-01-28 18:45:12 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2021-01-28 18:45:12 | INFO | train | epoch 199 | symm_kl 0.297 | self_kl 0 | self_cv 0 | loss 3.093 | nll_loss 0.883 | ppl 1.84 | wps 16233.3 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 38709 | lr 1.11356e-05 | gnorm 0.829 | train_wall 334 | wall 0
2021-01-28 18:45:12 | INFO | fairseq.trainer | begin training epoch 200
2021-01-28 18:45:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:45:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:46:09 | INFO | train_inner | epoch 200:     91 / 561 symm_kl=0.294, self_kl=0, self_cv=0, loss=3.08, nll_loss=0.874, ppl=1.83, wps=12138.7, ups=1.15, wpb=10513.6, bsz=366.8, num_updates=38800, lr=1.11226e-05, gnorm=0.829, train_wall=59, wall=0
2021-01-28 18:47:09 | INFO | train_inner | epoch 200:    191 / 561 symm_kl=0.296, self_kl=0, self_cv=0, loss=3.09, nll_loss=0.881, ppl=1.84, wps=17434.1, ups=1.66, wpb=10522.1, bsz=391.5, num_updates=38900, lr=1.11083e-05, gnorm=0.822, train_wall=60, wall=0
2021-01-28 18:48:09 | INFO | train_inner | epoch 200:    291 / 561 symm_kl=0.298, self_kl=0, self_cv=0, loss=3.094, nll_loss=0.882, ppl=1.84, wps=17407.7, ups=1.66, wpb=10462.8, bsz=365.5, num_updates=39000, lr=1.1094e-05, gnorm=0.831, train_wall=60, wall=0
2021-01-28 18:49:09 | INFO | train_inner | epoch 200:    391 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.096, nll_loss=0.887, ppl=1.85, wps=17315, ups=1.67, wpb=10349.9, bsz=366.2, num_updates=39100, lr=1.10798e-05, gnorm=0.841, train_wall=60, wall=0
2021-01-28 18:50:09 | INFO | train_inner | epoch 200:    491 / 561 symm_kl=0.297, self_kl=0, self_cv=0, loss=3.098, nll_loss=0.887, ppl=1.85, wps=17789.9, ups=1.68, wpb=10616.9, bsz=352.1, num_updates=39200, lr=1.10657e-05, gnorm=0.836, train_wall=59, wall=0
2021-01-28 18:50:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-01-28 18:50:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:50:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:50:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2021-01-28 18:50:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:50:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:50:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2021-01-28 18:50:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:50:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:50:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:50:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:50:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:50:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:50:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:50:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:50:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:50:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:50:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:50:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:50:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:50:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:50:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:50:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:50:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:50:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:50:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:50:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:50:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:50:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:50:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:50:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:50:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:50:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:50:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:50:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:50:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:50:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:51:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:51:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:51:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:51:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:51:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:51:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:51:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2021-01-28 18:51:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2021-01-28 18:51:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2021-01-28 18:51:11 | INFO | valid | epoch 200 | valid on 'valid' subset | symm_kl 0 | self_kl 0 | self_cv 0 | loss 5.231 | nll_loss 3.718 | ppl 13.16 | bleu 23.14 | wps 4839.6 | wpb 7508.5 | bsz 272.7 | num_updates 39270 | best_bleu 23.39
2021-01-28 18:51:11 | INFO | fairseq_cli.train | begin save checkpoint
2021-01-28 18:51:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer-all/checkpoint_last.pt (epoch 200 @ 39270 updates, score 23.14) (writing took 3.147259433986619 seconds)
2021-01-28 18:51:14 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2021-01-28 18:51:14 | INFO | train | epoch 200 | symm_kl 0.297 | self_kl 0 | self_cv 0 | loss 3.094 | nll_loss 0.883 | ppl 1.84 | wps 16229.5 | ups 1.55 | wpb 10483.4 | bsz 369.6 | num_updates 39270 | lr 1.10558e-05 | gnorm 0.833 | train_wall 335 | wall 0
2021-01-28 18:51:14 | INFO | fairseq_cli.train | done training in 26576.8 seconds
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 420 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
