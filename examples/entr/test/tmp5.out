nohup: ignoring input
criterion=label_smoothed_cross_entropy_r3f
label_smoothing=0.1
dropout=0.3
lr=0.0000125
lrscheduler=fixed
warmup_updates=0
max_epoch=100
r3f_lambda=0.065
save_dir=./examples/entr/bash/../checkpoints/closer_gap
extr=--seed 2 --noised-no-grad --noised-eval-model
2020-12-10 11:20:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:20:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:20:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:20:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:20:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:21:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:21:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:21:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:21:00 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:11618
2020-12-10 11:21:00 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:11618
2020-12-10 11:21:00 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-10 11:21:00 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:11618
2020-12-10 11:21:00 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-10 11:21:00 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-10 11:21:04 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy_r3f', cross_self_attention=False, curriculum=0, data='./examples/entr/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:11618', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[1.25e-05], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, noise_type='normal', noised_eval_model=True, noised_no_grad=True, nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.065, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./examples/entr/bash/../checkpoints/closer_gap', save_interval=1, save_interval_updates=0, scoring='bleu', seed=2, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='en', stop_time_hours=0, target_lang='tr', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.0, zero_sharding='none')
2020-12-10 11:21:04 | INFO | fairseq.tasks.translation | [en] dictionary: 19784 types
2020-12-10 11:21:04 | INFO | fairseq.tasks.translation | [tr] dictionary: 19784 types
2020-12-10 11:21:04 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.en
2020-12-10 11:21:04 | INFO | fairseq.data.data_utils | loaded 3000 examples from: ./examples/entr/bash/../data-bin/valid.en-tr.tr
2020-12-10 11:21:04 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin valid en-tr 3000 examples
2020-12-10 11:21:05 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19784, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19784, bias=False)
  )
)
2020-12-10 11:21:05 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-10 11:21:05 | INFO | fairseq_cli.train | model: transformer (TransformerModel)
2020-12-10 11:21:05 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy_r3f (LabelSmoothedCrossEntropyR3FCriterion)
2020-12-10 11:21:05 | INFO | fairseq_cli.train | num. model params: 54267904 (num. trained: 54267904)
2020-12-10 11:21:05 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2020-12-10 11:21:05 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2020-12-10 11:21:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-10 11:21:05 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-10 11:21:05 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-10 11:21:05 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-10 11:21:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-10 11:21:05 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-12-10 11:21:05 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2020-12-10 11:21:05 | INFO | fairseq.checkpoint_utils | loading pretrained model from ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt: optimizer, lr scheduler, meters, dataloader will be reset
2020-12-10 11:21:06 | INFO | fairseq.trainer | loaded checkpoint ./examples/entr/bash/../checkpoints/baseline/checkpoint_last.pt (epoch 106 @ 0 updates)
2020-12-10 11:21:06 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-10 11:21:06 | INFO | fairseq.trainer | loading train data for epoch 1
2020-12-10 11:21:06 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.en
2020-12-10 11:21:06 | INFO | fairseq.data.data_utils | loaded 207373 examples from: ./examples/entr/bash/../data-bin/train.en-tr.tr
2020-12-10 11:21:06 | INFO | fairseq.tasks.translation | ./examples/entr/bash/../data-bin train en-tr 207373 examples
2020-12-10 11:21:06 | INFO | fairseq.trainer | begin training epoch 1
2020-12-10 11:21:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:21:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:21:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:21:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:21:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:21:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:21:52 | INFO | train_inner | epoch 001:    100 / 561 symm_kl=27.886, loss=5.347, nll_loss=0.759, ppl=1.69, wps=24823.9, ups=2.37, wpb=10482.7, bsz=376.2, num_updates=100, lr=1.25e-05, gnorm=6.323, train_wall=43, wall=47
2020-12-10 11:22:35 | INFO | train_inner | epoch 001:    200 / 561 symm_kl=23.673, loss=5.014, nll_loss=0.73, ppl=1.66, wps=24606.8, ups=2.34, wpb=10517.8, bsz=376.3, num_updates=200, lr=1.25e-05, gnorm=4.98, train_wall=43, wall=90
2020-12-10 11:23:19 | INFO | train_inner | epoch 001:    300 / 561 symm_kl=23.667, loss=5.031, nll_loss=0.726, ppl=1.65, wps=23934.2, ups=2.27, wpb=10521.4, bsz=370.6, num_updates=300, lr=1.25e-05, gnorm=5.101, train_wall=44, wall=134
2020-12-10 11:24:03 | INFO | train_inner | epoch 001:    400 / 561 symm_kl=24.171, loss=5.097, nll_loss=0.733, ppl=1.66, wps=23974.6, ups=2.27, wpb=10568, bsz=353, num_updates=400, lr=1.25e-05, gnorm=5.244, train_wall=44, wall=178
2020-12-10 11:24:47 | INFO | train_inner | epoch 001:    500 / 561 symm_kl=23.151, loss=5.011, nll_loss=0.743, ppl=1.67, wps=23346.2, ups=2.25, wpb=10371.9, bsz=375.8, num_updates=500, lr=1.25e-05, gnorm=5.162, train_wall=44, wall=222
2020-12-10 11:25:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 11:25:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:25:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:25:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:25:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:25:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:25:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:25:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:25:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:25:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:25:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:25:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:25:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:25:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:25:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:25:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:25:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:25:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:25:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:25:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:25:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:25:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:25:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:25:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:25:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:25:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:25:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:25:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:25:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:25:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:25:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:25:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:25:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:25:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:25:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:25:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:25:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:25:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:25:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:25:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:25:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:25:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:25:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:25:34 | INFO | valid | epoch 001 | valid on 'valid' subset | symm_kl 0 | loss 6.504 | nll_loss 4.647 | ppl 25.05 | bleu 21.47 | wps 4984.8 | wpb 7508.5 | bsz 272.7 | num_updates 561
2020-12-10 11:25:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 11:25:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:25:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:25:36 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_best.pt (epoch 1 @ 561 updates, score 21.47) (writing took 2.146782523021102 seconds)
2020-12-10 11:25:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2020-12-10 11:25:36 | INFO | train | epoch 001 | symm_kl 24.416 | loss 5.095 | nll_loss 0.738 | ppl 1.67 | wps 22068.3 | ups 2.11 | wpb 10483.4 | bsz 369.6 | num_updates 561 | lr 1.25e-05 | gnorm 5.366 | train_wall 244 | wall 271
2020-12-10 11:25:36 | INFO | fairseq.trainer | begin training epoch 2
2020-12-10 11:25:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:25:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:25:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:25:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:25:56 | INFO | train_inner | epoch 002:     39 / 561 symm_kl=23.621, loss=5.044, nll_loss=0.731, ppl=1.66, wps=15224.8, ups=1.45, wpb=10484.5, bsz=365.2, num_updates=600, lr=1.25e-05, gnorm=5.32, train_wall=44, wall=291
2020-12-10 11:26:41 | INFO | train_inner | epoch 002:    139 / 561 symm_kl=22.623, loss=4.965, nll_loss=0.74, ppl=1.67, wps=23417.9, ups=2.25, wpb=10391.6, bsz=377.3, num_updates=700, lr=1.25e-05, gnorm=5.201, train_wall=44, wall=335
2020-12-10 11:27:25 | INFO | train_inner | epoch 002:    239 / 561 symm_kl=22.221, loss=4.926, nll_loss=0.74, ppl=1.67, wps=23238.2, ups=2.23, wpb=10429.3, bsz=372.1, num_updates=800, lr=1.25e-05, gnorm=5.07, train_wall=45, wall=380
2020-12-10 11:28:10 | INFO | train_inner | epoch 002:    339 / 561 symm_kl=23.339, loss=5.041, nll_loss=0.746, ppl=1.68, wps=23646.9, ups=2.25, wpb=10525.1, bsz=360.6, num_updates=900, lr=1.25e-05, gnorm=5.233, train_wall=44, wall=425
2020-12-10 11:28:55 | INFO | train_inner | epoch 002:    439 / 561 symm_kl=21.83, loss=4.892, nll_loss=0.743, ppl=1.67, wps=23493.8, ups=2.23, wpb=10547, bsz=392.9, num_updates=1000, lr=1.25e-05, gnorm=5.107, train_wall=45, wall=470
2020-12-10 11:29:40 | INFO | train_inner | epoch 002:    539 / 561 symm_kl=23.68, loss=5.079, nll_loss=0.745, ppl=1.68, wps=23681.9, ups=2.23, wpb=10601.2, bsz=353.4, num_updates=1100, lr=1.25e-05, gnorm=5.417, train_wall=45, wall=514
2020-12-10 11:29:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 11:29:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:29:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:29:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:29:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:29:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:29:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:29:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:29:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:29:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:29:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:29:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:29:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:29:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:29:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:29:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:29:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:29:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:29:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:29:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:29:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:29:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:29:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:29:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:29:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:29:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:29:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:29:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:29:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:29:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:29:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:29:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:29:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:29:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:29:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:29:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:29:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:29:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:29:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:29:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:30:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:30:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:30:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:30:12 | INFO | valid | epoch 002 | valid on 'valid' subset | symm_kl 0 | loss 6.572 | nll_loss 4.703 | ppl 26.05 | bleu 21.41 | wps 4159.7 | wpb 7508.5 | bsz 272.7 | num_updates 1122 | best_bleu 21.47
2020-12-10 11:30:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 11:30:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:30:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:30:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:30:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:30:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 2 @ 1122 updates, score 21.41) (writing took 3.036855783313513 seconds)
2020-12-10 11:30:15 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2020-12-10 11:30:15 | INFO | train | epoch 002 | symm_kl 22.853 | loss 4.99 | nll_loss 0.741 | ppl 1.67 | wps 21120.2 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 1122 | lr 1.25e-05 | gnorm 5.228 | train_wall 249 | wall 550
2020-12-10 11:30:15 | INFO | fairseq.trainer | begin training epoch 3
2020-12-10 11:30:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:30:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:30:52 | INFO | train_inner | epoch 003:     78 / 561 symm_kl=23.666, loss=5.079, nll_loss=0.749, ppl=1.68, wps=14457.9, ups=1.38, wpb=10451.2, bsz=357.2, num_updates=1200, lr=1.25e-05, gnorm=5.547, train_wall=43, wall=587
2020-12-10 11:31:36 | INFO | train_inner | epoch 003:    178 / 561 symm_kl=22.967, loss=5.017, nll_loss=0.753, ppl=1.68, wps=23004.8, ups=2.25, wpb=10210.1, bsz=364.4, num_updates=1300, lr=1.25e-05, gnorm=5.449, train_wall=44, wall=631
2020-12-10 11:32:21 | INFO | train_inner | epoch 003:    278 / 561 symm_kl=22.937, loss=5.02, nll_loss=0.754, ppl=1.69, wps=23593.6, ups=2.23, wpb=10559.7, bsz=356.9, num_updates=1400, lr=1.25e-05, gnorm=5.368, train_wall=45, wall=676
2020-12-10 11:33:06 | INFO | train_inner | epoch 003:    378 / 561 symm_kl=21.742, loss=4.889, nll_loss=0.74, ppl=1.67, wps=23494.6, ups=2.23, wpb=10526.6, bsz=371.2, num_updates=1500, lr=1.25e-05, gnorm=5.078, train_wall=45, wall=721
2020-12-10 11:33:51 | INFO | train_inner | epoch 003:    478 / 561 symm_kl=21.757, loss=4.911, nll_loss=0.761, ppl=1.69, wps=23579.3, ups=2.23, wpb=10588.5, bsz=385.2, num_updates=1600, lr=1.25e-05, gnorm=5.115, train_wall=45, wall=766
2020-12-10 11:34:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 11:34:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:34:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:34:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:34:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:34:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:34:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:34:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:34:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:34:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:34:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:34:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:34:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:34:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:34:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:34:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:34:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:34:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:34:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:34:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:34:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:34:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:34:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:34:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:34:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:34:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:34:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:34:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:34:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:34:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:34:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:34:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:34:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:34:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:34:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:34:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:34:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:34:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:34:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:34:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:34:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:34:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:34:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:34:48 | INFO | valid | epoch 003 | valid on 'valid' subset | symm_kl 0 | loss 6.585 | nll_loss 4.714 | ppl 26.25 | bleu 21.22 | wps 4802.5 | wpb 7508.5 | bsz 272.7 | num_updates 1683 | best_bleu 21.47
2020-12-10 11:34:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 11:34:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:34:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:34:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:34:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:34:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 3 @ 1683 updates, score 21.22) (writing took 3.094011854380369 seconds)
2020-12-10 11:34:51 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2020-12-10 11:34:51 | INFO | train | epoch 003 | symm_kl 22.26 | loss 4.949 | nll_loss 0.751 | ppl 1.68 | wps 21299.1 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 1683 | lr 1.25e-05 | gnorm 5.216 | train_wall 249 | wall 826
2020-12-10 11:34:51 | INFO | fairseq.trainer | begin training epoch 4
2020-12-10 11:34:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:34:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:35:01 | INFO | train_inner | epoch 004:     17 / 561 symm_kl=21.373, loss=4.86, nll_loss=0.751, ppl=1.68, wps=14828.1, ups=1.42, wpb=10457.3, bsz=371.7, num_updates=1700, lr=1.25e-05, gnorm=4.996, train_wall=44, wall=836
2020-12-10 11:35:45 | INFO | train_inner | epoch 004:    117 / 561 symm_kl=21.511, loss=4.875, nll_loss=0.743, ppl=1.67, wps=23811.1, ups=2.26, wpb=10525.7, bsz=369.4, num_updates=1800, lr=1.25e-05, gnorm=5.081, train_wall=44, wall=880
2020-12-10 11:36:30 | INFO | train_inner | epoch 004:    217 / 561 symm_kl=21.987, loss=4.939, nll_loss=0.767, ppl=1.7, wps=23199, ups=2.24, wpb=10360.6, bsz=362.4, num_updates=1900, lr=1.25e-05, gnorm=5.182, train_wall=44, wall=925
2020-12-10 11:37:15 | INFO | train_inner | epoch 004:    317 / 561 symm_kl=22.568, loss=4.996, nll_loss=0.765, ppl=1.7, wps=23703, ups=2.24, wpb=10595.1, bsz=366.3, num_updates=2000, lr=1.25e-05, gnorm=5.292, train_wall=45, wall=970
2020-12-10 11:37:59 | INFO | train_inner | epoch 004:    417 / 561 symm_kl=20.52, loss=4.79, nll_loss=0.757, ppl=1.69, wps=23541.7, ups=2.25, wpb=10466.1, bsz=390, num_updates=2100, lr=1.25e-05, gnorm=4.835, train_wall=44, wall=1014
2020-12-10 11:38:44 | INFO | train_inner | epoch 004:    517 / 561 symm_kl=21.746, loss=4.901, nll_loss=0.746, ppl=1.68, wps=23773.6, ups=2.24, wpb=10609.2, bsz=365.7, num_updates=2200, lr=1.25e-05, gnorm=5.275, train_wall=44, wall=1059
2020-12-10 11:39:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 11:39:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:39:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:39:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:39:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:39:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:39:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:39:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:39:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:39:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:39:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:39:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:39:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:39:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:39:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:39:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:39:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:39:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:39:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:39:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:39:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:39:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:39:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:39:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:39:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:39:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:39:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:39:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:39:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:39:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:39:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:39:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:39:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:39:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:39:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:39:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:39:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:39:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:39:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:39:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:39:25 | INFO | valid | epoch 004 | valid on 'valid' subset | symm_kl 0 | loss 6.617 | nll_loss 4.739 | ppl 26.71 | bleu 21.31 | wps 4373.7 | wpb 7508.5 | bsz 272.7 | num_updates 2244 | best_bleu 21.47
2020-12-10 11:39:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 11:39:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:39:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:39:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:39:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:39:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 4 @ 2244 updates, score 21.31) (writing took 3.033178685232997 seconds)
2020-12-10 11:39:28 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2020-12-10 11:39:28 | INFO | train | epoch 004 | symm_kl 21.785 | loss 4.912 | nll_loss 0.756 | ppl 1.69 | wps 21201.8 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 2244 | lr 1.25e-05 | gnorm 5.171 | train_wall 249 | wall 1103
2020-12-10 11:39:28 | INFO | fairseq.trainer | begin training epoch 5
2020-12-10 11:39:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:39:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:39:55 | INFO | train_inner | epoch 005:     56 / 561 symm_kl=21.477, loss=4.881, nll_loss=0.755, ppl=1.69, wps=14329, ups=1.4, wpb=10262.6, bsz=376.2, num_updates=2300, lr=1.25e-05, gnorm=5.216, train_wall=44, wall=1130
2020-12-10 11:40:40 | INFO | train_inner | epoch 005:    156 / 561 symm_kl=21.64, loss=4.899, nll_loss=0.756, ppl=1.69, wps=23555.9, ups=2.24, wpb=10538.7, bsz=372, num_updates=2400, lr=1.25e-05, gnorm=5.178, train_wall=45, wall=1175
2020-12-10 11:41:25 | INFO | train_inner | epoch 005:    256 / 561 symm_kl=20.904, loss=4.832, nll_loss=0.757, ppl=1.69, wps=23503.6, ups=2.22, wpb=10569.5, bsz=376.6, num_updates=2500, lr=1.25e-05, gnorm=5.043, train_wall=45, wall=1220
2020-12-10 11:42:10 | INFO | train_inner | epoch 005:    356 / 561 symm_kl=22.231, loss=4.97, nll_loss=0.767, ppl=1.7, wps=23427.2, ups=2.23, wpb=10486.3, bsz=360, num_updates=2600, lr=1.25e-05, gnorm=5.385, train_wall=45, wall=1265
2020-12-10 11:42:54 | INFO | train_inner | epoch 005:    456 / 561 symm_kl=21.209, loss=4.868, nll_loss=0.762, ppl=1.7, wps=23662.6, ups=2.25, wpb=10509.6, bsz=364.1, num_updates=2700, lr=1.25e-05, gnorm=4.991, train_wall=44, wall=1309
2020-12-10 11:43:39 | INFO | train_inner | epoch 005:    556 / 561 symm_kl=20.984, loss=4.855, nll_loss=0.773, ppl=1.71, wps=23486.6, ups=2.24, wpb=10469.7, bsz=372.2, num_updates=2800, lr=1.25e-05, gnorm=5.05, train_wall=44, wall=1354
2020-12-10 11:43:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 11:43:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:43:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:43:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:43:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:43:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:43:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:43:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:43:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:43:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:43:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:43:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:43:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:43:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:43:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:43:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:43:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:43:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:43:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:43:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:43:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:43:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:43:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:43:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:43:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:43:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:43:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:43:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:43:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:43:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:43:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:44:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:44:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:44:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:44:02 | INFO | valid | epoch 005 | valid on 'valid' subset | symm_kl 0 | loss 6.627 | nll_loss 4.747 | ppl 26.85 | bleu 21.33 | wps 4615.3 | wpb 7508.5 | bsz 272.7 | num_updates 2805 | best_bleu 21.47
2020-12-10 11:44:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 11:44:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:44:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:44:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:44:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:44:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 5 @ 2805 updates, score 21.33) (writing took 3.025022814050317 seconds)
2020-12-10 11:44:05 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2020-12-10 11:44:05 | INFO | train | epoch 005 | symm_kl 21.395 | loss 4.883 | nll_loss 0.761 | ppl 1.7 | wps 21242.7 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 2805 | lr 1.25e-05 | gnorm 5.137 | train_wall 249 | wall 1380
2020-12-10 11:44:05 | INFO | fairseq.trainer | begin training epoch 6
2020-12-10 11:44:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:44:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:44:50 | INFO | train_inner | epoch 006:     95 / 561 symm_kl=21.146, loss=4.859, nll_loss=0.764, ppl=1.7, wps=14517.7, ups=1.4, wpb=10363.8, bsz=371, num_updates=2900, lr=1.25e-05, gnorm=5.106, train_wall=44, wall=1425
2020-12-10 11:45:35 | INFO | train_inner | epoch 006:    195 / 561 symm_kl=21.066, loss=4.864, nll_loss=0.771, ppl=1.71, wps=23346.2, ups=2.24, wpb=10429.6, bsz=360.2, num_updates=3000, lr=1.25e-05, gnorm=5.042, train_wall=45, wall=1470
2020-12-10 11:46:20 | INFO | train_inner | epoch 006:    295 / 561 symm_kl=21.087, loss=4.858, nll_loss=0.764, ppl=1.7, wps=23252.4, ups=2.24, wpb=10376.3, bsz=369.1, num_updates=3100, lr=1.25e-05, gnorm=5.19, train_wall=44, wall=1514
2020-12-10 11:47:05 | INFO | train_inner | epoch 006:    395 / 561 symm_kl=21.307, loss=4.882, nll_loss=0.767, ppl=1.7, wps=23408.5, ups=2.23, wpb=10514.7, bsz=367.6, num_updates=3200, lr=1.25e-05, gnorm=5.211, train_wall=45, wall=1559
2020-12-10 11:47:49 | INFO | train_inner | epoch 006:    495 / 561 symm_kl=21.339, loss=4.876, nll_loss=0.755, ppl=1.69, wps=23605.1, ups=2.23, wpb=10597.2, bsz=369.5, num_updates=3300, lr=1.25e-05, gnorm=5.333, train_wall=45, wall=1604
2020-12-10 11:48:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 11:48:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:48:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:48:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:48:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:48:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:48:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:48:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:48:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:48:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:48:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:48:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:48:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:48:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:48:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:48:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:48:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:48:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:48:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:48:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:48:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:48:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:48:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:48:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:48:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:48:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:48:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:48:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:48:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:48:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:48:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:48:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:48:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:48:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:48:39 | INFO | valid | epoch 006 | valid on 'valid' subset | symm_kl 0 | loss 6.651 | nll_loss 4.766 | ppl 27.22 | bleu 21.18 | wps 4747.4 | wpb 7508.5 | bsz 272.7 | num_updates 3366 | best_bleu 21.47
2020-12-10 11:48:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 11:48:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:48:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:48:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:48:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:48:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 6 @ 3366 updates, score 21.18) (writing took 3.001941254362464 seconds)
2020-12-10 11:48:42 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2020-12-10 11:48:42 | INFO | train | epoch 006 | symm_kl 21.064 | loss 4.857 | nll_loss 0.765 | ppl 1.7 | wps 21232.1 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 3366 | lr 1.25e-05 | gnorm 5.128 | train_wall 250 | wall 1657
2020-12-10 11:48:42 | INFO | fairseq.trainer | begin training epoch 7
2020-12-10 11:48:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:48:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:49:00 | INFO | train_inner | epoch 007:     34 / 561 symm_kl=20.468, loss=4.795, nll_loss=0.76, ppl=1.69, wps=14977.4, ups=1.42, wpb=10549.6, bsz=375.3, num_updates=3400, lr=1.25e-05, gnorm=4.946, train_wall=44, wall=1675
2020-12-10 11:49:44 | INFO | train_inner | epoch 007:    134 / 561 symm_kl=21.946, loss=4.954, nll_loss=0.773, ppl=1.71, wps=23864, ups=2.25, wpb=10616.7, bsz=363.8, num_updates=3500, lr=1.25e-05, gnorm=5.246, train_wall=44, wall=1719
2020-12-10 11:50:29 | INFO | train_inner | epoch 007:    234 / 561 symm_kl=20.735, loss=4.83, nll_loss=0.767, ppl=1.7, wps=23535.7, ups=2.25, wpb=10442.2, bsz=371.5, num_updates=3600, lr=1.25e-05, gnorm=5.082, train_wall=44, wall=1764
2020-12-10 11:51:14 | INFO | train_inner | epoch 007:    334 / 561 symm_kl=20.301, loss=4.788, nll_loss=0.768, ppl=1.7, wps=23171.5, ups=2.23, wpb=10375.1, bsz=374.2, num_updates=3700, lr=1.25e-05, gnorm=5.009, train_wall=45, wall=1808
2020-12-10 11:51:58 | INFO | train_inner | epoch 007:    434 / 561 symm_kl=19.387, loss=4.685, nll_loss=0.755, ppl=1.69, wps=23568.9, ups=2.23, wpb=10591.7, bsz=386.2, num_updates=3800, lr=1.25e-05, gnorm=4.751, train_wall=45, wall=1853
2020-12-10 11:52:43 | INFO | train_inner | epoch 007:    534 / 561 symm_kl=21.357, loss=4.904, nll_loss=0.781, ppl=1.72, wps=23369.3, ups=2.23, wpb=10460.4, bsz=356.2, num_updates=3900, lr=1.25e-05, gnorm=5.081, train_wall=45, wall=1898
2020-12-10 11:52:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 11:52:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:52:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:52:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:52:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:52:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:52:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:53:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:53:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:53:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:53:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:53:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:53:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:53:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:53:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:53:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:53:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:53:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:53:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:53:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:53:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:53:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:53:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:53:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:53:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:53:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:53:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:53:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:53:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:53:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:53:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:53:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:53:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:53:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:53:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:53:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:53:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:53:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:53:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:53:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:53:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:53:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:53:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:53:18 | INFO | valid | epoch 007 | valid on 'valid' subset | symm_kl 0 | loss 6.662 | nll_loss 4.775 | ppl 27.37 | bleu 21.2 | wps 4162.5 | wpb 7508.5 | bsz 272.7 | num_updates 3927 | best_bleu 21.47
2020-12-10 11:53:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 11:53:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:53:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:53:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:53:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:53:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 7 @ 3927 updates, score 21.2) (writing took 3.0340638048946857 seconds)
2020-12-10 11:53:21 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2020-12-10 11:53:21 | INFO | train | epoch 007 | symm_kl 20.753 | loss 4.832 | nll_loss 0.768 | ppl 1.7 | wps 21094 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 3927 | lr 1.25e-05 | gnorm 5.053 | train_wall 249 | wall 1936
2020-12-10 11:53:21 | INFO | fairseq.trainer | begin training epoch 8
2020-12-10 11:53:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:53:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:53:56 | INFO | train_inner | epoch 008:     73 / 561 symm_kl=20.473, loss=4.808, nll_loss=0.771, ppl=1.71, wps=14375.1, ups=1.38, wpb=10399.4, bsz=373.3, num_updates=4000, lr=1.25e-05, gnorm=5.019, train_wall=43, wall=1970
2020-12-10 11:54:40 | INFO | train_inner | epoch 008:    173 / 561 symm_kl=21.785, loss=4.941, nll_loss=0.775, ppl=1.71, wps=23444.3, ups=2.23, wpb=10490.8, bsz=358.6, num_updates=4100, lr=1.25e-05, gnorm=5.414, train_wall=45, wall=2015
2020-12-10 11:55:25 | INFO | train_inner | epoch 008:    273 / 561 symm_kl=20.557, loss=4.825, nll_loss=0.775, ppl=1.71, wps=23596, ups=2.24, wpb=10548, bsz=363.5, num_updates=4200, lr=1.25e-05, gnorm=4.998, train_wall=45, wall=2060
2020-12-10 11:56:10 | INFO | train_inner | epoch 008:    373 / 561 symm_kl=19.875, loss=4.744, nll_loss=0.767, ppl=1.7, wps=23357.8, ups=2.24, wpb=10431.3, bsz=376.8, num_updates=4300, lr=1.25e-05, gnorm=4.989, train_wall=44, wall=2105
2020-12-10 11:56:55 | INFO | train_inner | epoch 008:    473 / 561 symm_kl=20.005, loss=4.762, nll_loss=0.771, ppl=1.71, wps=23563.7, ups=2.23, wpb=10561.2, bsz=379.6, num_updates=4400, lr=1.25e-05, gnorm=5.027, train_wall=45, wall=2149
2020-12-10 11:57:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 11:57:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:57:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:57:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:57:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:57:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:57:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:57:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:57:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:57:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:57:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:57:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:57:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:57:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:57:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:57:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:57:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:57:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:57:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:57:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:57:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:57:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:57:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:57:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:57:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:57:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:57:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:57:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:57:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:57:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:57:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:57:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:57:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:57:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:57:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:57:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:57:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:57:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 11:57:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 11:57:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 11:57:54 | INFO | valid | epoch 008 | valid on 'valid' subset | symm_kl 0 | loss 6.673 | nll_loss 4.784 | ppl 27.54 | bleu 21.26 | wps 4700.4 | wpb 7508.5 | bsz 272.7 | num_updates 4488 | best_bleu 21.47
2020-12-10 11:57:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 11:57:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:57:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:57:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:57:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:57:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 8 @ 4488 updates, score 21.26) (writing took 3.005347555503249 seconds)
2020-12-10 11:57:57 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2020-12-10 11:57:57 | INFO | train | epoch 008 | symm_kl 20.565 | loss 4.82 | nll_loss 0.773 | ppl 1.71 | wps 21286.9 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 4488 | lr 1.25e-05 | gnorm 5.106 | train_wall 249 | wall 2212
2020-12-10 11:57:57 | INFO | fairseq.trainer | begin training epoch 9
2020-12-10 11:57:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 11:58:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 11:58:06 | INFO | train_inner | epoch 009:     12 / 561 symm_kl=20.369, loss=4.808, nll_loss=0.779, ppl=1.72, wps=14571.8, ups=1.41, wpb=10353.3, bsz=367.4, num_updates=4500, lr=1.25e-05, gnorm=5.119, train_wall=44, wall=2220
2020-12-10 11:58:50 | INFO | train_inner | epoch 009:    112 / 561 symm_kl=20.452, loss=4.813, nll_loss=0.777, ppl=1.71, wps=23851.1, ups=2.26, wpb=10574.2, bsz=365.3, num_updates=4600, lr=1.25e-05, gnorm=5.046, train_wall=44, wall=2265
2020-12-10 11:59:34 | INFO | train_inner | epoch 009:    212 / 561 symm_kl=21.065, loss=4.881, nll_loss=0.783, ppl=1.72, wps=23460, ups=2.25, wpb=10443.6, bsz=357.4, num_updates=4700, lr=1.25e-05, gnorm=5.19, train_wall=44, wall=2309
2020-12-10 12:00:19 | INFO | train_inner | epoch 009:    312 / 561 symm_kl=20.718, loss=4.842, nll_loss=0.776, ppl=1.71, wps=23411.3, ups=2.23, wpb=10502.1, bsz=359.9, num_updates=4800, lr=1.25e-05, gnorm=5.098, train_wall=45, wall=2354
2020-12-10 12:01:04 | INFO | train_inner | epoch 009:    412 / 561 symm_kl=18.878, loss=4.639, nll_loss=0.758, ppl=1.69, wps=23505.2, ups=2.23, wpb=10549.5, bsz=395.3, num_updates=4900, lr=1.25e-05, gnorm=4.851, train_wall=45, wall=2399
2020-12-10 12:01:49 | INFO | train_inner | epoch 009:    512 / 561 symm_kl=20.774, loss=4.85, nll_loss=0.781, ppl=1.72, wps=23278.8, ups=2.24, wpb=10400.3, bsz=362.6, num_updates=5000, lr=1.25e-05, gnorm=5.173, train_wall=45, wall=2444
2020-12-10 12:02:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:02:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:02:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:02:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:02:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:02:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:02:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:02:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:02:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:02:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:02:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:02:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:02:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:02:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:02:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:02:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:02:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:02:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:02:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:02:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:02:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:02:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:02:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:02:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:02:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:02:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:02:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:02:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:02:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:02:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:02:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:02:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:02:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:02:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:02:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:02:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:02:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:02:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:02:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:02:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:02:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:02:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:02:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:02:30 | INFO | valid | epoch 009 | valid on 'valid' subset | symm_kl 0 | loss 6.67 | nll_loss 4.782 | ppl 27.52 | bleu 21.34 | wps 4970.3 | wpb 7508.5 | bsz 272.7 | num_updates 5049 | best_bleu 21.47
2020-12-10 12:02:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:02:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:02:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:02:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 9 @ 5049 updates, score 21.34) (writing took 3.00262001901865 seconds)
2020-12-10 12:02:33 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-12-10 12:02:33 | INFO | train | epoch 009 | symm_kl 20.287 | loss 4.797 | nll_loss 0.775 | ppl 1.71 | wps 21294.4 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 5049 | lr 1.25e-05 | gnorm 5.047 | train_wall 249 | wall 2488
2020-12-10 12:02:33 | INFO | fairseq.trainer | begin training epoch 10
2020-12-10 12:02:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:02:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:02:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:02:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:02:58 | INFO | train_inner | epoch 010:     51 / 561 symm_kl=20.266, loss=4.8, nll_loss=0.777, ppl=1.71, wps=14968.7, ups=1.44, wpb=10423.2, bsz=367.1, num_updates=5100, lr=1.25e-05, gnorm=5.008, train_wall=44, wall=2513
2020-12-10 12:03:43 | INFO | train_inner | epoch 010:    151 / 561 symm_kl=20.306, loss=4.805, nll_loss=0.781, ppl=1.72, wps=23425.1, ups=2.25, wpb=10429.2, bsz=366.6, num_updates=5200, lr=1.25e-05, gnorm=5.047, train_wall=44, wall=2558
2020-12-10 12:04:28 | INFO | train_inner | epoch 010:    251 / 561 symm_kl=19.62, loss=4.723, nll_loss=0.768, ppl=1.7, wps=23523.4, ups=2.22, wpb=10576.6, bsz=378.6, num_updates=5300, lr=1.25e-05, gnorm=5.015, train_wall=45, wall=2603
2020-12-10 12:05:13 | INFO | train_inner | epoch 010:    351 / 561 symm_kl=21.199, loss=4.905, nll_loss=0.797, ppl=1.74, wps=23181.8, ups=2.23, wpb=10400, bsz=367.6, num_updates=5400, lr=1.25e-05, gnorm=5.404, train_wall=45, wall=2648
2020-12-10 12:05:58 | INFO | train_inner | epoch 010:    451 / 561 symm_kl=19.678, loss=4.738, nll_loss=0.773, ppl=1.71, wps=23582.9, ups=2.23, wpb=10577.2, bsz=375, num_updates=5500, lr=1.25e-05, gnorm=4.875, train_wall=45, wall=2693
2020-12-10 12:06:43 | INFO | train_inner | epoch 010:    551 / 561 symm_kl=19.519, loss=4.729, nll_loss=0.78, ppl=1.72, wps=23332.9, ups=2.22, wpb=10496.6, bsz=368.8, num_updates=5600, lr=1.25e-05, gnorm=4.813, train_wall=45, wall=2737
2020-12-10 12:06:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:06:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:06:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:06:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:06:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:06:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:06:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:06:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:06:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:06:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:06:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:06:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:06:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:06:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:06:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:06:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:06:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:06:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:06:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:06:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:06:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:06:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:06:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:06:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:06:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:06:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:06:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:06:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:06:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:06:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:06:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:06:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:06:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:06:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:06:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:06:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:06:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:06:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:06:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:06:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:07:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:07:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:07:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:07:07 | INFO | valid | epoch 010 | valid on 'valid' subset | symm_kl 0 | loss 6.679 | nll_loss 4.791 | ppl 27.68 | bleu 21.18 | wps 4852.8 | wpb 7508.5 | bsz 272.7 | num_updates 5610 | best_bleu 21.47
2020-12-10 12:07:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:07:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:07:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:07:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:07:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:07:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 10 @ 5610 updates, score 21.18) (writing took 2.957403875887394 seconds)
2020-12-10 12:07:10 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2020-12-10 12:07:10 | INFO | train | epoch 010 | symm_kl 20.116 | loss 4.784 | nll_loss 0.779 | ppl 1.72 | wps 21255.9 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 5610 | lr 1.25e-05 | gnorm 5.049 | train_wall 250 | wall 2765
2020-12-10 12:07:10 | INFO | fairseq.trainer | begin training epoch 11
2020-12-10 12:07:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:07:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:07:53 | INFO | train_inner | epoch 011:     90 / 561 symm_kl=19.641, loss=4.734, nll_loss=0.771, ppl=1.71, wps=14872.2, ups=1.43, wpb=10419.9, bsz=365.3, num_updates=5700, lr=1.25e-05, gnorm=4.994, train_wall=44, wall=2808
2020-12-10 12:08:37 | INFO | train_inner | epoch 011:    190 / 561 symm_kl=19.36, loss=4.714, nll_loss=0.782, ppl=1.72, wps=23245.3, ups=2.23, wpb=10404.6, bsz=380.4, num_updates=5800, lr=1.25e-05, gnorm=4.906, train_wall=45, wall=2852
2020-12-10 12:09:23 | INFO | train_inner | epoch 011:    290 / 561 symm_kl=20.308, loss=4.807, nll_loss=0.783, ppl=1.72, wps=23551.5, ups=2.22, wpb=10628.1, bsz=374.5, num_updates=5900, lr=1.25e-05, gnorm=5.224, train_wall=45, wall=2897
2020-12-10 12:10:08 | INFO | train_inner | epoch 011:    390 / 561 symm_kl=20.257, loss=4.809, nll_loss=0.79, ppl=1.73, wps=23464.3, ups=2.22, wpb=10554.1, bsz=366.4, num_updates=6000, lr=1.25e-05, gnorm=5.098, train_wall=45, wall=2942
2020-12-10 12:10:52 | INFO | train_inner | epoch 011:    490 / 561 symm_kl=19.815, loss=4.759, nll_loss=0.782, ppl=1.72, wps=23318.9, ups=2.24, wpb=10409.1, bsz=365.9, num_updates=6100, lr=1.25e-05, gnorm=4.972, train_wall=44, wall=2987
2020-12-10 12:11:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:11:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:11:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:11:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:11:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:11:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:11:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:11:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:11:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:11:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:11:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:11:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:11:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:11:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:11:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:11:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:11:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:11:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:11:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:11:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:11:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:11:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:11:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:11:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:11:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:11:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:11:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:11:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:11:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:11:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:11:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:11:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:11:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:11:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:11:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:11:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:11:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:11:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:11:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:11:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:11:45 | INFO | valid | epoch 011 | valid on 'valid' subset | symm_kl 0 | loss 6.685 | nll_loss 4.796 | ppl 27.79 | bleu 21.14 | wps 4579.7 | wpb 7508.5 | bsz 272.7 | num_updates 6171 | best_bleu 21.47
2020-12-10 12:11:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:11:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:11:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:11:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:11:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:11:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 11 @ 6171 updates, score 21.14) (writing took 3.076664511114359 seconds)
2020-12-10 12:11:48 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2020-12-10 12:11:48 | INFO | train | epoch 011 | symm_kl 19.907 | loss 4.768 | nll_loss 0.782 | ppl 1.72 | wps 21141 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 6171 | lr 1.25e-05 | gnorm 5.041 | train_wall 250 | wall 3043
2020-12-10 12:11:48 | INFO | fairseq.trainer | begin training epoch 12
2020-12-10 12:11:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:11:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:12:04 | INFO | train_inner | epoch 012:     29 / 561 symm_kl=20.208, loss=4.798, nll_loss=0.782, ppl=1.72, wps=14596.7, ups=1.4, wpb=10442.9, bsz=363.4, num_updates=6200, lr=1.25e-05, gnorm=5.141, train_wall=44, wall=3059
2020-12-10 12:12:48 | INFO | train_inner | epoch 012:    129 / 561 symm_kl=21.534, loss=4.941, nll_loss=0.79, ppl=1.73, wps=23701.1, ups=2.25, wpb=10525.3, bsz=348.6, num_updates=6300, lr=1.25e-05, gnorm=5.462, train_wall=44, wall=3103
2020-12-10 12:13:33 | INFO | train_inner | epoch 012:    229 / 561 symm_kl=18.844, loss=4.665, nll_loss=0.777, ppl=1.71, wps=23622.8, ups=2.23, wpb=10608.6, bsz=373.5, num_updates=6400, lr=1.25e-05, gnorm=4.711, train_wall=45, wall=3148
2020-12-10 12:14:18 | INFO | train_inner | epoch 012:    329 / 561 symm_kl=19.874, loss=4.76, nll_loss=0.779, ppl=1.72, wps=23140.6, ups=2.22, wpb=10404.7, bsz=375.6, num_updates=6500, lr=1.25e-05, gnorm=5.011, train_wall=45, wall=3193
2020-12-10 12:15:03 | INFO | train_inner | epoch 012:    429 / 561 symm_kl=19.053, loss=4.689, nll_loss=0.786, ppl=1.72, wps=23623, ups=2.24, wpb=10567.5, bsz=376.2, num_updates=6600, lr=1.25e-05, gnorm=4.836, train_wall=45, wall=3238
2020-12-10 12:15:47 | INFO | train_inner | epoch 012:    529 / 561 symm_kl=19.45, loss=4.73, nll_loss=0.789, ppl=1.73, wps=23167.9, ups=2.24, wpb=10354, bsz=370.4, num_updates=6700, lr=1.25e-05, gnorm=4.947, train_wall=45, wall=3282
2020-12-10 12:16:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:16:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:16:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:16:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:16:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:16:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:16:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:16:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:16:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:16:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:16:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:16:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:16:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:16:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:16:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:16:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:16:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:16:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:16:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:16:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:16:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:16:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:16:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:16:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:16:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:16:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:16:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:16:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:16:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:16:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:16:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:16:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:16:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:16:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:16:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:16:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:16:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:16:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:16:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:16:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:16:24 | INFO | valid | epoch 012 | valid on 'valid' subset | symm_kl 0 | loss 6.687 | nll_loss 4.799 | ppl 27.85 | bleu 21.14 | wps 4324.6 | wpb 7508.5 | bsz 272.7 | num_updates 6732 | best_bleu 21.47
2020-12-10 12:16:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:16:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:16:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:16:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:16:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:16:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 12 @ 6732 updates, score 21.14) (writing took 3.031795222312212 seconds)
2020-12-10 12:16:27 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2020-12-10 12:16:27 | INFO | train | epoch 012 | symm_kl 19.747 | loss 4.757 | nll_loss 0.785 | ppl 1.72 | wps 21123.5 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 6732 | lr 1.25e-05 | gnorm 4.988 | train_wall 249 | wall 3322
2020-12-10 12:16:27 | INFO | fairseq.trainer | begin training epoch 13
2020-12-10 12:16:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:16:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:16:59 | INFO | train_inner | epoch 013:     68 / 561 symm_kl=19.669, loss=4.752, nll_loss=0.787, ppl=1.73, wps=14621.4, ups=1.39, wpb=10529.5, bsz=376.6, num_updates=6800, lr=1.25e-05, gnorm=4.919, train_wall=44, wall=3354
2020-12-10 12:17:45 | INFO | train_inner | epoch 013:    168 / 561 symm_kl=20.131, loss=4.796, nll_loss=0.786, ppl=1.72, wps=23542.3, ups=2.22, wpb=10615, bsz=360.4, num_updates=6900, lr=1.25e-05, gnorm=5.133, train_wall=45, wall=3399
2020-12-10 12:18:29 | INFO | train_inner | epoch 013:    268 / 561 symm_kl=19.15, loss=4.698, nll_loss=0.784, ppl=1.72, wps=23371.5, ups=2.25, wpb=10407.2, bsz=369.4, num_updates=7000, lr=1.25e-05, gnorm=4.858, train_wall=44, wall=3444
2020-12-10 12:19:13 | INFO | train_inner | epoch 013:    368 / 561 symm_kl=20.082, loss=4.799, nll_loss=0.796, ppl=1.74, wps=23375.9, ups=2.25, wpb=10369, bsz=371.4, num_updates=7100, lr=1.25e-05, gnorm=5.217, train_wall=44, wall=3488
2020-12-10 12:19:58 | INFO | train_inner | epoch 013:    468 / 561 symm_kl=19.68, loss=4.76, nll_loss=0.794, ppl=1.73, wps=23382.4, ups=2.23, wpb=10465.5, bsz=368.8, num_updates=7200, lr=1.25e-05, gnorm=5.075, train_wall=45, wall=3533
2020-12-10 12:20:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:20:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:20:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:20:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:20:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:20:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:20:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:20:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:20:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:20:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:20:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:20:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:20:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:20:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:20:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:20:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:20:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:20:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:20:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:20:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:20:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:20:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:20:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:20:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:20:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:20:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:20:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:20:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:20:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:20:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:20:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:20:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:20:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:20:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:20:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:20:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:20:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:21:00 | INFO | valid | epoch 013 | valid on 'valid' subset | symm_kl 0 | loss 6.687 | nll_loss 4.801 | ppl 27.88 | bleu 21.2 | wps 4763 | wpb 7508.5 | bsz 272.7 | num_updates 7293 | best_bleu 21.47
2020-12-10 12:21:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:21:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:21:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:21:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:21:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:21:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 13 @ 7293 updates, score 21.2) (writing took 2.8479112405329943 seconds)
2020-12-10 12:21:03 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2020-12-10 12:21:03 | INFO | train | epoch 013 | symm_kl 19.535 | loss 4.74 | nll_loss 0.788 | ppl 1.73 | wps 21272.2 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 7293 | lr 1.25e-05 | gnorm 4.995 | train_wall 249 | wall 3598
2020-12-10 12:21:03 | INFO | fairseq.trainer | begin training epoch 14
2020-12-10 12:21:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:21:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:21:09 | INFO | train_inner | epoch 014:      7 / 561 symm_kl=18.622, loss=4.648, nll_loss=0.785, ppl=1.72, wps=14764.6, ups=1.41, wpb=10495.9, bsz=374.4, num_updates=7300, lr=1.25e-05, gnorm=4.73, train_wall=45, wall=3604
2020-12-10 12:21:53 | INFO | train_inner | epoch 014:    107 / 561 symm_kl=19.398, loss=4.725, nll_loss=0.786, ppl=1.72, wps=23720.1, ups=2.27, wpb=10458.4, bsz=369.9, num_updates=7400, lr=1.25e-05, gnorm=4.964, train_wall=44, wall=3648
2020-12-10 12:22:39 | INFO | train_inner | epoch 014:    207 / 561 symm_kl=19.238, loss=4.715, nll_loss=0.791, ppl=1.73, wps=23238.3, ups=2.21, wpb=10512.4, bsz=377.2, num_updates=7500, lr=1.25e-05, gnorm=4.98, train_wall=45, wall=3693
2020-12-10 12:23:24 | INFO | train_inner | epoch 014:    307 / 561 symm_kl=19.347, loss=4.714, nll_loss=0.781, ppl=1.72, wps=23202.7, ups=2.22, wpb=10452.7, bsz=366.4, num_updates=7600, lr=1.25e-05, gnorm=4.996, train_wall=45, wall=3739
2020-12-10 12:24:09 | INFO | train_inner | epoch 014:    407 / 561 symm_kl=18.816, loss=4.679, nll_loss=0.796, ppl=1.74, wps=23316.5, ups=2.22, wpb=10481.2, bsz=371.7, num_updates=7700, lr=1.25e-05, gnorm=4.742, train_wall=45, wall=3783
2020-12-10 12:24:53 | INFO | train_inner | epoch 014:    507 / 561 symm_kl=19.793, loss=4.778, nll_loss=0.803, ppl=1.74, wps=23365.6, ups=2.24, wpb=10434.9, bsz=367.4, num_updates=7800, lr=1.25e-05, gnorm=5.094, train_wall=44, wall=3828
2020-12-10 12:25:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:25:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:25:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:25:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:25:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:25:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:25:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:25:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:25:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:25:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:25:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:25:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:25:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:25:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:25:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:25:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:25:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:25:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:25:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:25:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:25:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:25:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:25:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:25:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:25:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:25:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:25:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:25:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:25:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:25:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:25:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:25:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:25:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:25:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:25:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:25:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:25:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:25:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:25:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:25:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:25:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:25:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:25:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:25:39 | INFO | valid | epoch 014 | valid on 'valid' subset | symm_kl 0 | loss 6.702 | nll_loss 4.813 | ppl 28.11 | bleu 21.27 | wps 4386 | wpb 7508.5 | bsz 272.7 | num_updates 7854 | best_bleu 21.47
2020-12-10 12:25:39 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:25:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:25:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:25:42 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 14 @ 7854 updates, score 21.27) (writing took 2.9051029961556196 seconds)
2020-12-10 12:25:42 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2020-12-10 12:25:42 | INFO | train | epoch 014 | symm_kl 19.371 | loss 4.726 | nll_loss 0.79 | ppl 1.73 | wps 21089.7 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 7854 | lr 1.25e-05 | gnorm 4.972 | train_wall 250 | wall 3877
2020-12-10 12:25:42 | INFO | fairseq.trainer | begin training epoch 15
2020-12-10 12:25:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:25:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:25:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:25:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:26:05 | INFO | train_inner | epoch 015:     46 / 561 symm_kl=19.302, loss=4.714, nll_loss=0.784, ppl=1.72, wps=14632.4, ups=1.39, wpb=10511.7, bsz=374.6, num_updates=7900, lr=1.25e-05, gnorm=5.013, train_wall=44, wall=3900
2020-12-10 12:26:50 | INFO | train_inner | epoch 015:    146 / 561 symm_kl=19.163, loss=4.708, nll_loss=0.793, ppl=1.73, wps=23154.2, ups=2.23, wpb=10393.8, bsz=368.2, num_updates=8000, lr=1.25e-05, gnorm=4.985, train_wall=45, wall=3945
2020-12-10 12:27:35 | INFO | train_inner | epoch 015:    246 / 561 symm_kl=19.794, loss=4.772, nll_loss=0.794, ppl=1.73, wps=23725.7, ups=2.25, wpb=10568.1, bsz=375.5, num_updates=8100, lr=1.25e-05, gnorm=5.19, train_wall=44, wall=3989
2020-12-10 12:28:20 | INFO | train_inner | epoch 015:    346 / 561 symm_kl=18.283, loss=4.612, nll_loss=0.783, ppl=1.72, wps=23313.2, ups=2.21, wpb=10531.1, bsz=382.6, num_updates=8200, lr=1.25e-05, gnorm=4.724, train_wall=45, wall=4035
2020-12-10 12:29:05 | INFO | train_inner | epoch 015:    446 / 561 symm_kl=19.207, loss=4.72, nll_loss=0.795, ppl=1.73, wps=23424.7, ups=2.23, wpb=10501.1, bsz=363.3, num_updates=8300, lr=1.25e-05, gnorm=4.903, train_wall=45, wall=4079
2020-12-10 12:29:49 | INFO | train_inner | epoch 015:    546 / 561 symm_kl=20.117, loss=4.814, nll_loss=0.804, ppl=1.75, wps=23568, ups=2.24, wpb=10518.8, bsz=353.3, num_updates=8400, lr=1.25e-05, gnorm=5.122, train_wall=44, wall=4124
2020-12-10 12:29:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:29:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:29:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:29:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:29:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:29:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:29:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:30:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:30:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:30:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:30:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:30:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:30:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:30:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:30:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:30:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:30:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:30:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:30:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:30:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:30:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:30:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:30:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:30:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:30:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:30:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:30:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:30:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:30:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:30:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:30:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:30:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:30:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:30:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:30:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:30:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:30:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:30:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:30:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:30:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:30:16 | INFO | valid | epoch 015 | valid on 'valid' subset | symm_kl 0 | loss 6.711 | nll_loss 4.821 | ppl 28.26 | bleu 21.22 | wps 4950.8 | wpb 7508.5 | bsz 272.7 | num_updates 8415 | best_bleu 21.47
2020-12-10 12:30:16 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:30:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:30:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:30:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:30:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:30:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 15 @ 8415 updates, score 21.22) (writing took 2.9368795845657587 seconds)
2020-12-10 12:30:18 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2020-12-10 12:30:18 | INFO | train | epoch 015 | symm_kl 19.239 | loss 4.718 | nll_loss 0.794 | ppl 1.73 | wps 21271.4 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 8415 | lr 1.25e-05 | gnorm 4.972 | train_wall 250 | wall 4153
2020-12-10 12:30:18 | INFO | fairseq.trainer | begin training epoch 16
2020-12-10 12:30:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:30:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:30:59 | INFO | train_inner | epoch 016:     85 / 561 symm_kl=19.055, loss=4.71, nll_loss=0.804, ppl=1.75, wps=14846.9, ups=1.43, wpb=10363.8, bsz=361.8, num_updates=8500, lr=1.25e-05, gnorm=4.926, train_wall=44, wall=4194
2020-12-10 12:31:44 | INFO | train_inner | epoch 016:    185 / 561 symm_kl=18.555, loss=4.63, nll_loss=0.772, ppl=1.71, wps=23459.3, ups=2.2, wpb=10661.1, bsz=385, num_updates=8600, lr=1.25e-05, gnorm=4.848, train_wall=45, wall=4239
2020-12-10 12:32:29 | INFO | train_inner | epoch 016:    285 / 561 symm_kl=19.677, loss=4.755, nll_loss=0.786, ppl=1.72, wps=23283.4, ups=2.23, wpb=10431.6, bsz=361.7, num_updates=8700, lr=1.25e-05, gnorm=5.152, train_wall=45, wall=4284
2020-12-10 12:33:14 | INFO | train_inner | epoch 016:    385 / 561 symm_kl=18.423, loss=4.639, nll_loss=0.794, ppl=1.73, wps=23588.7, ups=2.23, wpb=10567, bsz=376.6, num_updates=8800, lr=1.25e-05, gnorm=4.727, train_wall=45, wall=4329
2020-12-10 12:33:59 | INFO | train_inner | epoch 016:    485 / 561 symm_kl=18.89, loss=4.701, nll_loss=0.812, ppl=1.76, wps=23359.3, ups=2.24, wpb=10428.4, bsz=372.6, num_updates=8900, lr=1.25e-05, gnorm=4.908, train_wall=44, wall=4374
2020-12-10 12:34:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:34:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:34:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:34:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:34:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:34:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:34:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:34:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:34:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:34:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:34:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:34:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:34:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:34:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:34:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:34:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:34:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:34:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:34:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:34:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:34:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:34:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:34:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:34:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:34:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:34:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:34:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:34:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:34:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:34:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:34:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:34:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:34:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:34:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:34:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:34:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:34:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:34:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:34:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:34:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:34:53 | INFO | valid | epoch 016 | valid on 'valid' subset | symm_kl 0 | loss 6.709 | nll_loss 4.817 | ppl 28.2 | bleu 21.22 | wps 4828.6 | wpb 7508.5 | bsz 272.7 | num_updates 8976 | best_bleu 21.47
2020-12-10 12:34:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:34:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:34:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:34:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:34:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:34:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 16 @ 8976 updates, score 21.22) (writing took 2.8764185570180416 seconds)
2020-12-10 12:34:56 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2020-12-10 12:34:56 | INFO | train | epoch 016 | symm_kl 19.086 | loss 4.706 | nll_loss 0.796 | ppl 1.74 | wps 21200.2 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 8976 | lr 1.25e-05 | gnorm 4.959 | train_wall 250 | wall 4431
2020-12-10 12:34:56 | INFO | fairseq.trainer | begin training epoch 17
2020-12-10 12:34:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:34:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:35:09 | INFO | train_inner | epoch 017:     24 / 561 symm_kl=19.709, loss=4.779, nll_loss=0.808, ppl=1.75, wps=14689.3, ups=1.42, wpb=10375.4, bsz=355.4, num_updates=9000, lr=1.25e-05, gnorm=5.172, train_wall=44, wall=4444
2020-12-10 12:35:54 | INFO | train_inner | epoch 017:    124 / 561 symm_kl=19.411, loss=4.742, nll_loss=0.799, ppl=1.74, wps=23600.6, ups=2.25, wpb=10486.4, bsz=375.2, num_updates=9100, lr=1.25e-05, gnorm=5.073, train_wall=44, wall=4489
2020-12-10 12:36:39 | INFO | train_inner | epoch 017:    224 / 561 symm_kl=19.502, loss=4.749, nll_loss=0.796, ppl=1.74, wps=23246.8, ups=2.21, wpb=10524.4, bsz=351.8, num_updates=9200, lr=1.25e-05, gnorm=5.018, train_wall=45, wall=4534
2020-12-10 12:37:24 | INFO | train_inner | epoch 017:    324 / 561 symm_kl=18.627, loss=4.663, nll_loss=0.798, ppl=1.74, wps=23072.5, ups=2.22, wpb=10393.9, bsz=381, num_updates=9300, lr=1.25e-05, gnorm=4.9, train_wall=45, wall=4579
2020-12-10 12:38:09 | INFO | train_inner | epoch 017:    424 / 561 symm_kl=17.986, loss=4.602, nll_loss=0.8, ppl=1.74, wps=23396, ups=2.24, wpb=10458.2, bsz=376.9, num_updates=9400, lr=1.25e-05, gnorm=4.614, train_wall=45, wall=4624
2020-12-10 12:38:54 | INFO | train_inner | epoch 017:    524 / 561 symm_kl=19.152, loss=4.719, nll_loss=0.801, ppl=1.74, wps=23538.5, ups=2.22, wpb=10584.2, bsz=369, num_updates=9500, lr=1.25e-05, gnorm=4.885, train_wall=45, wall=4669
2020-12-10 12:39:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:39:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:39:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:39:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:39:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:39:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:39:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:39:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:39:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:39:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:39:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:39:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:39:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:39:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:39:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:39:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:39:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:39:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:39:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:39:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:39:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:39:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:39:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:39:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:39:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:39:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:39:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:39:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:39:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:39:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:39:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:39:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:39:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:39:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:39:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:39:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:39:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:39:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:39:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:39:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:39:30 | INFO | valid | epoch 017 | valid on 'valid' subset | symm_kl 0 | loss 6.713 | nll_loss 4.823 | ppl 28.3 | bleu 21.1 | wps 5028.2 | wpb 7508.5 | bsz 272.7 | num_updates 9537 | best_bleu 21.47
2020-12-10 12:39:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:39:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:39:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:39:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:39:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:39:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 17 @ 9537 updates, score 21.1) (writing took 2.8824085742235184 seconds)
2020-12-10 12:39:33 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2020-12-10 12:39:33 | INFO | train | epoch 017 | symm_kl 18.954 | loss 4.697 | nll_loss 0.799 | ppl 1.74 | wps 21251.8 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 9537 | lr 1.25e-05 | gnorm 4.929 | train_wall 250 | wall 4707
2020-12-10 12:39:33 | INFO | fairseq.trainer | begin training epoch 18
2020-12-10 12:39:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:39:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:40:03 | INFO | train_inner | epoch 018:     63 / 561 symm_kl=18.871, loss=4.692, nll_loss=0.802, ppl=1.74, wps=14902.9, ups=1.44, wpb=10355.4, bsz=367.2, num_updates=9600, lr=1.25e-05, gnorm=5.051, train_wall=44, wall=4738
2020-12-10 12:40:48 | INFO | train_inner | epoch 018:    163 / 561 symm_kl=18.005, loss=4.6, nll_loss=0.798, ppl=1.74, wps=23603.7, ups=2.22, wpb=10615, bsz=379.8, num_updates=9700, lr=1.25e-05, gnorm=4.72, train_wall=45, wall=4783
2020-12-10 12:41:33 | INFO | train_inner | epoch 018:    263 / 561 symm_kl=18.621, loss=4.662, nll_loss=0.794, ppl=1.73, wps=23343.5, ups=2.22, wpb=10534.5, bsz=372.8, num_updates=9800, lr=1.25e-05, gnorm=4.791, train_wall=45, wall=4828
2020-12-10 12:42:18 | INFO | train_inner | epoch 018:    363 / 561 symm_kl=19.56, loss=4.766, nll_loss=0.805, ppl=1.75, wps=23255.9, ups=2.22, wpb=10480.4, bsz=361.7, num_updates=9900, lr=1.25e-05, gnorm=5.071, train_wall=45, wall=4873
2020-12-10 12:43:03 | INFO | train_inner | epoch 018:    463 / 561 symm_kl=19.403, loss=4.749, nll_loss=0.805, ppl=1.75, wps=23154, ups=2.22, wpb=10435.8, bsz=363.1, num_updates=10000, lr=1.25e-05, gnorm=5.105, train_wall=45, wall=4918
2020-12-10 12:43:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:43:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:43:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:43:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:43:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:43:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:43:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:43:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:43:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:43:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:43:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:43:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:43:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:43:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:43:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:43:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:43:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:43:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:43:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:43:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:43:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:43:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:43:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:43:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:43:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:43:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:43:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:43:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:43:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:43:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:43:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:43:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:43:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:43:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:43:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:43:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:43:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:44:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:44:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:44:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:44:08 | INFO | valid | epoch 018 | valid on 'valid' subset | symm_kl 0 | loss 6.709 | nll_loss 4.821 | ppl 28.26 | bleu 21.27 | wps 4740.5 | wpb 7508.5 | bsz 272.7 | num_updates 10098 | best_bleu 21.47
2020-12-10 12:44:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:44:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:44:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:44:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:44:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:44:11 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 18 @ 10098 updates, score 21.27) (writing took 2.9438140336424112 seconds)
2020-12-10 12:44:11 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2020-12-10 12:44:11 | INFO | train | epoch 018 | symm_kl 18.798 | loss 4.684 | nll_loss 0.801 | ppl 1.74 | wps 21163.2 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 10098 | lr 1.25e-05 | gnorm 4.914 | train_wall 250 | wall 4985
2020-12-10 12:44:11 | INFO | fairseq.trainer | begin training epoch 19
2020-12-10 12:44:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:44:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:44:15 | INFO | train_inner | epoch 019:      2 / 561 symm_kl=18.344, loss=4.638, nll_loss=0.803, ppl=1.74, wps=14723.2, ups=1.41, wpb=10459.9, bsz=372.6, num_updates=10100, lr=1.25e-05, gnorm=4.861, train_wall=44, wall=4989
2020-12-10 12:44:59 | INFO | train_inner | epoch 019:    102 / 561 symm_kl=18.917, loss=4.695, nll_loss=0.8, ppl=1.74, wps=24032, ups=2.27, wpb=10587.8, bsz=369.6, num_updates=10200, lr=1.25e-05, gnorm=4.937, train_wall=44, wall=5033
2020-12-10 12:45:44 | INFO | train_inner | epoch 019:    202 / 561 symm_kl=18.889, loss=4.708, nll_loss=0.815, ppl=1.76, wps=23143, ups=2.21, wpb=10452.4, bsz=363.9, num_updates=10300, lr=1.25e-05, gnorm=4.967, train_wall=45, wall=5079
2020-12-10 12:46:29 | INFO | train_inner | epoch 019:    302 / 561 symm_kl=18.146, loss=4.613, nll_loss=0.791, ppl=1.73, wps=23375.7, ups=2.23, wpb=10492.3, bsz=370.6, num_updates=10400, lr=1.25e-05, gnorm=4.76, train_wall=45, wall=5123
2020-12-10 12:47:14 | INFO | train_inner | epoch 019:    402 / 561 symm_kl=18.676, loss=4.681, nll_loss=0.811, ppl=1.75, wps=23605.5, ups=2.23, wpb=10598.7, bsz=372.4, num_updates=10500, lr=1.25e-05, gnorm=4.83, train_wall=45, wall=5168
2020-12-10 12:47:58 | INFO | train_inner | epoch 019:    502 / 561 symm_kl=18.451, loss=4.649, nll_loss=0.801, ppl=1.74, wps=23159.6, ups=2.23, wpb=10384, bsz=378.9, num_updates=10600, lr=1.25e-05, gnorm=4.949, train_wall=45, wall=5213
2020-12-10 12:48:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:48:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:48:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:48:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:48:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:48:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:48:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:48:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:48:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:48:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:48:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:48:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:48:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:48:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:48:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:48:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:48:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:48:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:48:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:48:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:48:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:48:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:48:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:48:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:48:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:48:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:48:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:48:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:48:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:48:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:48:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:48:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:48:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:48:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:48:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:48:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:48:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:48:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:48:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:48:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:48:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:48:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:48:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:48:45 | INFO | valid | epoch 019 | valid on 'valid' subset | symm_kl 0 | loss 6.714 | nll_loss 4.825 | ppl 28.35 | bleu 21.11 | wps 4750.7 | wpb 7508.5 | bsz 272.7 | num_updates 10659 | best_bleu 21.47
2020-12-10 12:48:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:48:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:48:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:48:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:48:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:48:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 19 @ 10659 updates, score 21.11) (writing took 2.825278652831912 seconds)
2020-12-10 12:48:48 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2020-12-10 12:48:48 | INFO | train | epoch 019 | symm_kl 18.663 | loss 4.674 | nll_loss 0.803 | ppl 1.75 | wps 21199.8 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 10659 | lr 1.25e-05 | gnorm 4.91 | train_wall 250 | wall 5263
2020-12-10 12:48:48 | INFO | fairseq.trainer | begin training epoch 20
2020-12-10 12:48:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:48:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:49:09 | INFO | train_inner | epoch 020:     41 / 561 symm_kl=19.087, loss=4.721, nll_loss=0.811, ppl=1.75, wps=14605.4, ups=1.42, wpb=10295.5, bsz=357.8, num_updates=10700, lr=1.25e-05, gnorm=5.102, train_wall=44, wall=5284
2020-12-10 12:49:53 | INFO | train_inner | epoch 020:    141 / 561 symm_kl=18.235, loss=4.634, nll_loss=0.806, ppl=1.75, wps=23462, ups=2.25, wpb=10430.1, bsz=376.6, num_updates=10800, lr=1.25e-05, gnorm=4.826, train_wall=44, wall=5328
2020-12-10 12:50:39 | INFO | train_inner | epoch 020:    241 / 561 symm_kl=19.752, loss=4.78, nll_loss=0.803, ppl=1.74, wps=23167.1, ups=2.2, wpb=10508.8, bsz=360.1, num_updates=10900, lr=1.25e-05, gnorm=5.36, train_wall=45, wall=5374
2020-12-10 12:51:23 | INFO | train_inner | epoch 020:    341 / 561 symm_kl=17.376, loss=4.534, nll_loss=0.791, ppl=1.73, wps=23760.1, ups=2.24, wpb=10591.2, bsz=386.8, num_updates=11000, lr=1.25e-05, gnorm=4.557, train_wall=44, wall=5418
2020-12-10 12:52:08 | INFO | train_inner | epoch 020:    441 / 561 symm_kl=18.749, loss=4.695, nll_loss=0.815, ppl=1.76, wps=23404.6, ups=2.23, wpb=10508.4, bsz=363.5, num_updates=11100, lr=1.25e-05, gnorm=4.889, train_wall=45, wall=5463
2020-12-10 12:52:53 | INFO | train_inner | epoch 020:    541 / 561 symm_kl=18.639, loss=4.672, nll_loss=0.803, ppl=1.74, wps=23537.7, ups=2.23, wpb=10568.1, bsz=373, num_updates=11200, lr=1.25e-05, gnorm=4.953, train_wall=45, wall=5508
2020-12-10 12:53:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:53:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:53:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:53:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:53:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:53:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:53:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:53:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:53:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:53:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:53:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:53:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:53:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:53:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:53:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:53:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:53:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:53:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:53:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:53:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:53:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:53:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:53:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:53:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:53:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:53:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:53:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:53:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:53:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:53:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:53:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:53:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:53:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:53:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:53:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:53:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:53:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:53:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:53:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:53:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:53:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:53:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:53:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:53:24 | INFO | valid | epoch 020 | valid on 'valid' subset | symm_kl 0 | loss 6.721 | nll_loss 4.831 | ppl 28.46 | bleu 21.14 | wps 4255.5 | wpb 7508.5 | bsz 272.7 | num_updates 11220 | best_bleu 21.47
2020-12-10 12:53:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:53:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:53:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:53:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:53:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:53:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 20 @ 11220 updates, score 21.14) (writing took 2.858954105526209 seconds)
2020-12-10 12:53:27 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2020-12-10 12:53:27 | INFO | train | epoch 020 | symm_kl 18.584 | loss 4.668 | nll_loss 0.806 | ppl 1.75 | wps 21097.3 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 11220 | lr 1.25e-05 | gnorm 4.928 | train_wall 250 | wall 5542
2020-12-10 12:53:27 | INFO | fairseq.trainer | begin training epoch 21
2020-12-10 12:53:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:53:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:54:05 | INFO | train_inner | epoch 021:     80 / 561 symm_kl=18.473, loss=4.663, nll_loss=0.809, ppl=1.75, wps=14478.2, ups=1.39, wpb=10444.4, bsz=358.2, num_updates=11300, lr=1.25e-05, gnorm=4.788, train_wall=44, wall=5580
2020-12-10 12:54:50 | INFO | train_inner | epoch 021:    180 / 561 symm_kl=17.44, loss=4.544, nll_loss=0.795, ppl=1.73, wps=23455.3, ups=2.22, wpb=10583.9, bsz=387.3, num_updates=11400, lr=1.25e-05, gnorm=4.624, train_wall=45, wall=5625
2020-12-10 12:55:35 | INFO | train_inner | epoch 021:    280 / 561 symm_kl=19.546, loss=4.776, nll_loss=0.818, ppl=1.76, wps=23159.6, ups=2.23, wpb=10403.5, bsz=362.5, num_updates=11500, lr=1.25e-05, gnorm=5.191, train_wall=45, wall=5670
2020-12-10 12:56:20 | INFO | train_inner | epoch 021:    380 / 561 symm_kl=18.502, loss=4.659, nll_loss=0.801, ppl=1.74, wps=23268.5, ups=2.22, wpb=10478.5, bsz=362.1, num_updates=11600, lr=1.25e-05, gnorm=4.904, train_wall=45, wall=5715
2020-12-10 12:57:05 | INFO | train_inner | epoch 021:    480 / 561 symm_kl=17.847, loss=4.602, nll_loss=0.81, ppl=1.75, wps=23620.7, ups=2.24, wpb=10523.6, bsz=374.4, num_updates=11700, lr=1.25e-05, gnorm=4.723, train_wall=44, wall=5760
2020-12-10 12:57:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 12:57:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:57:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:57:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:57:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:57:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:57:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:57:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:57:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:57:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:57:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:57:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:57:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:57:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:57:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:57:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:57:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:57:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:57:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:57:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:57:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:57:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:57:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:57:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:57:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:57:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:57:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:57:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:57:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:57:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:57:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:57:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:57:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:57:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:57:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:57:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:57:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:57:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:57:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:57:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:57:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 12:57:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 12:57:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 12:58:01 | INFO | valid | epoch 021 | valid on 'valid' subset | symm_kl 0 | loss 6.72 | nll_loss 4.83 | ppl 28.44 | bleu 21.05 | wps 5060.4 | wpb 7508.5 | bsz 272.7 | num_updates 11781 | best_bleu 21.47
2020-12-10 12:58:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 12:58:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:58:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:58:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:58:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:58:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 21 @ 11781 updates, score 21.05) (writing took 2.8392697870731354 seconds)
2020-12-10 12:58:03 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2020-12-10 12:58:03 | INFO | train | epoch 021 | symm_kl 18.429 | loss 4.656 | nll_loss 0.808 | ppl 1.75 | wps 21254.2 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 11781 | lr 1.25e-05 | gnorm 4.874 | train_wall 250 | wall 5818
2020-12-10 12:58:03 | INFO | fairseq.trainer | begin training epoch 22
2020-12-10 12:58:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 12:58:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 12:58:15 | INFO | train_inner | epoch 022:     19 / 561 symm_kl=19.245, loss=4.748, nll_loss=0.82, ppl=1.77, wps=14819.6, ups=1.43, wpb=10370.6, bsz=357.7, num_updates=11800, lr=1.25e-05, gnorm=5.114, train_wall=44, wall=5830
2020-12-10 12:58:59 | INFO | train_inner | epoch 022:    119 / 561 symm_kl=18.664, loss=4.695, nll_loss=0.819, ppl=1.76, wps=23685.8, ups=2.25, wpb=10516.9, bsz=360.6, num_updates=11900, lr=1.25e-05, gnorm=4.897, train_wall=44, wall=5874
2020-12-10 12:59:45 | INFO | train_inner | epoch 022:    219 / 561 symm_kl=18.495, loss=4.664, nll_loss=0.808, ppl=1.75, wps=22908.1, ups=2.2, wpb=10399.7, bsz=365.9, num_updates=12000, lr=1.25e-05, gnorm=4.964, train_wall=45, wall=5919
2020-12-10 13:00:30 | INFO | train_inner | epoch 022:    319 / 561 symm_kl=18.967, loss=4.71, nll_loss=0.812, ppl=1.76, wps=23290.8, ups=2.21, wpb=10543, bsz=360.8, num_updates=12100, lr=1.25e-05, gnorm=5.01, train_wall=45, wall=5965
2020-12-10 13:01:15 | INFO | train_inner | epoch 022:    419 / 561 symm_kl=17.965, loss=4.608, nll_loss=0.805, ppl=1.75, wps=23376.5, ups=2.22, wpb=10537.4, bsz=380.4, num_updates=12200, lr=1.25e-05, gnorm=4.856, train_wall=45, wall=6010
2020-12-10 13:02:00 | INFO | train_inner | epoch 022:    519 / 561 symm_kl=18.087, loss=4.626, nll_loss=0.811, ppl=1.75, wps=23309.8, ups=2.23, wpb=10442.1, bsz=373.6, num_updates=12300, lr=1.25e-05, gnorm=4.795, train_wall=45, wall=6055
2020-12-10 13:02:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:02:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:02:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:02:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:02:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:02:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:02:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:02:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:02:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:02:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:02:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:02:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:02:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:02:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:02:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:02:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:02:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:02:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:02:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:02:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:02:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:02:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:02:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:02:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:02:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:02:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:02:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:02:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:02:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:02:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:02:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:02:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:02:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:02:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:02:41 | INFO | valid | epoch 022 | valid on 'valid' subset | symm_kl 0 | loss 6.733 | nll_loss 4.842 | ppl 28.69 | bleu 21.24 | wps 4186.3 | wpb 7508.5 | bsz 272.7 | num_updates 12342 | best_bleu 21.47
2020-12-10 13:02:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:02:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:02:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:02:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:02:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:02:44 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 22 @ 12342 updates, score 21.24) (writing took 2.855034040287137 seconds)
2020-12-10 13:02:44 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2020-12-10 13:02:44 | INFO | train | epoch 022 | symm_kl 18.348 | loss 4.651 | nll_loss 0.81 | ppl 1.75 | wps 20965.2 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 12342 | lr 1.25e-05 | gnorm 4.879 | train_wall 251 | wall 6099
2020-12-10 13:02:44 | INFO | fairseq.trainer | begin training epoch 23
2020-12-10 13:02:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:02:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:03:12 | INFO | train_inner | epoch 023:     58 / 561 symm_kl=17.189, loss=4.525, nll_loss=0.797, ppl=1.74, wps=14563.1, ups=1.38, wpb=10577.4, bsz=388, num_updates=12400, lr=1.25e-05, gnorm=4.51, train_wall=44, wall=6127
2020-12-10 13:03:58 | INFO | train_inner | epoch 023:    158 / 561 symm_kl=18.815, loss=4.708, nll_loss=0.819, ppl=1.76, wps=23212.3, ups=2.21, wpb=10480.4, bsz=362.8, num_updates=12500, lr=1.25e-05, gnorm=4.968, train_wall=45, wall=6172
2020-12-10 13:04:42 | INFO | train_inner | epoch 023:    258 / 561 symm_kl=18.262, loss=4.644, nll_loss=0.809, ppl=1.75, wps=23218, ups=2.23, wpb=10417.1, bsz=366.9, num_updates=12600, lr=1.25e-05, gnorm=4.848, train_wall=45, wall=6217
2020-12-10 13:05:27 | INFO | train_inner | epoch 023:    358 / 561 symm_kl=18.434, loss=4.665, nll_loss=0.814, ppl=1.76, wps=23492.5, ups=2.24, wpb=10490.5, bsz=366.9, num_updates=12700, lr=1.25e-05, gnorm=4.899, train_wall=44, wall=6262
2020-12-10 13:06:12 | INFO | train_inner | epoch 023:    458 / 561 symm_kl=17.974, loss=4.621, nll_loss=0.819, ppl=1.76, wps=23074.1, ups=2.24, wpb=10289, bsz=369.8, num_updates=12800, lr=1.25e-05, gnorm=4.801, train_wall=44, wall=6306
2020-12-10 13:06:57 | INFO | train_inner | epoch 023:    558 / 561 symm_kl=18.014, loss=4.612, nll_loss=0.806, ppl=1.75, wps=23698.5, ups=2.22, wpb=10675.9, bsz=377.6, num_updates=12900, lr=1.25e-05, gnorm=4.805, train_wall=45, wall=6352
2020-12-10 13:06:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:06:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:06:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:06:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:07:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:07:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:07:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:07:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:07:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:07:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:07:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:07:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:07:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:07:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:07:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:07:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:07:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:07:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:07:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:07:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:07:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:07:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:07:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:07:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:07:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:07:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:07:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:07:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:07:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:07:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:07:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:07:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:07:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:07:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:07:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:07:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:07:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:07:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:07:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:07:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:07:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:07:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:07:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:07:17 | INFO | valid | epoch 023 | valid on 'valid' subset | symm_kl 0 | loss 6.729 | nll_loss 4.841 | ppl 28.66 | bleu 21.23 | wps 5000.8 | wpb 7508.5 | bsz 272.7 | num_updates 12903 | best_bleu 21.47
2020-12-10 13:07:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:07:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:07:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:07:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 23 @ 12903 updates, score 21.23) (writing took 2.348990634083748 seconds)
2020-12-10 13:07:20 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2020-12-10 13:07:20 | INFO | train | epoch 023 | symm_kl 18.216 | loss 4.64 | nll_loss 0.811 | ppl 1.75 | wps 21316.3 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 12903 | lr 1.25e-05 | gnorm 4.837 | train_wall 250 | wall 6375
2020-12-10 13:07:20 | INFO | fairseq.trainer | begin training epoch 24
2020-12-10 13:07:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:07:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:07:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:07:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:08:06 | INFO | train_inner | epoch 024:     97 / 561 symm_kl=18.898, loss=4.708, nll_loss=0.81, ppl=1.75, wps=15114.7, ups=1.44, wpb=10474, bsz=357.4, num_updates=13000, lr=1.25e-05, gnorm=5.106, train_wall=44, wall=6421
2020-12-10 13:08:50 | INFO | train_inner | epoch 024:    197 / 561 symm_kl=17.767, loss=4.602, nll_loss=0.817, ppl=1.76, wps=23527.6, ups=2.25, wpb=10460, bsz=366.7, num_updates=13100, lr=1.25e-05, gnorm=4.687, train_wall=44, wall=6465
2020-12-10 13:09:35 | INFO | train_inner | epoch 024:    297 / 561 symm_kl=17.752, loss=4.585, nll_loss=0.804, ppl=1.75, wps=23209.1, ups=2.22, wpb=10444.7, bsz=378.2, num_updates=13200, lr=1.25e-05, gnorm=4.861, train_wall=45, wall=6510
2020-12-10 13:10:21 | INFO | train_inner | epoch 024:    397 / 561 symm_kl=18.066, loss=4.639, nll_loss=0.824, ppl=1.77, wps=23552.6, ups=2.22, wpb=10616, bsz=368.1, num_updates=13300, lr=1.25e-05, gnorm=4.728, train_wall=45, wall=6555
2020-12-10 13:11:05 | INFO | train_inner | epoch 024:    497 / 561 symm_kl=17.646, loss=4.58, nll_loss=0.807, ppl=1.75, wps=23564.3, ups=2.24, wpb=10529.5, bsz=375.7, num_updates=13400, lr=1.25e-05, gnorm=4.663, train_wall=45, wall=6600
2020-12-10 13:11:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:11:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:11:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:11:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:11:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:11:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:11:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:11:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:11:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:11:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:11:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:11:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:11:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:11:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:11:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:11:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:11:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:11:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:11:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:11:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:11:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:11:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:11:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:11:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:11:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:11:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:11:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:11:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:11:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:11:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:11:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:11:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:11:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:11:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:11:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:11:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:11:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:11:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:11:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:11:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:11:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:11:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:11:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:11:56 | INFO | valid | epoch 024 | valid on 'valid' subset | symm_kl 0 | loss 6.738 | nll_loss 4.846 | ppl 28.76 | bleu 21.16 | wps 4255.2 | wpb 7508.5 | bsz 272.7 | num_updates 13464 | best_bleu 21.47
2020-12-10 13:11:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:11:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:11:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:11:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:11:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:11:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 24 @ 13464 updates, score 21.16) (writing took 2.909321526065469 seconds)
2020-12-10 13:11:59 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2020-12-10 13:11:59 | INFO | train | epoch 024 | symm_kl 18.132 | loss 4.633 | nll_loss 0.813 | ppl 1.76 | wps 21081.1 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 13464 | lr 1.25e-05 | gnorm 4.857 | train_wall 250 | wall 6654
2020-12-10 13:11:59 | INFO | fairseq.trainer | begin training epoch 25
2020-12-10 13:12:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:12:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:12:18 | INFO | train_inner | epoch 025:     36 / 561 symm_kl=17.75, loss=4.582, nll_loss=0.803, ppl=1.74, wps=14226.5, ups=1.38, wpb=10289.6, bsz=381.2, num_updates=13500, lr=1.25e-05, gnorm=4.881, train_wall=44, wall=6672
2020-12-10 13:13:02 | INFO | train_inner | epoch 025:    136 / 561 symm_kl=18.05, loss=4.624, nll_loss=0.813, ppl=1.76, wps=23521.3, ups=2.25, wpb=10467.1, bsz=371.7, num_updates=13600, lr=1.25e-05, gnorm=4.924, train_wall=44, wall=6717
2020-12-10 13:13:47 | INFO | train_inner | epoch 025:    236 / 561 symm_kl=17.685, loss=4.586, nll_loss=0.81, ppl=1.75, wps=23557.5, ups=2.22, wpb=10603.7, bsz=386.2, num_updates=13700, lr=1.25e-05, gnorm=4.826, train_wall=45, wall=6762
2020-12-10 13:14:32 | INFO | train_inner | epoch 025:    336 / 561 symm_kl=18.112, loss=4.638, nll_loss=0.82, ppl=1.77, wps=23483.7, ups=2.24, wpb=10495.4, bsz=371.7, num_updates=13800, lr=1.25e-05, gnorm=4.841, train_wall=45, wall=6807
2020-12-10 13:15:17 | INFO | train_inner | epoch 025:    436 / 561 symm_kl=18.769, loss=4.704, nll_loss=0.818, ppl=1.76, wps=23481.2, ups=2.22, wpb=10555.1, bsz=355.2, num_updates=13900, lr=1.25e-05, gnorm=5.012, train_wall=45, wall=6852
2020-12-10 13:16:01 | INFO | train_inner | epoch 025:    536 / 561 symm_kl=17.705, loss=4.602, nll_loss=0.824, ppl=1.77, wps=23498.5, ups=2.24, wpb=10481.5, bsz=369.8, num_updates=14000, lr=1.25e-05, gnorm=4.697, train_wall=44, wall=6896
2020-12-10 13:16:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:16:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:16:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:16:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:16:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:16:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:16:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:16:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:16:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:16:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:16:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:16:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:16:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:16:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:16:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:16:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:16:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:16:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:16:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:16:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:16:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:16:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:16:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:16:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:16:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:16:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:16:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:16:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:16:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:16:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:16:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:16:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:16:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:16:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:16:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:16:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:16:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:16:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:16:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:16:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:16:32 | INFO | valid | epoch 025 | valid on 'valid' subset | symm_kl 0 | loss 6.732 | nll_loss 4.842 | ppl 28.67 | bleu 21.15 | wps 4765.5 | wpb 7508.5 | bsz 272.7 | num_updates 14025 | best_bleu 21.47
2020-12-10 13:16:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:16:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:16:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:16:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:16:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:16:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 25 @ 14025 updates, score 21.15) (writing took 2.8702325467020273 seconds)
2020-12-10 13:16:35 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2020-12-10 13:16:35 | INFO | train | epoch 025 | symm_kl 18.038 | loss 4.627 | nll_loss 0.815 | ppl 1.76 | wps 21273.1 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 14025 | lr 1.25e-05 | gnorm 4.859 | train_wall 249 | wall 6930
2020-12-10 13:16:35 | INFO | fairseq.trainer | begin training epoch 26
2020-12-10 13:16:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:16:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:17:11 | INFO | train_inner | epoch 026:     75 / 561 symm_kl=18.343, loss=4.671, nll_loss=0.829, ppl=1.78, wps=14798.8, ups=1.42, wpb=10387.2, bsz=354.5, num_updates=14100, lr=1.25e-05, gnorm=4.897, train_wall=44, wall=6966
2020-12-10 13:17:57 | INFO | train_inner | epoch 026:    175 / 561 symm_kl=19.177, loss=4.75, nll_loss=0.825, ppl=1.77, wps=23331.3, ups=2.22, wpb=10521.3, bsz=357.7, num_updates=14200, lr=1.25e-05, gnorm=5.163, train_wall=45, wall=7011
2020-12-10 13:18:41 | INFO | train_inner | epoch 026:    275 / 561 symm_kl=17.241, loss=4.547, nll_loss=0.815, ppl=1.76, wps=23280.1, ups=2.23, wpb=10449, bsz=379.6, num_updates=14300, lr=1.25e-05, gnorm=4.66, train_wall=45, wall=7056
2020-12-10 13:19:27 | INFO | train_inner | epoch 026:    375 / 561 symm_kl=17.48, loss=4.563, nll_loss=0.805, ppl=1.75, wps=23516.9, ups=2.22, wpb=10595.2, bsz=373.6, num_updates=14400, lr=1.25e-05, gnorm=4.692, train_wall=45, wall=7101
2020-12-10 13:20:12 | INFO | train_inner | epoch 026:    475 / 561 symm_kl=17.805, loss=4.603, nll_loss=0.815, ppl=1.76, wps=23171.3, ups=2.22, wpb=10428.6, bsz=374.3, num_updates=14500, lr=1.25e-05, gnorm=4.87, train_wall=45, wall=7146
2020-12-10 13:20:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:20:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:20:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:20:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:20:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:20:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:20:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:20:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:20:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:20:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:20:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:20:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:20:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:20:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:20:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:20:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:20:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:20:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:20:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:20:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:20:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:20:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:20:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:20:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:20:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:20:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:20:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:20:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:20:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:20:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:20:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:20:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:20:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:20:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:21:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:21:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:21:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:21:10 | INFO | valid | epoch 026 | valid on 'valid' subset | symm_kl 0 | loss 6.741 | nll_loss 4.851 | ppl 28.85 | bleu 20.97 | wps 4905 | wpb 7508.5 | bsz 272.7 | num_updates 14586 | best_bleu 21.47
2020-12-10 13:21:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:21:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:21:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:21:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 26 @ 14586 updates, score 20.97) (writing took 2.8263378217816353 seconds)
2020-12-10 13:21:13 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2020-12-10 13:21:13 | INFO | train | epoch 026 | symm_kl 17.946 | loss 4.621 | nll_loss 0.818 | ppl 1.76 | wps 21201 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 14586 | lr 1.25e-05 | gnorm 4.867 | train_wall 251 | wall 7208
2020-12-10 13:21:13 | INFO | fairseq.trainer | begin training epoch 27
2020-12-10 13:21:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:21:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:21:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:21:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:21:22 | INFO | train_inner | epoch 027:     14 / 561 symm_kl=17.813, loss=4.606, nll_loss=0.817, ppl=1.76, wps=14714.9, ups=1.42, wpb=10345.5, bsz=365.7, num_updates=14600, lr=1.25e-05, gnorm=4.986, train_wall=44, wall=7217
2020-12-10 13:22:06 | INFO | train_inner | epoch 027:    114 / 561 symm_kl=18.199, loss=4.651, nll_loss=0.82, ppl=1.77, wps=23544, ups=2.27, wpb=10391.1, bsz=359, num_updates=14700, lr=1.25e-05, gnorm=4.923, train_wall=44, wall=7261
2020-12-10 13:22:51 | INFO | train_inner | epoch 027:    214 / 561 symm_kl=17.97, loss=4.618, nll_loss=0.812, ppl=1.76, wps=23443.1, ups=2.22, wpb=10581.4, bsz=366.7, num_updates=14800, lr=1.25e-05, gnorm=4.883, train_wall=45, wall=7306
2020-12-10 13:23:36 | INFO | train_inner | epoch 027:    314 / 561 symm_kl=18.006, loss=4.631, nll_loss=0.819, ppl=1.76, wps=23308.5, ups=2.22, wpb=10485.5, bsz=367.3, num_updates=14900, lr=1.25e-05, gnorm=4.841, train_wall=45, wall=7351
2020-12-10 13:24:21 | INFO | train_inner | epoch 027:    414 / 561 symm_kl=18.291, loss=4.674, nll_loss=0.84, ppl=1.79, wps=23432.1, ups=2.23, wpb=10519.6, bsz=366.3, num_updates=15000, lr=1.25e-05, gnorm=4.854, train_wall=45, wall=7396
2020-12-10 13:25:06 | INFO | train_inner | epoch 027:    514 / 561 symm_kl=17, loss=4.52, nll_loss=0.813, ppl=1.76, wps=23376.7, ups=2.23, wpb=10495.8, bsz=383.7, num_updates=15100, lr=1.25e-05, gnorm=4.674, train_wall=45, wall=7441
2020-12-10 13:25:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:25:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:25:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:25:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:25:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:25:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:25:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:25:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:25:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:25:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:25:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:25:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:25:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:25:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:25:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:25:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:25:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:25:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:25:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:25:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:25:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:25:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:25:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:25:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:25:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:25:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:25:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:25:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:25:48 | INFO | valid | epoch 027 | valid on 'valid' subset | symm_kl 0 | loss 6.735 | nll_loss 4.845 | ppl 28.74 | bleu 21.15 | wps 4609.7 | wpb 7508.5 | bsz 272.7 | num_updates 15147 | best_bleu 21.47
2020-12-10 13:25:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:25:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:25:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:25:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 27 @ 15147 updates, score 21.15) (writing took 2.341989044100046 seconds)
2020-12-10 13:25:50 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2020-12-10 13:25:50 | INFO | train | epoch 027 | symm_kl 17.827 | loss 4.611 | nll_loss 0.82 | ppl 1.77 | wps 21206.3 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 15147 | lr 1.25e-05 | gnorm 4.823 | train_wall 250 | wall 7485
2020-12-10 13:25:50 | INFO | fairseq.trainer | begin training epoch 28
2020-12-10 13:25:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:25:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:25:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:25:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:26:16 | INFO | train_inner | epoch 028:     53 / 561 symm_kl=18.127, loss=4.647, nll_loss=0.821, ppl=1.77, wps=14853.1, ups=1.42, wpb=10433.6, bsz=361.8, num_updates=15200, lr=1.25e-05, gnorm=4.969, train_wall=44, wall=7511
2020-12-10 13:27:01 | INFO | train_inner | epoch 028:    153 / 561 symm_kl=17.021, loss=4.533, nll_loss=0.823, ppl=1.77, wps=23820, ups=2.23, wpb=10688.7, bsz=379.7, num_updates=15300, lr=1.25e-05, gnorm=4.587, train_wall=45, wall=7556
2020-12-10 13:27:46 | INFO | train_inner | epoch 028:    253 / 561 symm_kl=18.335, loss=4.667, nll_loss=0.821, ppl=1.77, wps=23593.5, ups=2.24, wpb=10532.3, bsz=363.4, num_updates=15400, lr=1.25e-05, gnorm=4.966, train_wall=44, wall=7600
2020-12-10 13:28:30 | INFO | train_inner | epoch 028:    353 / 561 symm_kl=17.483, loss=4.578, nll_loss=0.823, ppl=1.77, wps=23291.5, ups=2.24, wpb=10399.2, bsz=375.9, num_updates=15500, lr=1.25e-05, gnorm=4.72, train_wall=44, wall=7645
2020-12-10 13:29:15 | INFO | train_inner | epoch 028:    453 / 561 symm_kl=17.834, loss=4.612, nll_loss=0.821, ppl=1.77, wps=23054.9, ups=2.22, wpb=10372, bsz=366.6, num_updates=15600, lr=1.25e-05, gnorm=4.917, train_wall=45, wall=7690
2020-12-10 13:30:00 | INFO | train_inner | epoch 028:    553 / 561 symm_kl=17.659, loss=4.601, nll_loss=0.827, ppl=1.77, wps=23259.7, ups=2.22, wpb=10486.6, bsz=375.3, num_updates=15700, lr=1.25e-05, gnorm=4.836, train_wall=45, wall=7735
2020-12-10 13:30:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:30:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:30:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:30:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:30:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:30:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:30:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:30:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:30:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:30:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:30:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:30:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:30:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:30:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:30:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:30:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:30:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:30:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:30:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:30:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:30:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:30:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:30:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:30:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:30:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:30:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:30:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:30:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:30:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:30:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:30:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:30:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:30:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:30:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:30:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:30:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:30:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:30:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:30:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:30:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:30:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:30:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:30:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:30:24 | INFO | valid | epoch 028 | valid on 'valid' subset | symm_kl 0 | loss 6.738 | nll_loss 4.846 | ppl 28.77 | bleu 21.08 | wps 4738.2 | wpb 7508.5 | bsz 272.7 | num_updates 15708 | best_bleu 21.47
2020-12-10 13:30:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:30:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:30:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:30:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:30:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:30:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 28 @ 15708 updates, score 21.08) (writing took 2.7296206019818783 seconds)
2020-12-10 13:30:27 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2020-12-10 13:30:27 | INFO | train | epoch 028 | symm_kl 17.704 | loss 4.602 | nll_loss 0.822 | ppl 1.77 | wps 21217.7 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 15708 | lr 1.25e-05 | gnorm 4.816 | train_wall 250 | wall 7762
2020-12-10 13:30:27 | INFO | fairseq.trainer | begin training epoch 29
2020-12-10 13:30:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:30:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:31:11 | INFO | train_inner | epoch 029:     92 / 561 symm_kl=17.148, loss=4.539, nll_loss=0.813, ppl=1.76, wps=14957.2, ups=1.42, wpb=10538, bsz=375.6, num_updates=15800, lr=1.25e-05, gnorm=4.605, train_wall=44, wall=7806
2020-12-10 13:31:56 | INFO | train_inner | epoch 029:    192 / 561 symm_kl=17.798, loss=4.617, nll_loss=0.831, ppl=1.78, wps=23153.7, ups=2.24, wpb=10357.6, bsz=361.3, num_updates=15900, lr=1.25e-05, gnorm=4.901, train_wall=45, wall=7850
2020-12-10 13:32:41 | INFO | train_inner | epoch 029:    292 / 561 symm_kl=17.914, loss=4.624, nll_loss=0.824, ppl=1.77, wps=23262.8, ups=2.2, wpb=10556.1, bsz=370.5, num_updates=16000, lr=1.25e-05, gnorm=4.878, train_wall=45, wall=7896
2020-12-10 13:33:26 | INFO | train_inner | epoch 029:    392 / 561 symm_kl=18.009, loss=4.629, nll_loss=0.818, ppl=1.76, wps=23259.7, ups=2.22, wpb=10480.1, bsz=376.2, num_updates=16100, lr=1.25e-05, gnorm=5.039, train_wall=45, wall=7941
2020-12-10 13:34:11 | INFO | train_inner | epoch 029:    492 / 561 symm_kl=17.13, loss=4.555, nll_loss=0.826, ppl=1.77, wps=23397, ups=2.21, wpb=10586.3, bsz=365.9, num_updates=16200, lr=1.25e-05, gnorm=4.578, train_wall=45, wall=7986
2020-12-10 13:34:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:34:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:34:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:34:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:34:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:34:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:34:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:34:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:34:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:34:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:34:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:34:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:34:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:34:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:34:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:34:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:34:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:34:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:34:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:34:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:34:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:34:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:34:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:34:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:34:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:34:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:34:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:34:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:34:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:34:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:34:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:34:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:34:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:34:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:34:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:34:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:34:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:35:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:35:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:35:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:35:03 | INFO | valid | epoch 029 | valid on 'valid' subset | symm_kl 0 | loss 6.739 | nll_loss 4.848 | ppl 28.81 | bleu 21.15 | wps 4554.6 | wpb 7508.5 | bsz 272.7 | num_updates 16269 | best_bleu 21.47
2020-12-10 13:35:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:35:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:35:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:35:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:35:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:35:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 29 @ 16269 updates, score 21.15) (writing took 2.803052047267556 seconds)
2020-12-10 13:35:06 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2020-12-10 13:35:06 | INFO | train | epoch 029 | symm_kl 17.694 | loss 4.602 | nll_loss 0.824 | ppl 1.77 | wps 21103 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 16269 | lr 1.25e-05 | gnorm 4.836 | train_wall 251 | wall 8041
2020-12-10 13:35:06 | INFO | fairseq.trainer | begin training epoch 30
2020-12-10 13:35:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:35:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:35:22 | INFO | train_inner | epoch 030:     31 / 561 symm_kl=18.304, loss=4.666, nll_loss=0.826, ppl=1.77, wps=14501.7, ups=1.41, wpb=10320, bsz=355.2, num_updates=16300, lr=1.25e-05, gnorm=5.005, train_wall=44, wall=8057
2020-12-10 13:36:07 | INFO | train_inner | epoch 030:    131 / 561 symm_kl=17.55, loss=4.594, nll_loss=0.83, ppl=1.78, wps=23701.5, ups=2.24, wpb=10579.8, bsz=376.9, num_updates=16400, lr=1.25e-05, gnorm=4.812, train_wall=44, wall=8102
2020-12-10 13:36:52 | INFO | train_inner | epoch 030:    231 / 561 symm_kl=17.497, loss=4.577, nll_loss=0.822, ppl=1.77, wps=23331.8, ups=2.22, wpb=10503.5, bsz=378.5, num_updates=16500, lr=1.25e-05, gnorm=4.756, train_wall=45, wall=8147
2020-12-10 13:37:37 | INFO | train_inner | epoch 030:    331 / 561 symm_kl=17.913, loss=4.629, nll_loss=0.823, ppl=1.77, wps=23286.5, ups=2.23, wpb=10427.6, bsz=355.9, num_updates=16600, lr=1.25e-05, gnorm=4.903, train_wall=45, wall=8192
2020-12-10 13:38:22 | INFO | train_inner | epoch 030:    431 / 561 symm_kl=17.331, loss=4.57, nll_loss=0.828, ppl=1.77, wps=23370.9, ups=2.23, wpb=10491.4, bsz=376.3, num_updates=16700, lr=1.25e-05, gnorm=4.67, train_wall=45, wall=8237
2020-12-10 13:39:07 | INFO | train_inner | epoch 030:    531 / 561 symm_kl=17.828, loss=4.617, nll_loss=0.821, ppl=1.77, wps=23396.3, ups=2.22, wpb=10525.2, bsz=362.6, num_updates=16800, lr=1.25e-05, gnorm=4.886, train_wall=45, wall=8282
2020-12-10 13:39:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:39:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:39:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:39:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:39:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:39:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:39:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:39:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:39:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:39:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:39:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:39:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:39:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:39:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:39:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:39:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:39:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:39:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:39:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:39:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:39:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:39:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:39:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:39:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:39:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:39:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:39:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:39:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:39:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:39:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:39:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:39:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:39:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:39:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:39:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:39:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:39:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:39:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:39:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:39:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:39:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:39:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:39:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:39:42 | INFO | valid | epoch 030 | valid on 'valid' subset | symm_kl 0 | loss 6.743 | nll_loss 4.854 | ppl 28.92 | bleu 21.13 | wps 4421 | wpb 7508.5 | bsz 272.7 | num_updates 16830 | best_bleu 21.47
2020-12-10 13:39:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:39:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:39:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:39:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:39:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:39:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 30 @ 16830 updates, score 21.13) (writing took 2.882993957027793 seconds)
2020-12-10 13:39:45 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2020-12-10 13:39:45 | INFO | train | epoch 030 | symm_kl 17.589 | loss 4.595 | nll_loss 0.825 | ppl 1.77 | wps 21104.5 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 16830 | lr 1.25e-05 | gnorm 4.787 | train_wall 250 | wall 8319
2020-12-10 13:39:45 | INFO | fairseq.trainer | begin training epoch 31
2020-12-10 13:39:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:39:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:40:18 | INFO | train_inner | epoch 031:     70 / 561 symm_kl=17.035, loss=4.545, nll_loss=0.833, ppl=1.78, wps=14358.1, ups=1.4, wpb=10238.4, bsz=371.5, num_updates=16900, lr=1.25e-05, gnorm=4.637, train_wall=44, wall=8353
2020-12-10 13:41:03 | INFO | train_inner | epoch 031:    170 / 561 symm_kl=16.709, loss=4.505, nll_loss=0.821, ppl=1.77, wps=23767.2, ups=2.23, wpb=10667.5, bsz=377.4, num_updates=17000, lr=1.25e-05, gnorm=4.441, train_wall=45, wall=8398
2020-12-10 13:41:48 | INFO | train_inner | epoch 031:    270 / 561 symm_kl=18.149, loss=4.649, nll_loss=0.823, ppl=1.77, wps=23461.7, ups=2.21, wpb=10608.7, bsz=358.7, num_updates=17100, lr=1.25e-05, gnorm=4.97, train_wall=45, wall=8443
2020-12-10 13:42:33 | INFO | train_inner | epoch 031:    370 / 561 symm_kl=17.576, loss=4.601, nll_loss=0.834, ppl=1.78, wps=23278.7, ups=2.23, wpb=10442.9, bsz=365.1, num_updates=17200, lr=1.25e-05, gnorm=4.762, train_wall=45, wall=8488
2020-12-10 13:43:18 | INFO | train_inner | epoch 031:    470 / 561 symm_kl=18.633, loss=4.702, nll_loss=0.828, ppl=1.78, wps=23255.7, ups=2.23, wpb=10445.1, bsz=357.7, num_updates=17300, lr=1.25e-05, gnorm=5.162, train_wall=45, wall=8533
2020-12-10 13:43:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:43:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:43:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:44:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:44:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:44:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:44:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:44:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:44:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:44:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:44:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:44:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:44:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:44:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:44:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:44:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:44:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:44:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:44:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:44:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:44:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:44:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:44:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:44:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:44:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:44:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:44:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:44:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:44:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:44:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:44:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:44:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:44:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:44:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:44:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:44:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:44:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:44:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:44:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:44:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:44:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:44:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:44:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:44:19 | INFO | valid | epoch 031 | valid on 'valid' subset | symm_kl 0 | loss 6.741 | nll_loss 4.851 | ppl 28.85 | bleu 21.15 | wps 4666.7 | wpb 7508.5 | bsz 272.7 | num_updates 17391 | best_bleu 21.47
2020-12-10 13:44:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:44:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:44:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:44:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:44:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:44:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 31 @ 17391 updates, score 21.15) (writing took 2.8489860016852617 seconds)
2020-12-10 13:44:22 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2020-12-10 13:44:22 | INFO | train | epoch 031 | symm_kl 17.486 | loss 4.587 | nll_loss 0.828 | ppl 1.78 | wps 21199.9 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 17391 | lr 1.25e-05 | gnorm 4.757 | train_wall 250 | wall 8597
2020-12-10 13:44:22 | INFO | fairseq.trainer | begin training epoch 32
2020-12-10 13:44:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:44:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:44:29 | INFO | train_inner | epoch 032:      9 / 561 symm_kl=16.689, loss=4.51, nll_loss=0.834, ppl=1.78, wps=14615.1, ups=1.41, wpb=10401, bsz=388.8, num_updates=17400, lr=1.25e-05, gnorm=4.533, train_wall=44, wall=8604
2020-12-10 13:45:13 | INFO | train_inner | epoch 032:    109 / 561 symm_kl=16.087, loss=4.446, nll_loss=0.826, ppl=1.77, wps=23622.8, ups=2.25, wpb=10489.3, bsz=385.8, num_updates=17500, lr=1.25e-05, gnorm=4.302, train_wall=44, wall=8648
2020-12-10 13:45:58 | INFO | train_inner | epoch 032:    209 / 561 symm_kl=16.776, loss=4.51, nll_loss=0.82, ppl=1.77, wps=23409, ups=2.23, wpb=10513.7, bsz=376.9, num_updates=17600, lr=1.25e-05, gnorm=4.64, train_wall=45, wall=8693
2020-12-10 13:46:44 | INFO | train_inner | epoch 032:    309 / 561 symm_kl=17.215, loss=4.557, nll_loss=0.824, ppl=1.77, wps=23292, ups=2.21, wpb=10553.4, bsz=374, num_updates=17700, lr=1.25e-05, gnorm=4.74, train_wall=45, wall=8739
2020-12-10 13:47:29 | INFO | train_inner | epoch 032:    409 / 561 symm_kl=18.672, loss=4.712, nll_loss=0.832, ppl=1.78, wps=23140.9, ups=2.22, wpb=10414.5, bsz=347.2, num_updates=17800, lr=1.25e-05, gnorm=5.119, train_wall=45, wall=8784
2020-12-10 13:48:14 | INFO | train_inner | epoch 032:    509 / 561 symm_kl=18.345, loss=4.682, nll_loss=0.84, ppl=1.79, wps=23196.6, ups=2.22, wpb=10427.4, bsz=366.3, num_updates=17900, lr=1.25e-05, gnorm=5.083, train_wall=45, wall=8829
2020-12-10 13:48:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:48:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:48:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:48:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:48:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:48:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:48:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:48:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:48:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:48:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:48:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:48:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:48:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:48:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:48:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:48:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:48:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:48:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:48:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:48:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:48:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:48:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:48:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:48:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:48:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:48:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:48:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:48:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:48:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:48:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:48:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:48:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:48:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:48:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:48:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:48:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:48:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:48:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:48:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:48:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:48:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:48:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:48:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:48:57 | INFO | valid | epoch 032 | valid on 'valid' subset | symm_kl 0 | loss 6.755 | nll_loss 4.863 | ppl 29.1 | bleu 21.14 | wps 4794.9 | wpb 7508.5 | bsz 272.7 | num_updates 17952 | best_bleu 21.47
2020-12-10 13:48:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:48:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:48:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:49:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:49:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:49:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 32 @ 17952 updates, score 21.14) (writing took 2.7761019337922335 seconds)
2020-12-10 13:49:00 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2020-12-10 13:49:00 | INFO | train | epoch 032 | symm_kl 17.417 | loss 4.581 | nll_loss 0.828 | ppl 1.78 | wps 21168.2 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 17952 | lr 1.25e-05 | gnorm 4.787 | train_wall 251 | wall 8875
2020-12-10 13:49:00 | INFO | fairseq.trainer | begin training epoch 33
2020-12-10 13:49:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:49:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:49:24 | INFO | train_inner | epoch 033:     48 / 561 symm_kl=17.389, loss=4.58, nll_loss=0.83, ppl=1.78, wps=15035, ups=1.43, wpb=10533.2, bsz=369.2, num_updates=18000, lr=1.25e-05, gnorm=4.82, train_wall=44, wall=8899
2020-12-10 13:50:09 | INFO | train_inner | epoch 033:    148 / 561 symm_kl=17.182, loss=4.562, nll_loss=0.835, ppl=1.78, wps=23201.5, ups=2.22, wpb=10454.4, bsz=383.2, num_updates=18100, lr=1.25e-05, gnorm=4.741, train_wall=45, wall=8944
2020-12-10 13:50:54 | INFO | train_inner | epoch 033:    248 / 561 symm_kl=18.004, loss=4.653, nll_loss=0.84, ppl=1.79, wps=23390, ups=2.23, wpb=10507.2, bsz=360.2, num_updates=18200, lr=1.25e-05, gnorm=4.922, train_wall=45, wall=8989
2020-12-10 13:51:39 | INFO | train_inner | epoch 033:    348 / 561 symm_kl=16.627, loss=4.495, nll_loss=0.825, ppl=1.77, wps=23070.8, ups=2.22, wpb=10407.4, bsz=381.3, num_updates=18300, lr=1.25e-05, gnorm=4.594, train_wall=45, wall=9034
2020-12-10 13:52:24 | INFO | train_inner | epoch 033:    448 / 561 symm_kl=16.372, loss=4.467, nll_loss=0.818, ppl=1.76, wps=23496.2, ups=2.21, wpb=10637.1, bsz=387, num_updates=18400, lr=1.25e-05, gnorm=4.501, train_wall=45, wall=9079
2020-12-10 13:53:09 | INFO | train_inner | epoch 033:    548 / 561 symm_kl=18.404, loss=4.69, nll_loss=0.837, ppl=1.79, wps=23043.2, ups=2.22, wpb=10386.1, bsz=344.4, num_updates=18500, lr=1.25e-05, gnorm=5.044, train_wall=45, wall=9124
2020-12-10 13:53:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:53:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:53:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:53:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:53:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:53:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:53:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:53:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:53:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:53:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:53:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:53:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:53:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:53:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:53:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:53:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:53:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:53:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:53:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:53:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:53:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:53:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:53:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:53:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:53:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:53:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:53:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:53:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:53:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:53:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:53:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:53:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:53:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:53:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:53:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:53:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:53:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:53:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:53:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:53:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:53:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:53:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:53:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:53:34 | INFO | valid | epoch 033 | valid on 'valid' subset | symm_kl 0 | loss 6.763 | nll_loss 4.869 | ppl 29.22 | bleu 21.1 | wps 5022.4 | wpb 7508.5 | bsz 272.7 | num_updates 18513 | best_bleu 21.47
2020-12-10 13:53:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:53:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:53:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:53:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:53:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:53:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 33 @ 18513 updates, score 21.1) (writing took 2.78109647333622 seconds)
2020-12-10 13:53:37 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2020-12-10 13:53:37 | INFO | train | epoch 033 | symm_kl 17.355 | loss 4.578 | nll_loss 0.831 | ppl 1.78 | wps 21201.4 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 18513 | lr 1.25e-05 | gnorm 4.773 | train_wall 251 | wall 9152
2020-12-10 13:53:37 | INFO | fairseq.trainer | begin training epoch 34
2020-12-10 13:53:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:53:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:54:19 | INFO | train_inner | epoch 034:     87 / 561 symm_kl=16.712, loss=4.512, nll_loss=0.826, ppl=1.77, wps=15107.1, ups=1.44, wpb=10502.1, bsz=362.1, num_updates=18600, lr=1.25e-05, gnorm=4.538, train_wall=44, wall=9194
2020-12-10 13:55:04 | INFO | train_inner | epoch 034:    187 / 561 symm_kl=16.89, loss=4.533, nll_loss=0.832, ppl=1.78, wps=23208.9, ups=2.22, wpb=10461.1, bsz=380.2, num_updates=18700, lr=1.25e-05, gnorm=4.555, train_wall=45, wall=9239
2020-12-10 13:55:49 | INFO | train_inner | epoch 034:    287 / 561 symm_kl=17.046, loss=4.538, nll_loss=0.824, ppl=1.77, wps=23266.2, ups=2.21, wpb=10529.6, bsz=370.9, num_updates=18800, lr=1.25e-05, gnorm=4.751, train_wall=45, wall=9284
2020-12-10 13:56:34 | INFO | train_inner | epoch 034:    387 / 561 symm_kl=18.057, loss=4.664, nll_loss=0.842, ppl=1.79, wps=23532.4, ups=2.23, wpb=10573.5, bsz=353.7, num_updates=18900, lr=1.25e-05, gnorm=4.979, train_wall=45, wall=9329
2020-12-10 13:57:19 | INFO | train_inner | epoch 034:    487 / 561 symm_kl=17.41, loss=4.584, nll_loss=0.834, ppl=1.78, wps=23227.3, ups=2.22, wpb=10444.4, bsz=384.6, num_updates=19000, lr=1.25e-05, gnorm=4.827, train_wall=45, wall=9374
2020-12-10 13:57:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 13:57:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:57:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:57:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:57:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:57:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:57:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:57:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:57:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:57:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:57:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:57:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:57:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:57:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:57:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:57:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:57:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:57:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:57:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:57:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:57:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:57:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:57:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:57:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:57:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:57:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:57:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:57:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:57:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:57:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:57:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:57:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:57:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:57:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:58:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:58:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:58:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:58:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 13:58:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 13:58:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 13:58:12 | INFO | valid | epoch 034 | valid on 'valid' subset | symm_kl 0 | loss 6.746 | nll_loss 4.857 | ppl 28.98 | bleu 21.06 | wps 4842.4 | wpb 7508.5 | bsz 272.7 | num_updates 19074 | best_bleu 21.47
2020-12-10 13:58:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 13:58:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:58:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:58:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:58:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:58:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 34 @ 19074 updates, score 21.06) (writing took 2.8194359950721264 seconds)
2020-12-10 13:58:15 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2020-12-10 13:58:15 | INFO | train | epoch 034 | symm_kl 17.248 | loss 4.57 | nll_loss 0.833 | ppl 1.78 | wps 21190.8 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 19074 | lr 1.25e-05 | gnorm 4.735 | train_wall 251 | wall 9430
2020-12-10 13:58:15 | INFO | fairseq.trainer | begin training epoch 35
2020-12-10 13:58:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 13:58:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 13:58:29 | INFO | train_inner | epoch 035:     26 / 561 symm_kl=17.272, loss=4.585, nll_loss=0.845, ppl=1.8, wps=14747.7, ups=1.43, wpb=10342.6, bsz=364.7, num_updates=19100, lr=1.25e-05, gnorm=4.716, train_wall=44, wall=9444
2020-12-10 13:59:14 | INFO | train_inner | epoch 035:    126 / 561 symm_kl=17.414, loss=4.585, nll_loss=0.829, ppl=1.78, wps=23142.7, ups=2.25, wpb=10301.2, bsz=359.6, num_updates=19200, lr=1.25e-05, gnorm=4.929, train_wall=44, wall=9488
2020-12-10 13:59:59 | INFO | train_inner | epoch 035:    226 / 561 symm_kl=17.567, loss=4.614, nll_loss=0.843, ppl=1.79, wps=23409.3, ups=2.22, wpb=10545.1, bsz=358.8, num_updates=19300, lr=1.25e-05, gnorm=4.799, train_wall=45, wall=9533
2020-12-10 14:00:44 | INFO | train_inner | epoch 035:    326 / 561 symm_kl=16.569, loss=4.498, nll_loss=0.83, ppl=1.78, wps=23389.5, ups=2.21, wpb=10569.9, bsz=384.6, num_updates=19400, lr=1.25e-05, gnorm=4.547, train_wall=45, wall=9579
2020-12-10 14:01:29 | INFO | train_inner | epoch 035:    426 / 561 symm_kl=16.874, loss=4.525, nll_loss=0.826, ppl=1.77, wps=23587.8, ups=2.2, wpb=10707.8, bsz=382, num_updates=19500, lr=1.25e-05, gnorm=4.627, train_wall=45, wall=9624
2020-12-10 14:02:14 | INFO | train_inner | epoch 035:    526 / 561 symm_kl=17.22, loss=4.566, nll_loss=0.836, ppl=1.79, wps=23426.8, ups=2.23, wpb=10510.5, bsz=371.4, num_updates=19600, lr=1.25e-05, gnorm=4.807, train_wall=45, wall=9669
2020-12-10 14:02:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:02:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:02:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:02:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:02:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:02:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:02:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:02:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:02:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:02:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:02:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:02:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:02:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:02:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:02:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:02:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:02:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:02:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:02:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:02:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:02:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:02:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:02:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:02:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:02:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:02:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:02:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:02:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:02:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:02:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:02:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:02:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:02:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:02:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:02:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:02:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:02:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:02:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:02:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:02:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:02:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:02:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:02:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:02:49 | INFO | valid | epoch 035 | valid on 'valid' subset | symm_kl 0 | loss 6.756 | nll_loss 4.865 | ppl 29.13 | bleu 20.98 | wps 5075.9 | wpb 7508.5 | bsz 272.7 | num_updates 19635 | best_bleu 21.47
2020-12-10 14:02:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:02:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:02:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:02:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:02:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:02:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 35 @ 19635 updates, score 20.98) (writing took 2.8228709418326616 seconds)
2020-12-10 14:02:52 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2020-12-10 14:02:52 | INFO | train | epoch 035 | symm_kl 17.147 | loss 4.562 | nll_loss 0.835 | ppl 1.78 | wps 21202.5 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 19635 | lr 1.25e-05 | gnorm 4.736 | train_wall 251 | wall 9707
2020-12-10 14:02:52 | INFO | fairseq.trainer | begin training epoch 36
2020-12-10 14:02:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:02:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:03:24 | INFO | train_inner | epoch 036:     65 / 561 symm_kl=18.232, loss=4.687, nll_loss=0.849, ppl=1.8, wps=14874.2, ups=1.44, wpb=10362, bsz=340.7, num_updates=19700, lr=1.25e-05, gnorm=5.031, train_wall=44, wall=9739
2020-12-10 14:04:09 | INFO | train_inner | epoch 036:    165 / 561 symm_kl=17.074, loss=4.555, nll_loss=0.837, ppl=1.79, wps=22986.1, ups=2.23, wpb=10329.3, bsz=374.4, num_updates=19800, lr=1.25e-05, gnorm=4.807, train_wall=45, wall=9784
2020-12-10 14:04:54 | INFO | train_inner | epoch 036:    265 / 561 symm_kl=17.059, loss=4.555, nll_loss=0.835, ppl=1.78, wps=23304.1, ups=2.21, wpb=10538, bsz=363.8, num_updates=19900, lr=1.25e-05, gnorm=4.726, train_wall=45, wall=9829
2020-12-10 14:05:39 | INFO | train_inner | epoch 036:    365 / 561 symm_kl=15.863, loss=4.424, nll_loss=0.826, ppl=1.77, wps=23177.3, ups=2.21, wpb=10487.1, bsz=385.5, num_updates=20000, lr=1.25e-05, gnorm=4.346, train_wall=45, wall=9874
2020-12-10 14:06:24 | INFO | train_inner | epoch 036:    465 / 561 symm_kl=17.68, loss=4.623, nll_loss=0.841, ppl=1.79, wps=23565.4, ups=2.21, wpb=10649.2, bsz=366.2, num_updates=20100, lr=1.25e-05, gnorm=4.909, train_wall=45, wall=9919
2020-12-10 14:07:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:07:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:07:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:07:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:07:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:07:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:07:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:07:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:07:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:07:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:07:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:07:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:07:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:07:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:07:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:07:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:07:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:07:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:07:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:07:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:07:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:07:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:07:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:07:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:07:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:07:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:07:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:07:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:07:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:07:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:07:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:07:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:07:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:07:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:07:27 | INFO | valid | epoch 036 | valid on 'valid' subset | symm_kl 0 | loss 6.752 | nll_loss 4.863 | ppl 29.1 | bleu 21.02 | wps 5061.3 | wpb 7508.5 | bsz 272.7 | num_updates 20196 | best_bleu 21.47
2020-12-10 14:07:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:07:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:07:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:07:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:07:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:07:30 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 36 @ 20196 updates, score 21.02) (writing took 3.093601642176509 seconds)
2020-12-10 14:07:30 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2020-12-10 14:07:30 | INFO | train | epoch 036 | symm_kl 17.117 | loss 4.56 | nll_loss 0.837 | ppl 1.79 | wps 21192.8 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 20196 | lr 1.25e-05 | gnorm 4.763 | train_wall 251 | wall 9984
2020-12-10 14:07:30 | INFO | fairseq.trainer | begin training epoch 37
2020-12-10 14:07:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:07:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:07:35 | INFO | train_inner | epoch 037:      4 / 561 symm_kl=16.91, loss=4.539, nll_loss=0.841, ppl=1.79, wps=14767.9, ups=1.42, wpb=10377, bsz=379.3, num_updates=20200, lr=1.25e-05, gnorm=4.752, train_wall=45, wall=9989
2020-12-10 14:08:18 | INFO | train_inner | epoch 037:    104 / 561 symm_kl=16.872, loss=4.543, nll_loss=0.843, ppl=1.79, wps=23290.4, ups=2.28, wpb=10222.2, bsz=366.7, num_updates=20300, lr=1.25e-05, gnorm=4.67, train_wall=44, wall=10033
2020-12-10 14:09:04 | INFO | train_inner | epoch 037:    204 / 561 symm_kl=17.022, loss=4.555, nll_loss=0.84, ppl=1.79, wps=23312.7, ups=2.21, wpb=10547.6, bsz=372.1, num_updates=20400, lr=1.25e-05, gnorm=4.792, train_wall=45, wall=10079
2020-12-10 14:09:49 | INFO | train_inner | epoch 037:    304 / 561 symm_kl=16.24, loss=4.468, nll_loss=0.833, ppl=1.78, wps=23413.4, ups=2.23, wpb=10518.4, bsz=382.1, num_updates=20500, lr=1.25e-05, gnorm=4.446, train_wall=45, wall=10123
2020-12-10 14:10:34 | INFO | train_inner | epoch 037:    404 / 561 symm_kl=17.758, loss=4.633, nll_loss=0.844, ppl=1.8, wps=23519.1, ups=2.22, wpb=10595.9, bsz=358.7, num_updates=20600, lr=1.25e-05, gnorm=4.984, train_wall=45, wall=10169
2020-12-10 14:11:19 | INFO | train_inner | epoch 037:    504 / 561 symm_kl=16.888, loss=4.533, nll_loss=0.832, ppl=1.78, wps=23403, ups=2.21, wpb=10592.1, bsz=370.3, num_updates=20700, lr=1.25e-05, gnorm=4.681, train_wall=45, wall=10214
2020-12-10 14:11:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:11:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:11:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:11:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:11:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:11:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:11:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:11:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:11:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:11:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:11:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:11:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:11:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:11:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:11:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:11:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:11:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:11:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:11:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:11:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:11:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:11:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:11:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:11:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:11:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:11:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:11:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:11:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:11:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:11:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:11:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:11:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:11:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:11:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:11:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:11:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:11:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:11:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:11:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:11:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:12:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:12:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:12:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:12:04 | INFO | valid | epoch 037 | valid on 'valid' subset | symm_kl 0 | loss 6.759 | nll_loss 4.869 | ppl 29.22 | bleu 21.03 | wps 5046.3 | wpb 7508.5 | bsz 272.7 | num_updates 20757 | best_bleu 21.47
2020-12-10 14:12:04 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:12:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:12:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:12:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 37 @ 20757 updates, score 21.03) (writing took 2.4145458918064833 seconds)
2020-12-10 14:12:06 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2020-12-10 14:12:06 | INFO | train | epoch 037 | symm_kl 16.996 | loss 4.551 | nll_loss 0.838 | ppl 1.79 | wps 21251.3 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 20757 | lr 1.25e-05 | gnorm 4.736 | train_wall 251 | wall 10261
2020-12-10 14:12:06 | INFO | fairseq.trainer | begin training epoch 38
2020-12-10 14:12:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:12:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:12:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:12:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:12:28 | INFO | train_inner | epoch 038:     43 / 561 symm_kl=17.3, loss=4.586, nll_loss=0.84, ppl=1.79, wps=14989.5, ups=1.45, wpb=10361, bsz=363, num_updates=20800, lr=1.25e-05, gnorm=4.847, train_wall=44, wall=10283
2020-12-10 14:13:13 | INFO | train_inner | epoch 038:    143 / 561 symm_kl=16.355, loss=4.495, nll_loss=0.846, ppl=1.8, wps=23611.5, ups=2.24, wpb=10522.8, bsz=379, num_updates=20900, lr=1.25e-05, gnorm=4.488, train_wall=44, wall=10327
2020-12-10 14:13:58 | INFO | train_inner | epoch 038:    243 / 561 symm_kl=16.389, loss=4.477, nll_loss=0.822, ppl=1.77, wps=23402.1, ups=2.22, wpb=10558.7, bsz=370.7, num_updates=21000, lr=1.25e-05, gnorm=4.592, train_wall=45, wall=10373
2020-12-10 14:14:43 | INFO | train_inner | epoch 038:    343 / 561 symm_kl=16.985, loss=4.551, nll_loss=0.838, ppl=1.79, wps=23387.7, ups=2.22, wpb=10553.8, bsz=368.2, num_updates=21100, lr=1.25e-05, gnorm=4.722, train_wall=45, wall=10418
2020-12-10 14:15:28 | INFO | train_inner | epoch 038:    443 / 561 symm_kl=17.375, loss=4.603, nll_loss=0.854, ppl=1.81, wps=23459.1, ups=2.23, wpb=10522.9, bsz=366.2, num_updates=21200, lr=1.25e-05, gnorm=4.828, train_wall=45, wall=10463
2020-12-10 14:16:13 | INFO | train_inner | epoch 038:    543 / 561 symm_kl=17.628, loss=4.615, nll_loss=0.844, ppl=1.8, wps=22854.8, ups=2.21, wpb=10355.7, bsz=368.3, num_updates=21300, lr=1.25e-05, gnorm=5.021, train_wall=45, wall=10508
2020-12-10 14:16:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:16:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:16:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:16:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:16:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:16:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:16:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:16:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:16:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:16:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:16:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:16:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:16:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:16:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:16:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:16:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:16:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:16:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:16:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:16:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:16:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:16:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:16:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:16:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:16:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:16:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:16:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:16:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:16:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:16:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:16:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:16:40 | INFO | valid | epoch 038 | valid on 'valid' subset | symm_kl 0 | loss 6.769 | nll_loss 4.876 | ppl 29.37 | bleu 21.11 | wps 5129.2 | wpb 7508.5 | bsz 272.7 | num_updates 21318 | best_bleu 21.47
2020-12-10 14:16:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:16:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:16:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:16:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:16:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:16:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 38 @ 21318 updates, score 21.11) (writing took 2.8845631014555693 seconds)
2020-12-10 14:16:43 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2020-12-10 14:16:43 | INFO | train | epoch 038 | symm_kl 16.953 | loss 4.548 | nll_loss 0.84 | ppl 1.79 | wps 21245.9 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 21318 | lr 1.25e-05 | gnorm 4.737 | train_wall 251 | wall 10538
2020-12-10 14:16:43 | INFO | fairseq.trainer | begin training epoch 39
2020-12-10 14:16:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:16:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:17:22 | INFO | train_inner | epoch 039:     82 / 561 symm_kl=17.663, loss=4.626, nll_loss=0.838, ppl=1.79, wps=15008.7, ups=1.44, wpb=10393.8, bsz=346.3, num_updates=21400, lr=1.25e-05, gnorm=5.007, train_wall=44, wall=10577
2020-12-10 14:18:08 | INFO | train_inner | epoch 039:    182 / 561 symm_kl=16.667, loss=4.526, nll_loss=0.848, ppl=1.8, wps=23006.1, ups=2.21, wpb=10399.1, bsz=380.4, num_updates=21500, lr=1.25e-05, gnorm=4.692, train_wall=45, wall=10622
2020-12-10 14:18:52 | INFO | train_inner | epoch 039:    282 / 561 symm_kl=16.561, loss=4.505, nll_loss=0.835, ppl=1.78, wps=23523, ups=2.23, wpb=10556, bsz=371.6, num_updates=21600, lr=1.25e-05, gnorm=4.613, train_wall=45, wall=10667
2020-12-10 14:19:37 | INFO | train_inner | epoch 039:    382 / 561 symm_kl=17.004, loss=4.553, nll_loss=0.838, ppl=1.79, wps=23537.9, ups=2.22, wpb=10587, bsz=364.3, num_updates=21700, lr=1.25e-05, gnorm=4.781, train_wall=45, wall=10712
2020-12-10 14:20:22 | INFO | train_inner | epoch 039:    482 / 561 symm_kl=16.649, loss=4.526, nll_loss=0.851, ppl=1.8, wps=23299.8, ups=2.22, wpb=10501.2, bsz=384.2, num_updates=21800, lr=1.25e-05, gnorm=4.724, train_wall=45, wall=10757
2020-12-10 14:20:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:20:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:20:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:20:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:21:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:21:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:21:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:21:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:21:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:21:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:21:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:21:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:21:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:21:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:21:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:21:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:21:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:21:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:21:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:21:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:21:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:21:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:21:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:21:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:21:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:21:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:21:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:21:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:21:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:21:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:21:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:21:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:21:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:21:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:21:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:21:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:21:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:21:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:21:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:21:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:21:17 | INFO | valid | epoch 039 | valid on 'valid' subset | symm_kl 0 | loss 6.759 | nll_loss 4.87 | ppl 29.24 | bleu 21.12 | wps 5055.8 | wpb 7508.5 | bsz 272.7 | num_updates 21879 | best_bleu 21.47
2020-12-10 14:21:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:21:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:21:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:21:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:21:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:21:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 39 @ 21879 updates, score 21.12) (writing took 2.746024038642645 seconds)
2020-12-10 14:21:20 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2020-12-10 14:21:20 | INFO | train | epoch 039 | symm_kl 16.888 | loss 4.545 | nll_loss 0.842 | ppl 1.79 | wps 21259.9 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 21879 | lr 1.25e-05 | gnorm 4.753 | train_wall 250 | wall 10815
2020-12-10 14:21:20 | INFO | fairseq.trainer | begin training epoch 40
2020-12-10 14:21:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:21:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:21:32 | INFO | train_inner | epoch 040:     21 / 561 symm_kl=17.017, loss=4.556, nll_loss=0.844, ppl=1.8, wps=14966.1, ups=1.43, wpb=10438, bsz=368.4, num_updates=21900, lr=1.25e-05, gnorm=4.792, train_wall=44, wall=10827
2020-12-10 14:22:17 | INFO | train_inner | epoch 040:    121 / 561 symm_kl=16.234, loss=4.477, nll_loss=0.841, ppl=1.79, wps=23464.5, ups=2.24, wpb=10465.6, bsz=372.5, num_updates=22000, lr=1.25e-05, gnorm=4.539, train_wall=44, wall=10872
2020-12-10 14:23:02 | INFO | train_inner | epoch 040:    221 / 561 symm_kl=17.076, loss=4.564, nll_loss=0.842, ppl=1.79, wps=23317, ups=2.21, wpb=10550.5, bsz=360.9, num_updates=22100, lr=1.25e-05, gnorm=4.909, train_wall=45, wall=10917
2020-12-10 14:23:47 | INFO | train_inner | epoch 040:    321 / 561 symm_kl=16.031, loss=4.45, nll_loss=0.84, ppl=1.79, wps=23106.1, ups=2.21, wpb=10469, bsz=397.3, num_updates=22200, lr=1.25e-05, gnorm=4.448, train_wall=45, wall=10962
2020-12-10 14:24:33 | INFO | train_inner | epoch 040:    421 / 561 symm_kl=17.082, loss=4.573, nll_loss=0.85, ppl=1.8, wps=23118.2, ups=2.21, wpb=10478.2, bsz=364.9, num_updates=22300, lr=1.25e-05, gnorm=4.801, train_wall=45, wall=11008
2020-12-10 14:25:18 | INFO | train_inner | epoch 040:    521 / 561 symm_kl=17.494, loss=4.609, nll_loss=0.842, ppl=1.79, wps=23094.6, ups=2.22, wpb=10424.1, bsz=355, num_updates=22400, lr=1.25e-05, gnorm=4.906, train_wall=45, wall=11053
2020-12-10 14:25:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:25:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:25:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:25:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:25:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:25:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:25:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:25:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:25:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:25:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:25:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:25:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:25:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:25:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:25:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:25:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:25:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:25:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:25:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:25:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:25:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:25:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:25:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:25:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:25:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:25:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:25:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:25:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:25:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:25:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:25:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:25:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:25:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:25:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:25:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:25:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:25:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:25:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:25:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:25:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:25:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:25:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:25:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:25:56 | INFO | valid | epoch 040 | valid on 'valid' subset | symm_kl 0 | loss 6.773 | nll_loss 4.881 | ppl 29.47 | bleu 21.1 | wps 4833 | wpb 7508.5 | bsz 272.7 | num_updates 22440 | best_bleu 21.47
2020-12-10 14:25:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:25:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:25:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:25:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:25:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:25:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 40 @ 22440 updates, score 21.1) (writing took 2.786430288106203 seconds)
2020-12-10 14:25:59 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2020-12-10 14:25:59 | INFO | train | epoch 040 | symm_kl 16.842 | loss 4.541 | nll_loss 0.844 | ppl 1.79 | wps 21085.1 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 22440 | lr 1.25e-05 | gnorm 4.736 | train_wall 252 | wall 11094
2020-12-10 14:25:59 | INFO | fairseq.trainer | begin training epoch 41
2020-12-10 14:26:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:26:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:26:28 | INFO | train_inner | epoch 041:     60 / 561 symm_kl=16.826, loss=4.539, nll_loss=0.839, ppl=1.79, wps=14936.5, ups=1.42, wpb=10525.7, bsz=360, num_updates=22500, lr=1.25e-05, gnorm=4.734, train_wall=44, wall=11123
2020-12-10 14:27:14 | INFO | train_inner | epoch 041:    160 / 561 symm_kl=16.686, loss=4.527, nll_loss=0.844, ppl=1.79, wps=22987.4, ups=2.21, wpb=10399.4, bsz=362.2, num_updates=22600, lr=1.25e-05, gnorm=4.674, train_wall=45, wall=11168
2020-12-10 14:27:59 | INFO | train_inner | epoch 041:    260 / 561 symm_kl=16.837, loss=4.546, nll_loss=0.847, ppl=1.8, wps=23116.8, ups=2.21, wpb=10483.5, bsz=374.1, num_updates=22700, lr=1.25e-05, gnorm=4.793, train_wall=45, wall=11214
2020-12-10 14:28:44 | INFO | train_inner | epoch 041:    360 / 561 symm_kl=16.492, loss=4.51, nll_loss=0.848, ppl=1.8, wps=23598.6, ups=2.22, wpb=10621.1, bsz=376.9, num_updates=22800, lr=1.25e-05, gnorm=4.492, train_wall=45, wall=11259
2020-12-10 14:29:29 | INFO | train_inner | epoch 041:    460 / 561 symm_kl=16.767, loss=4.529, nll_loss=0.841, ppl=1.79, wps=23254.1, ups=2.22, wpb=10489.2, bsz=377.4, num_updates=22900, lr=1.25e-05, gnorm=4.803, train_wall=45, wall=11304
2020-12-10 14:30:14 | INFO | train_inner | epoch 041:    560 / 561 symm_kl=16.967, loss=4.565, nll_loss=0.855, ppl=1.81, wps=23235.8, ups=2.22, wpb=10443.2, bsz=368.8, num_updates=23000, lr=1.25e-05, gnorm=4.772, train_wall=45, wall=11349
2020-12-10 14:30:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:30:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:30:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:30:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:30:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:30:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:30:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:30:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:30:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:30:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:30:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:30:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:30:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:30:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:30:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:30:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:30:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:30:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:30:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:30:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:30:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:30:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:30:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:30:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:30:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:30:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:30:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:30:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:30:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:30:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:30:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:30:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:30:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:30:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:30:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:30:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:30:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:30:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:30:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:30:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:30:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:30:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:30:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:30:34 | INFO | valid | epoch 041 | valid on 'valid' subset | symm_kl 0 | loss 6.77 | nll_loss 4.879 | ppl 29.43 | bleu 21.01 | wps 5060.6 | wpb 7508.5 | bsz 272.7 | num_updates 23001 | best_bleu 21.47
2020-12-10 14:30:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:30:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:30:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:30:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:30:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:30:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 41 @ 23001 updates, score 21.01) (writing took 2.8436834514141083 seconds)
2020-12-10 14:30:37 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2020-12-10 14:30:37 | INFO | train | epoch 041 | symm_kl 16.746 | loss 4.534 | nll_loss 0.846 | ppl 1.8 | wps 21169.1 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 23001 | lr 1.25e-05 | gnorm 4.711 | train_wall 251 | wall 11371
2020-12-10 14:30:37 | INFO | fairseq.trainer | begin training epoch 42
2020-12-10 14:30:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:30:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:31:24 | INFO | train_inner | epoch 042:     99 / 561 symm_kl=16.204, loss=4.471, nll_loss=0.836, ppl=1.78, wps=14872.5, ups=1.43, wpb=10382.2, bsz=370.8, num_updates=23100, lr=1.25e-05, gnorm=4.587, train_wall=44, wall=11419
2020-12-10 14:32:09 | INFO | train_inner | epoch 042:    199 / 561 symm_kl=17.384, loss=4.611, nll_loss=0.854, ppl=1.81, wps=23283.4, ups=2.22, wpb=10482.9, bsz=352.5, num_updates=23200, lr=1.25e-05, gnorm=4.819, train_wall=45, wall=11464
2020-12-10 14:32:54 | INFO | train_inner | epoch 042:    299 / 561 symm_kl=17.118, loss=4.579, nll_loss=0.857, ppl=1.81, wps=23271.7, ups=2.22, wpb=10489.9, bsz=368.6, num_updates=23300, lr=1.25e-05, gnorm=4.852, train_wall=45, wall=11509
2020-12-10 14:33:39 | INFO | train_inner | epoch 042:    399 / 561 symm_kl=16.215, loss=4.481, nll_loss=0.844, ppl=1.8, wps=22923.7, ups=2.19, wpb=10464, bsz=381.9, num_updates=23400, lr=1.25e-05, gnorm=4.559, train_wall=45, wall=11554
2020-12-10 14:34:25 | INFO | train_inner | epoch 042:    499 / 561 symm_kl=16.643, loss=4.532, nll_loss=0.855, ppl=1.81, wps=23304.4, ups=2.22, wpb=10491.5, bsz=365.4, num_updates=23500, lr=1.25e-05, gnorm=4.627, train_wall=45, wall=11599
2020-12-10 14:34:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:34:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:34:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:34:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:34:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:34:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:34:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:34:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:34:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:34:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:34:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:34:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:34:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:34:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:34:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:34:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:34:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:34:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:34:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:34:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:34:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:34:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:34:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:34:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:34:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:34:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:34:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:34:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:35:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:35:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:35:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:35:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:35:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:35:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:35:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:35:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:35:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:35:13 | INFO | valid | epoch 042 | valid on 'valid' subset | symm_kl 0 | loss 6.771 | nll_loss 4.88 | ppl 29.44 | bleu 21 | wps 4746.5 | wpb 7508.5 | bsz 272.7 | num_updates 23562 | best_bleu 21.47
2020-12-10 14:35:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:35:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:35:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:35:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:35:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:35:16 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 42 @ 23562 updates, score 21.0) (writing took 2.8537929002195597 seconds)
2020-12-10 14:35:16 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2020-12-10 14:35:16 | INFO | train | epoch 042 | symm_kl 16.7 | loss 4.532 | nll_loss 0.848 | ppl 1.8 | wps 21053.5 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 23562 | lr 1.25e-05 | gnorm 4.7 | train_wall 252 | wall 11651
2020-12-10 14:35:16 | INFO | fairseq.trainer | begin training epoch 43
2020-12-10 14:35:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:35:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:35:36 | INFO | train_inner | epoch 043:     38 / 561 symm_kl=16.521, loss=4.501, nll_loss=0.837, ppl=1.79, wps=14775, ups=1.41, wpb=10502.1, bsz=385.8, num_updates=23600, lr=1.25e-05, gnorm=4.844, train_wall=45, wall=11670
2020-12-10 14:36:21 | INFO | train_inner | epoch 043:    138 / 561 symm_kl=16.421, loss=4.501, nll_loss=0.841, ppl=1.79, wps=23190.1, ups=2.21, wpb=10475.4, bsz=367, num_updates=23700, lr=1.25e-05, gnorm=4.78, train_wall=45, wall=11716
2020-12-10 14:37:06 | INFO | train_inner | epoch 043:    238 / 561 symm_kl=16.434, loss=4.497, nll_loss=0.838, ppl=1.79, wps=23170.3, ups=2.2, wpb=10540.2, bsz=367.1, num_updates=23800, lr=1.25e-05, gnorm=4.633, train_wall=45, wall=11761
2020-12-10 14:37:51 | INFO | train_inner | epoch 043:    338 / 561 symm_kl=16.004, loss=4.444, nll_loss=0.836, ppl=1.79, wps=23231.5, ups=2.21, wpb=10494.7, bsz=385.2, num_updates=23900, lr=1.25e-05, gnorm=4.595, train_wall=45, wall=11806
2020-12-10 14:38:37 | INFO | train_inner | epoch 043:    438 / 561 symm_kl=16.563, loss=4.54, nll_loss=0.872, ppl=1.83, wps=23252.7, ups=2.22, wpb=10482.8, bsz=371.3, num_updates=24000, lr=1.25e-05, gnorm=4.564, train_wall=45, wall=11851
2020-12-10 14:39:21 | INFO | train_inner | epoch 043:    538 / 561 symm_kl=17.874, loss=4.671, nll_loss=0.867, ppl=1.82, wps=23790.5, ups=2.25, wpb=10562.2, bsz=345.7, num_updates=24100, lr=1.25e-05, gnorm=5.007, train_wall=44, wall=11896
2020-12-10 14:39:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:39:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:39:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:39:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:39:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:39:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:39:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:39:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:39:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:39:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:39:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:39:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:39:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:39:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:39:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:39:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:39:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:39:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:39:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:39:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:39:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:39:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:39:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:39:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:39:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:39:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:39:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:39:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:39:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:39:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:39:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:39:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:39:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:39:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:39:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:39:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:39:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:39:53 | INFO | valid | epoch 043 | valid on 'valid' subset | symm_kl 0 | loss 6.763 | nll_loss 4.873 | ppl 29.3 | bleu 21.03 | wps 4522.7 | wpb 7508.5 | bsz 272.7 | num_updates 24123 | best_bleu 21.47
2020-12-10 14:39:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:39:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:39:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:39:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:39:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:39:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 43 @ 24123 updates, score 21.03) (writing took 2.851815227419138 seconds)
2020-12-10 14:39:55 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2020-12-10 14:39:55 | INFO | train | epoch 043 | symm_kl 16.625 | loss 4.526 | nll_loss 0.85 | ppl 1.8 | wps 21041.7 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 24123 | lr 1.25e-05 | gnorm 4.727 | train_wall 251 | wall 11930
2020-12-10 14:39:55 | INFO | fairseq.trainer | begin training epoch 44
2020-12-10 14:39:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:39:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:40:32 | INFO | train_inner | epoch 044:     77 / 561 symm_kl=15.984, loss=4.461, nll_loss=0.849, ppl=1.8, wps=14345.5, ups=1.4, wpb=10248.9, bsz=367.8, num_updates=24200, lr=1.25e-05, gnorm=4.52, train_wall=44, wall=11967
2020-12-10 14:41:18 | INFO | train_inner | epoch 044:    177 / 561 symm_kl=16.491, loss=4.51, nll_loss=0.846, ppl=1.8, wps=23255.8, ups=2.21, wpb=10502.4, bsz=375.4, num_updates=24300, lr=1.25e-05, gnorm=4.65, train_wall=45, wall=12012
2020-12-10 14:42:03 | INFO | train_inner | epoch 044:    277 / 561 symm_kl=16.646, loss=4.539, nll_loss=0.861, ppl=1.82, wps=23309.5, ups=2.21, wpb=10543.4, bsz=374.8, num_updates=24400, lr=1.25e-05, gnorm=4.698, train_wall=45, wall=12058
2020-12-10 14:42:48 | INFO | train_inner | epoch 044:    377 / 561 symm_kl=17.232, loss=4.593, nll_loss=0.852, ppl=1.81, wps=23285.8, ups=2.23, wpb=10445.2, bsz=355.8, num_updates=24500, lr=1.25e-05, gnorm=4.901, train_wall=45, wall=12102
2020-12-10 14:43:33 | INFO | train_inner | epoch 044:    477 / 561 symm_kl=16.331, loss=4.49, nll_loss=0.843, ppl=1.79, wps=23326.1, ups=2.2, wpb=10593, bsz=379.9, num_updates=24600, lr=1.25e-05, gnorm=4.69, train_wall=45, wall=12148
2020-12-10 14:44:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:44:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:44:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:44:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:44:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:44:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:44:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:44:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:44:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:44:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:44:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:44:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:44:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:44:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:44:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:44:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:44:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:44:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:44:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:44:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:44:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:44:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:44:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:44:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:44:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:44:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:44:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:44:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:44:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:44:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:44:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:44:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:44:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:44:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:44:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:44:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:44:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:44:30 | INFO | valid | epoch 044 | valid on 'valid' subset | symm_kl 0 | loss 6.762 | nll_loss 4.872 | ppl 29.28 | bleu 21.11 | wps 5121.7 | wpb 7508.5 | bsz 272.7 | num_updates 24684 | best_bleu 21.47
2020-12-10 14:44:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:44:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:44:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:44:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:44:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:44:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 44 @ 24684 updates, score 21.11) (writing took 2.899412939324975 seconds)
2020-12-10 14:44:33 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2020-12-10 14:44:33 | INFO | train | epoch 044 | symm_kl 16.54 | loss 4.52 | nll_loss 0.85 | ppl 1.8 | wps 21195 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 24684 | lr 1.25e-05 | gnorm 4.679 | train_wall 251 | wall 12208
2020-12-10 14:44:33 | INFO | fairseq.trainer | begin training epoch 45
2020-12-10 14:44:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:44:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:44:43 | INFO | train_inner | epoch 045:     16 / 561 symm_kl=16.48, loss=4.512, nll_loss=0.848, ppl=1.8, wps=14992.4, ups=1.43, wpb=10481.2, bsz=371.1, num_updates=24700, lr=1.25e-05, gnorm=4.662, train_wall=45, wall=12218
2020-12-10 14:45:27 | INFO | train_inner | epoch 045:    116 / 561 symm_kl=16.777, loss=4.549, nll_loss=0.858, ppl=1.81, wps=23448.3, ups=2.25, wpb=10436.8, bsz=361.7, num_updates=24800, lr=1.25e-05, gnorm=4.771, train_wall=44, wall=12262
2020-12-10 14:46:13 | INFO | train_inner | epoch 045:    216 / 561 symm_kl=16.108, loss=4.477, nll_loss=0.85, ppl=1.8, wps=22999.6, ups=2.2, wpb=10444.1, bsz=375, num_updates=24900, lr=1.25e-05, gnorm=4.589, train_wall=45, wall=12308
2020-12-10 14:46:58 | INFO | train_inner | epoch 045:    316 / 561 symm_kl=16.867, loss=4.56, nll_loss=0.857, ppl=1.81, wps=23333.7, ups=2.2, wpb=10588.6, bsz=369.6, num_updates=25000, lr=1.25e-05, gnorm=4.731, train_wall=45, wall=12353
2020-12-10 14:47:44 | INFO | train_inner | epoch 045:    416 / 561 symm_kl=16.411, loss=4.503, nll_loss=0.846, ppl=1.8, wps=23249.9, ups=2.2, wpb=10591.3, bsz=375.4, num_updates=25100, lr=1.25e-05, gnorm=4.711, train_wall=45, wall=12399
2020-12-10 14:48:29 | INFO | train_inner | epoch 045:    516 / 561 symm_kl=16.317, loss=4.502, nll_loss=0.857, ppl=1.81, wps=23319.1, ups=2.23, wpb=10437.8, bsz=370.2, num_updates=25200, lr=1.25e-05, gnorm=4.736, train_wall=45, wall=12443
2020-12-10 14:48:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:48:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:48:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:48:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:48:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:48:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:48:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:48:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:48:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:48:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:48:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:48:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:48:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:48:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:48:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:48:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:48:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:48:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:48:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:48:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:48:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:48:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:48:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:48:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:48:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:48:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:48:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:48:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:48:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:48:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:48:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:48:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:48:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:48:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:48:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:48:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:48:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:48:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:48:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:48:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:49:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:49:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:49:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:49:09 | INFO | valid | epoch 045 | valid on 'valid' subset | symm_kl 0 | loss 6.765 | nll_loss 4.877 | ppl 29.38 | bleu 20.99 | wps 4740.4 | wpb 7508.5 | bsz 272.7 | num_updates 25245 | best_bleu 21.47
2020-12-10 14:49:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:49:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:49:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:49:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 45 @ 25245 updates, score 20.99) (writing took 2.79944240860641 seconds)
2020-12-10 14:49:12 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2020-12-10 14:49:12 | INFO | train | epoch 045 | symm_kl 16.505 | loss 4.519 | nll_loss 0.853 | ppl 1.81 | wps 21088.5 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 25245 | lr 1.25e-05 | gnorm 4.717 | train_wall 252 | wall 12487
2020-12-10 14:49:12 | INFO | fairseq.trainer | begin training epoch 46
2020-12-10 14:49:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:49:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:49:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:49:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:49:39 | INFO | train_inner | epoch 046:     55 / 561 symm_kl=16.213, loss=4.487, nll_loss=0.851, ppl=1.8, wps=14957.1, ups=1.42, wpb=10523.7, bsz=364.2, num_updates=25300, lr=1.25e-05, gnorm=4.622, train_wall=44, wall=12514
2020-12-10 14:50:24 | INFO | train_inner | epoch 046:    155 / 561 symm_kl=17.482, loss=4.632, nll_loss=0.864, ppl=1.82, wps=22927.9, ups=2.21, wpb=10387.2, bsz=345.5, num_updates=25400, lr=1.25e-05, gnorm=4.982, train_wall=45, wall=12559
2020-12-10 14:51:09 | INFO | train_inner | epoch 046:    255 / 561 symm_kl=16.289, loss=4.498, nll_loss=0.853, ppl=1.81, wps=23408, ups=2.21, wpb=10594.3, bsz=372, num_updates=25500, lr=1.25e-05, gnorm=4.553, train_wall=45, wall=12604
2020-12-10 14:51:55 | INFO | train_inner | epoch 046:    355 / 561 symm_kl=16.389, loss=4.506, nll_loss=0.856, ppl=1.81, wps=22969.3, ups=2.2, wpb=10421.4, bsz=373.7, num_updates=25600, lr=1.25e-05, gnorm=4.75, train_wall=45, wall=12650
2020-12-10 14:52:40 | INFO | train_inner | epoch 046:    455 / 561 symm_kl=15.744, loss=4.438, nll_loss=0.85, ppl=1.8, wps=23158.8, ups=2.21, wpb=10460.1, bsz=381.5, num_updates=25700, lr=1.25e-05, gnorm=4.44, train_wall=45, wall=12695
2020-12-10 14:53:25 | INFO | train_inner | epoch 046:    555 / 561 symm_kl=16.563, loss=4.526, nll_loss=0.855, ppl=1.81, wps=23235.3, ups=2.22, wpb=10456.9, bsz=371.7, num_updates=25800, lr=1.25e-05, gnorm=4.777, train_wall=45, wall=12740
2020-12-10 14:53:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:53:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:53:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:53:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:53:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:53:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:53:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:53:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:53:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:53:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:53:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:53:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:53:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:53:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:53:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:53:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:53:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:53:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:53:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:53:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:53:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:53:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:53:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:53:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:53:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:53:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:53:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:53:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:53:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:53:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:53:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:53:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:53:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:53:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:53:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:53:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:53:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:53:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:53:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:53:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:53:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:53:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:53:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:53:47 | INFO | valid | epoch 046 | valid on 'valid' subset | symm_kl 0 | loss 6.773 | nll_loss 4.882 | ppl 29.48 | bleu 21.06 | wps 4983.2 | wpb 7508.5 | bsz 272.7 | num_updates 25806 | best_bleu 21.47
2020-12-10 14:53:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:53:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:53:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:53:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:53:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:53:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 46 @ 25806 updates, score 21.06) (writing took 2.604140056297183 seconds)
2020-12-10 14:53:50 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2020-12-10 14:53:50 | INFO | train | epoch 046 | symm_kl 16.418 | loss 4.512 | nll_loss 0.855 | ppl 1.81 | wps 21149.7 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 25806 | lr 1.25e-05 | gnorm 4.691 | train_wall 252 | wall 12765
2020-12-10 14:53:50 | INFO | fairseq.trainer | begin training epoch 47
2020-12-10 14:53:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:53:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:54:34 | INFO | train_inner | epoch 047:     94 / 561 symm_kl=17, loss=4.574, nll_loss=0.855, ppl=1.81, wps=14999.4, ups=1.44, wpb=10412.5, bsz=362.2, num_updates=25900, lr=1.25e-05, gnorm=5.066, train_wall=44, wall=12809
2020-12-10 14:55:19 | INFO | train_inner | epoch 047:    194 / 561 symm_kl=15.775, loss=4.452, nll_loss=0.862, ppl=1.82, wps=23482.2, ups=2.22, wpb=10557.9, bsz=382.2, num_updates=26000, lr=1.25e-05, gnorm=4.536, train_wall=45, wall=12854
2020-12-10 14:56:04 | INFO | train_inner | epoch 047:    294 / 561 symm_kl=16.948, loss=4.57, nll_loss=0.857, ppl=1.81, wps=23233.1, ups=2.23, wpb=10441.1, bsz=346.2, num_updates=26100, lr=1.25e-05, gnorm=4.807, train_wall=45, wall=12899
2020-12-10 14:56:49 | INFO | train_inner | epoch 047:    394 / 561 symm_kl=16.427, loss=4.522, nll_loss=0.865, ppl=1.82, wps=23526, ups=2.25, wpb=10460.6, bsz=375, num_updates=26200, lr=1.25e-05, gnorm=4.713, train_wall=44, wall=12944
2020-12-10 14:57:34 | INFO | train_inner | epoch 047:    494 / 561 symm_kl=16.092, loss=4.467, nll_loss=0.842, ppl=1.79, wps=23423.6, ups=2.21, wpb=10602.2, bsz=378.7, num_updates=26300, lr=1.25e-05, gnorm=4.577, train_wall=45, wall=12989
2020-12-10 14:58:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 14:58:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:58:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:58:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:58:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:58:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:58:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:58:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:58:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:58:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:58:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:58:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:58:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:58:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:58:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:58:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:58:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:58:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:58:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:58:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:58:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:58:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:58:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:58:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:58:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:58:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:58:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:58:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:58:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:58:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:58:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:58:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:58:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:58:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:58:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:58:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:58:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:58:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:58:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:58:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:58:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 14:58:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 14:58:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 14:58:24 | INFO | valid | epoch 047 | valid on 'valid' subset | symm_kl 0 | loss 6.767 | nll_loss 4.878 | ppl 29.4 | bleu 20.96 | wps 4790.5 | wpb 7508.5 | bsz 272.7 | num_updates 26367 | best_bleu 21.47
2020-12-10 14:58:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 14:58:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:58:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:58:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:58:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:58:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 47 @ 26367 updates, score 20.96) (writing took 3.0083805434405804 seconds)
2020-12-10 14:58:27 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2020-12-10 14:58:27 | INFO | train | epoch 047 | symm_kl 16.364 | loss 4.508 | nll_loss 0.855 | ppl 1.81 | wps 21188.7 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 26367 | lr 1.25e-05 | gnorm 4.685 | train_wall 250 | wall 13042
2020-12-10 14:58:27 | INFO | fairseq.trainer | begin training epoch 48
2020-12-10 14:58:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 14:58:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 14:58:45 | INFO | train_inner | epoch 048:     33 / 561 symm_kl=15.98, loss=4.464, nll_loss=0.853, ppl=1.81, wps=14679.6, ups=1.41, wpb=10411.8, bsz=376.6, num_updates=26400, lr=1.25e-05, gnorm=4.514, train_wall=44, wall=13060
2020-12-10 14:59:30 | INFO | train_inner | epoch 048:    133 / 561 symm_kl=16.307, loss=4.497, nll_loss=0.851, ppl=1.8, wps=23350.4, ups=2.23, wpb=10490.9, bsz=380.7, num_updates=26500, lr=1.25e-05, gnorm=4.715, train_wall=45, wall=13105
2020-12-10 15:00:15 | INFO | train_inner | epoch 048:    233 / 561 symm_kl=17.145, loss=4.603, nll_loss=0.874, ppl=1.83, wps=23230.1, ups=2.22, wpb=10465.3, bsz=361.5, num_updates=26600, lr=1.25e-05, gnorm=5.038, train_wall=45, wall=13150
2020-12-10 15:01:00 | INFO | train_inner | epoch 048:    333 / 561 symm_kl=16.022, loss=4.478, nll_loss=0.859, ppl=1.81, wps=23296.5, ups=2.24, wpb=10407.5, bsz=369.1, num_updates=26700, lr=1.25e-05, gnorm=4.51, train_wall=45, wall=13194
2020-12-10 15:01:45 | INFO | train_inner | epoch 048:    433 / 561 symm_kl=15.727, loss=4.43, nll_loss=0.842, ppl=1.79, wps=23214.8, ups=2.23, wpb=10430.6, bsz=366.3, num_updates=26800, lr=1.25e-05, gnorm=4.461, train_wall=45, wall=13239
2020-12-10 15:02:30 | INFO | train_inner | epoch 048:    533 / 561 symm_kl=16.742, loss=4.555, nll_loss=0.864, ppl=1.82, wps=23620.4, ups=2.22, wpb=10637.6, bsz=357.2, num_updates=26900, lr=1.25e-05, gnorm=4.774, train_wall=45, wall=13284
2020-12-10 15:02:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:02:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:02:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:02:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:02:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:02:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:02:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:02:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:02:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:02:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:02:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:02:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:02:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:02:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:02:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:02:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:02:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:02:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:02:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:02:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:02:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:02:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:02:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:02:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:02:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:02:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:02:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:02:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:02:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:02:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:02:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:02:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:02:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:02:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:02:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:02:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:02:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:02:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:02:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:02:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:03:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:03:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:03:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:03:02 | INFO | valid | epoch 048 | valid on 'valid' subset | symm_kl 0 | loss 6.764 | nll_loss 4.874 | ppl 29.33 | bleu 21.05 | wps 5004.9 | wpb 7508.5 | bsz 272.7 | num_updates 26928 | best_bleu 21.47
2020-12-10 15:03:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:03:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:03:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:03:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:03:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:03:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 48 @ 26928 updates, score 21.05) (writing took 3.0793499518185854 seconds)
2020-12-10 15:03:05 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2020-12-10 15:03:05 | INFO | train | epoch 048 | symm_kl 16.328 | loss 4.506 | nll_loss 0.858 | ppl 1.81 | wps 21198.5 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 26928 | lr 1.25e-05 | gnorm 4.691 | train_wall 251 | wall 13320
2020-12-10 15:03:05 | INFO | fairseq.trainer | begin training epoch 49
2020-12-10 15:03:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:03:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:03:40 | INFO | train_inner | epoch 049:     72 / 561 symm_kl=15.709, loss=4.443, nll_loss=0.858, ppl=1.81, wps=14918.4, ups=1.42, wpb=10473.6, bsz=379.8, num_updates=27000, lr=1.25e-05, gnorm=4.512, train_wall=44, wall=13355
2020-12-10 15:04:25 | INFO | train_inner | epoch 049:    172 / 561 symm_kl=17.13, loss=4.593, nll_loss=0.858, ppl=1.81, wps=23434.8, ups=2.23, wpb=10486, bsz=354.9, num_updates=27100, lr=1.25e-05, gnorm=5.001, train_wall=45, wall=13399
2020-12-10 15:05:10 | INFO | train_inner | epoch 049:    272 / 561 symm_kl=16.425, loss=4.523, nll_loss=0.865, ppl=1.82, wps=23120.3, ups=2.22, wpb=10391.3, bsz=360.1, num_updates=27200, lr=1.25e-05, gnorm=4.783, train_wall=45, wall=13444
2020-12-10 15:05:55 | INFO | train_inner | epoch 049:    372 / 561 symm_kl=15.312, loss=4.389, nll_loss=0.844, ppl=1.79, wps=23500.4, ups=2.21, wpb=10635.2, bsz=389, num_updates=27300, lr=1.25e-05, gnorm=4.382, train_wall=45, wall=13490
2020-12-10 15:06:40 | INFO | train_inner | epoch 049:    472 / 561 symm_kl=15.838, loss=4.458, nll_loss=0.86, ppl=1.82, wps=23039.6, ups=2.21, wpb=10429.3, bsz=383.7, num_updates=27400, lr=1.25e-05, gnorm=4.594, train_wall=45, wall=13535
2020-12-10 15:07:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:07:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:07:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:07:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:07:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:07:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:07:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:07:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:07:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:07:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:07:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:07:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:07:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:07:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:07:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:07:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:07:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:07:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:07:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:07:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:07:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:07:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:07:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:07:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:07:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:07:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:07:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:07:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:07:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:07:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:07:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:07:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:07:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:07:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:07:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:07:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:07:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:07:40 | INFO | valid | epoch 049 | valid on 'valid' subset | symm_kl 0 | loss 6.781 | nll_loss 4.889 | ppl 29.64 | bleu 21 | wps 4875.8 | wpb 7508.5 | bsz 272.7 | num_updates 27489 | best_bleu 21.47
2020-12-10 15:07:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:07:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:07:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:07:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 49 @ 27489 updates, score 21.0) (writing took 3.116482215002179 seconds)
2020-12-10 15:07:43 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2020-12-10 15:07:43 | INFO | train | epoch 049 | symm_kl 16.233 | loss 4.499 | nll_loss 0.859 | ppl 1.81 | wps 21116.8 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 27489 | lr 1.25e-05 | gnorm 4.679 | train_wall 251 | wall 13598
2020-12-10 15:07:43 | INFO | fairseq.trainer | begin training epoch 50
2020-12-10 15:07:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:07:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:07:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:07:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:07:51 | INFO | train_inner | epoch 050:     11 / 561 symm_kl=16.542, loss=4.54, nll_loss=0.87, ppl=1.83, wps=14636, ups=1.4, wpb=10417.5, bsz=362.4, num_updates=27500, lr=1.25e-05, gnorm=4.721, train_wall=45, wall=13606
2020-12-10 15:08:36 | INFO | train_inner | epoch 050:    111 / 561 symm_kl=16.769, loss=4.56, nll_loss=0.865, ppl=1.82, wps=23421.3, ups=2.24, wpb=10459.5, bsz=363.2, num_updates=27600, lr=1.25e-05, gnorm=4.838, train_wall=44, wall=13651
2020-12-10 15:09:21 | INFO | train_inner | epoch 050:    211 / 561 symm_kl=15.904, loss=4.47, nll_loss=0.865, ppl=1.82, wps=23453.2, ups=2.22, wpb=10548.2, bsz=373.6, num_updates=27700, lr=1.25e-05, gnorm=4.515, train_wall=45, wall=13696
2020-12-10 15:10:05 | INFO | train_inner | epoch 050:    311 / 561 symm_kl=16.256, loss=4.505, nll_loss=0.862, ppl=1.82, wps=23348.4, ups=2.24, wpb=10404.5, bsz=376.9, num_updates=27800, lr=1.25e-05, gnorm=4.705, train_wall=44, wall=13740
2020-12-10 15:10:51 | INFO | train_inner | epoch 050:    411 / 561 symm_kl=16.451, loss=4.52, nll_loss=0.858, ppl=1.81, wps=23169.8, ups=2.2, wpb=10512.1, bsz=361.7, num_updates=27900, lr=1.25e-05, gnorm=4.73, train_wall=45, wall=13786
2020-12-10 15:11:36 | INFO | train_inner | epoch 050:    511 / 561 symm_kl=15.329, loss=4.395, nll_loss=0.847, ppl=1.8, wps=23206.9, ups=2.21, wpb=10519.7, bsz=381.7, num_updates=28000, lr=1.25e-05, gnorm=4.361, train_wall=45, wall=13831
2020-12-10 15:11:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:11:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:11:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:11:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:12:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:12:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:12:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:12:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:12:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:12:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:12:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:12:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:12:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:12:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:12:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:12:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:12:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:12:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:12:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:12:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:12:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:12:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:12:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:12:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:12:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:12:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:12:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:12:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:12:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:12:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:12:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:12:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:12:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:12:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:12:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:12:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:12:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:12:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:12:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:12:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:12:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:12:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:12:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:12:18 | INFO | valid | epoch 050 | valid on 'valid' subset | symm_kl 0 | loss 6.779 | nll_loss 4.889 | ppl 29.63 | bleu 21.02 | wps 5050 | wpb 7508.5 | bsz 272.7 | num_updates 28050 | best_bleu 21.47
2020-12-10 15:12:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:12:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:12:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:12:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:12:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:12:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 50 @ 28050 updates, score 21.02) (writing took 3.0736222993582487 seconds)
2020-12-10 15:12:21 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2020-12-10 15:12:21 | INFO | train | epoch 050 | symm_kl 16.185 | loss 4.495 | nll_loss 0.86 | ppl 1.82 | wps 21186.3 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 28050 | lr 1.25e-05 | gnorm 4.641 | train_wall 251 | wall 13876
2020-12-10 15:12:21 | INFO | fairseq.trainer | begin training epoch 51
2020-12-10 15:12:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:12:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:12:46 | INFO | train_inner | epoch 051:     50 / 561 symm_kl=16.674, loss=4.551, nll_loss=0.863, ppl=1.82, wps=15076.9, ups=1.43, wpb=10517, bsz=352.3, num_updates=28100, lr=1.25e-05, gnorm=4.776, train_wall=44, wall=13901
2020-12-10 15:13:31 | INFO | train_inner | epoch 051:    150 / 561 symm_kl=15.979, loss=4.471, nll_loss=0.857, ppl=1.81, wps=23223.5, ups=2.2, wpb=10575, bsz=382, num_updates=28200, lr=1.25e-05, gnorm=4.855, train_wall=45, wall=13946
2020-12-10 15:14:16 | INFO | train_inner | epoch 051:    250 / 561 symm_kl=16.901, loss=4.586, nll_loss=0.878, ppl=1.84, wps=23306.7, ups=2.23, wpb=10463.3, bsz=348.8, num_updates=28300, lr=1.25e-05, gnorm=4.832, train_wall=45, wall=13991
2020-12-10 15:15:01 | INFO | train_inner | epoch 051:    350 / 561 symm_kl=16.436, loss=4.516, nll_loss=0.853, ppl=1.81, wps=23338.7, ups=2.22, wpb=10491.3, bsz=358.1, num_updates=28400, lr=1.25e-05, gnorm=4.75, train_wall=45, wall=14036
2020-12-10 15:15:46 | INFO | train_inner | epoch 051:    450 / 561 symm_kl=16.003, loss=4.482, nll_loss=0.864, ppl=1.82, wps=23164.1, ups=2.25, wpb=10300.2, bsz=370.1, num_updates=28500, lr=1.25e-05, gnorm=4.679, train_wall=44, wall=14081
2020-12-10 15:16:31 | INFO | train_inner | epoch 051:    550 / 561 symm_kl=15.088, loss=4.373, nll_loss=0.854, ppl=1.81, wps=23506.8, ups=2.22, wpb=10580.7, bsz=399.1, num_updates=28600, lr=1.25e-05, gnorm=4.308, train_wall=45, wall=14126
2020-12-10 15:16:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:16:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:16:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:16:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:16:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:16:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:16:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:16:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:16:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:16:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:16:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:16:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:16:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:16:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:16:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:16:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:16:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:16:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:16:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:16:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:16:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:16:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:16:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:16:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:16:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:16:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:16:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:16:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:16:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:16:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:16:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:16:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:16:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:16:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:16:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:16:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:16:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:16:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:16:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:16:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:16:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:16:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:16:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:16:56 | INFO | valid | epoch 051 | valid on 'valid' subset | symm_kl 0 | loss 6.773 | nll_loss 4.886 | ppl 29.56 | bleu 21.21 | wps 4791.5 | wpb 7508.5 | bsz 272.7 | num_updates 28611 | best_bleu 21.47
2020-12-10 15:16:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:16:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:16:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:16:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:16:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:16:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 51 @ 28611 updates, score 21.21) (writing took 2.9185212776064873 seconds)
2020-12-10 15:16:59 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2020-12-10 15:16:59 | INFO | train | epoch 051 | symm_kl 16.143 | loss 4.492 | nll_loss 0.861 | ppl 1.82 | wps 21168.3 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 28611 | lr 1.25e-05 | gnorm 4.704 | train_wall 251 | wall 14154
2020-12-10 15:16:59 | INFO | fairseq.trainer | begin training epoch 52
2020-12-10 15:17:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:17:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:17:41 | INFO | train_inner | epoch 052:     89 / 561 symm_kl=16.121, loss=4.492, nll_loss=0.863, ppl=1.82, wps=14887.4, ups=1.42, wpb=10474.7, bsz=370.8, num_updates=28700, lr=1.25e-05, gnorm=4.756, train_wall=44, wall=14196
2020-12-10 15:18:26 | INFO | train_inner | epoch 052:    189 / 561 symm_kl=16.49, loss=4.537, nll_loss=0.868, ppl=1.83, wps=23253.1, ups=2.21, wpb=10501.8, bsz=354, num_updates=28800, lr=1.25e-05, gnorm=4.753, train_wall=45, wall=14241
2020-12-10 15:19:12 | INFO | train_inner | epoch 052:    289 / 561 symm_kl=15.618, loss=4.433, nll_loss=0.854, ppl=1.81, wps=23137.8, ups=2.2, wpb=10495.5, bsz=380.1, num_updates=28900, lr=1.25e-05, gnorm=4.506, train_wall=45, wall=14286
2020-12-10 15:19:57 | INFO | train_inner | epoch 052:    389 / 561 symm_kl=16.029, loss=4.476, nll_loss=0.856, ppl=1.81, wps=23322.4, ups=2.2, wpb=10577.6, bsz=370.9, num_updates=29000, lr=1.25e-05, gnorm=4.628, train_wall=45, wall=14332
2020-12-10 15:20:42 | INFO | train_inner | epoch 052:    489 / 561 symm_kl=16.004, loss=4.48, nll_loss=0.865, ppl=1.82, wps=23153, ups=2.22, wpb=10427.2, bsz=373.4, num_updates=29100, lr=1.25e-05, gnorm=4.672, train_wall=45, wall=14377
2020-12-10 15:21:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:21:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:21:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:21:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:21:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:21:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:21:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:21:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:21:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:21:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:21:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:21:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:21:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:21:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:21:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:21:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:21:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:21:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:21:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:21:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:21:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:21:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:21:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:21:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:21:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:21:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:21:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:21:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:21:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:21:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:21:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:21:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:21:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:21:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:21:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:21:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:21:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:21:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:21:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:21:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:21:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:21:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:21:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:21:34 | INFO | valid | epoch 052 | valid on 'valid' subset | symm_kl 0 | loss 6.777 | nll_loss 4.888 | ppl 29.61 | bleu 21.03 | wps 4993.8 | wpb 7508.5 | bsz 272.7 | num_updates 29172 | best_bleu 21.47
2020-12-10 15:21:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:21:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:21:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:21:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:21:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:21:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 52 @ 29172 updates, score 21.03) (writing took 2.8694585282355547 seconds)
2020-12-10 15:21:37 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2020-12-10 15:21:37 | INFO | train | epoch 052 | symm_kl 16.082 | loss 4.488 | nll_loss 0.863 | ppl 1.82 | wps 21152.7 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 29172 | lr 1.25e-05 | gnorm 4.661 | train_wall 251 | wall 14432
2020-12-10 15:21:37 | INFO | fairseq.trainer | begin training epoch 53
2020-12-10 15:21:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:21:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:21:52 | INFO | train_inner | epoch 053:     28 / 561 symm_kl=16.107, loss=4.497, nll_loss=0.871, ppl=1.83, wps=14841.6, ups=1.43, wpb=10412.8, bsz=369.4, num_updates=29200, lr=1.25e-05, gnorm=4.606, train_wall=44, wall=14447
2020-12-10 15:22:37 | INFO | train_inner | epoch 053:    128 / 561 symm_kl=16.622, loss=4.547, nll_loss=0.866, ppl=1.82, wps=23544.9, ups=2.24, wpb=10517.2, bsz=362.4, num_updates=29300, lr=1.25e-05, gnorm=4.851, train_wall=44, wall=14492
2020-12-10 15:23:22 | INFO | train_inner | epoch 053:    228 / 561 symm_kl=15.787, loss=4.464, nll_loss=0.871, ppl=1.83, wps=23399.8, ups=2.22, wpb=10538.4, bsz=369.5, num_updates=29400, lr=1.25e-05, gnorm=4.57, train_wall=45, wall=14537
2020-12-10 15:24:07 | INFO | train_inner | epoch 053:    328 / 561 symm_kl=16.072, loss=4.501, nll_loss=0.877, ppl=1.84, wps=23288, ups=2.23, wpb=10450.3, bsz=371.4, num_updates=29500, lr=1.25e-05, gnorm=4.476, train_wall=45, wall=14582
2020-12-10 15:24:52 | INFO | train_inner | epoch 053:    428 / 561 symm_kl=15.25, loss=4.388, nll_loss=0.847, ppl=1.8, wps=23613.6, ups=2.22, wpb=10625.5, bsz=387.6, num_updates=29600, lr=1.25e-05, gnorm=4.432, train_wall=45, wall=14627
2020-12-10 15:25:37 | INFO | train_inner | epoch 053:    528 / 561 symm_kl=16.742, loss=4.564, nll_loss=0.872, ppl=1.83, wps=22954.2, ups=2.23, wpb=10290.3, bsz=352.6, num_updates=29700, lr=1.25e-05, gnorm=4.884, train_wall=45, wall=14671
2020-12-10 15:25:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:25:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:25:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:25:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:25:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:25:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:25:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:25:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:25:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:25:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:25:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:25:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:25:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:25:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:25:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:25:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:25:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:25:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:25:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:25:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:25:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:25:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:25:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:25:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:25:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:25:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:25:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:25:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:25:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:25:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:25:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:25:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:25:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:25:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:26:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:26:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:26:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:26:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:26:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:26:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:26:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:26:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:26:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:26:12 | INFO | valid | epoch 053 | valid on 'valid' subset | symm_kl 0 | loss 6.779 | nll_loss 4.892 | ppl 29.7 | bleu 21.11 | wps 4736.8 | wpb 7508.5 | bsz 272.7 | num_updates 29733 | best_bleu 21.47
2020-12-10 15:26:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:26:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:26:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:26:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:26:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:26:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 53 @ 29733 updates, score 21.11) (writing took 3.139688540250063 seconds)
2020-12-10 15:26:15 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2020-12-10 15:26:15 | INFO | train | epoch 053 | symm_kl 16.022 | loss 4.484 | nll_loss 0.865 | ppl 1.82 | wps 21151.7 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 29733 | lr 1.25e-05 | gnorm 4.621 | train_wall 250 | wall 14710
2020-12-10 15:26:15 | INFO | fairseq.trainer | begin training epoch 54
2020-12-10 15:26:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:26:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:26:47 | INFO | train_inner | epoch 054:     67 / 561 symm_kl=16.02, loss=4.493, nll_loss=0.872, ppl=1.83, wps=14820.9, ups=1.42, wpb=10457, bsz=368, num_updates=29800, lr=1.25e-05, gnorm=4.642, train_wall=44, wall=14742
2020-12-10 15:27:32 | INFO | train_inner | epoch 054:    167 / 561 symm_kl=15.668, loss=4.439, nll_loss=0.861, ppl=1.82, wps=23507.6, ups=2.22, wpb=10573.8, bsz=388.2, num_updates=29900, lr=1.25e-05, gnorm=4.556, train_wall=45, wall=14787
2020-12-10 15:28:17 | INFO | train_inner | epoch 054:    267 / 561 symm_kl=15.553, loss=4.437, nll_loss=0.867, ppl=1.82, wps=23327.5, ups=2.23, wpb=10452.7, bsz=372.8, num_updates=30000, lr=1.25e-05, gnorm=4.452, train_wall=45, wall=14832
2020-12-10 15:29:02 | INFO | train_inner | epoch 054:    367 / 561 symm_kl=16.404, loss=4.527, nll_loss=0.866, ppl=1.82, wps=23551.7, ups=2.22, wpb=10594.9, bsz=360.5, num_updates=30100, lr=1.25e-05, gnorm=4.682, train_wall=45, wall=14877
2020-12-10 15:29:47 | INFO | train_inner | epoch 054:    467 / 561 symm_kl=16.092, loss=4.497, nll_loss=0.87, ppl=1.83, wps=23163.7, ups=2.24, wpb=10341.8, bsz=361.8, num_updates=30200, lr=1.25e-05, gnorm=4.721, train_wall=44, wall=14921
2020-12-10 15:30:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:30:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:30:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:30:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:30:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:30:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:30:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:30:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:30:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:30:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:30:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:30:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:30:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:30:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:30:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:30:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:30:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:30:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:30:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:30:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:30:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:30:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:30:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:30:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:30:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:30:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:30:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:30:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:30:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:30:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:30:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:30:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:30:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:30:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:30:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:30:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:30:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:30:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:30:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:30:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:30:50 | INFO | valid | epoch 054 | valid on 'valid' subset | symm_kl 0 | loss 6.778 | nll_loss 4.891 | ppl 29.66 | bleu 20.98 | wps 4475.7 | wpb 7508.5 | bsz 272.7 | num_updates 30294 | best_bleu 21.47
2020-12-10 15:30:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:30:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:30:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:30:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 54 @ 30294 updates, score 20.98) (writing took 3.038214832544327 seconds)
2020-12-10 15:30:53 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2020-12-10 15:30:53 | INFO | train | epoch 054 | symm_kl 15.947 | loss 4.478 | nll_loss 0.866 | ppl 1.82 | wps 21154.4 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 30294 | lr 1.25e-05 | gnorm 4.617 | train_wall 250 | wall 14988
2020-12-10 15:30:53 | INFO | fairseq.trainer | begin training epoch 55
2020-12-10 15:30:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:30:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:30:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:30:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:30:59 | INFO | train_inner | epoch 055:      6 / 561 symm_kl=15.922, loss=4.471, nll_loss=0.86, ppl=1.82, wps=14458.7, ups=1.39, wpb=10419, bsz=366.8, num_updates=30300, lr=1.25e-05, gnorm=4.707, train_wall=44, wall=14993
2020-12-10 15:31:43 | INFO | train_inner | epoch 055:    106 / 561 symm_kl=15.89, loss=4.478, nll_loss=0.869, ppl=1.83, wps=23585.2, ups=2.25, wpb=10492, bsz=368.5, num_updates=30400, lr=1.25e-05, gnorm=4.624, train_wall=44, wall=15038
2020-12-10 15:32:29 | INFO | train_inner | epoch 055:    206 / 561 symm_kl=14.989, loss=4.373, nll_loss=0.862, ppl=1.82, wps=23226.7, ups=2.2, wpb=10579.2, bsz=398.7, num_updates=30500, lr=1.25e-05, gnorm=4.338, train_wall=45, wall=15083
2020-12-10 15:33:14 | INFO | train_inner | epoch 055:    306 / 561 symm_kl=15.597, loss=4.44, nll_loss=0.863, ppl=1.82, wps=23281.8, ups=2.22, wpb=10487.6, bsz=367.8, num_updates=30600, lr=1.25e-05, gnorm=4.531, train_wall=45, wall=15129
2020-12-10 15:33:59 | INFO | train_inner | epoch 055:    406 / 561 symm_kl=16.196, loss=4.507, nll_loss=0.871, ppl=1.83, wps=23039.1, ups=2.22, wpb=10398.6, bsz=367.4, num_updates=30700, lr=1.25e-05, gnorm=4.804, train_wall=45, wall=15174
2020-12-10 15:34:44 | INFO | train_inner | epoch 055:    506 / 561 symm_kl=16.662, loss=4.555, nll_loss=0.868, ppl=1.82, wps=23258.3, ups=2.21, wpb=10502, bsz=350.7, num_updates=30800, lr=1.25e-05, gnorm=4.912, train_wall=45, wall=15219
2020-12-10 15:35:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:35:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:35:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:35:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:35:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:35:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:35:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:35:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:35:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:35:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:35:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:35:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:35:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:35:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:35:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:35:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:35:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:35:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:35:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:35:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:35:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:35:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:35:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:35:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:35:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:35:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:35:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:35:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:35:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:35:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:35:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:35:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:35:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:35:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:35:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:35:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:35:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:35:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:35:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:35:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:35:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:35:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:35:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:35:28 | INFO | valid | epoch 055 | valid on 'valid' subset | symm_kl 0 | loss 6.795 | nll_loss 4.905 | ppl 29.96 | bleu 21.1 | wps 4991.7 | wpb 7508.5 | bsz 272.7 | num_updates 30855 | best_bleu 21.47
2020-12-10 15:35:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:35:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:35:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:35:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:35:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:35:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 55 @ 30855 updates, score 21.1) (writing took 2.94288451038301 seconds)
2020-12-10 15:35:31 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2020-12-10 15:35:31 | INFO | train | epoch 055 | symm_kl 15.913 | loss 4.477 | nll_loss 0.869 | ppl 1.83 | wps 21156.9 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 30855 | lr 1.25e-05 | gnorm 4.643 | train_wall 251 | wall 15266
2020-12-10 15:35:31 | INFO | fairseq.trainer | begin training epoch 56
2020-12-10 15:35:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:35:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:35:54 | INFO | train_inner | epoch 056:     45 / 561 symm_kl=15.917, loss=4.489, nll_loss=0.882, ppl=1.84, wps=15046.9, ups=1.44, wpb=10470.7, bsz=374.1, num_updates=30900, lr=1.25e-05, gnorm=4.488, train_wall=44, wall=15288
2020-12-10 15:36:38 | INFO | train_inner | epoch 056:    145 / 561 symm_kl=15.375, loss=4.421, nll_loss=0.865, ppl=1.82, wps=23106.8, ups=2.23, wpb=10358.8, bsz=371.2, num_updates=31000, lr=1.25e-05, gnorm=4.511, train_wall=45, wall=15333
2020-12-10 15:37:23 | INFO | train_inner | epoch 056:    245 / 561 symm_kl=16.052, loss=4.499, nll_loss=0.875, ppl=1.83, wps=23373.7, ups=2.23, wpb=10503.7, bsz=359.2, num_updates=31100, lr=1.25e-05, gnorm=4.684, train_wall=45, wall=15378
2020-12-10 15:38:08 | INFO | train_inner | epoch 056:    345 / 561 symm_kl=16.464, loss=4.534, nll_loss=0.874, ppl=1.83, wps=23345.4, ups=2.22, wpb=10520.9, bsz=375.1, num_updates=31200, lr=1.25e-05, gnorm=4.88, train_wall=45, wall=15423
2020-12-10 15:38:53 | INFO | train_inner | epoch 056:    445 / 561 symm_kl=15.508, loss=4.427, nll_loss=0.857, ppl=1.81, wps=23431.3, ups=2.22, wpb=10545.3, bsz=370.1, num_updates=31300, lr=1.25e-05, gnorm=4.416, train_wall=45, wall=15468
2020-12-10 15:39:38 | INFO | train_inner | epoch 056:    545 / 561 symm_kl=16.095, loss=4.503, nll_loss=0.874, ppl=1.83, wps=23410.7, ups=2.22, wpb=10542.9, bsz=364.2, num_updates=31400, lr=1.25e-05, gnorm=4.652, train_wall=45, wall=15513
2020-12-10 15:39:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:39:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:39:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:39:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:39:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:39:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:39:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:39:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:39:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:39:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:39:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:39:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:39:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:39:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:39:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:39:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:39:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:39:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:39:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:39:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:39:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:39:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:39:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:39:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:39:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:39:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:39:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:39:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:40:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:40:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:40:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:40:06 | INFO | valid | epoch 056 | valid on 'valid' subset | symm_kl 0 | loss 6.784 | nll_loss 4.896 | ppl 29.78 | bleu 21.05 | wps 4697.9 | wpb 7508.5 | bsz 272.7 | num_updates 31416 | best_bleu 21.47
2020-12-10 15:40:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:40:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:40:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:40:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:40:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:40:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 56 @ 31416 updates, score 21.05) (writing took 2.933820065110922 seconds)
2020-12-10 15:40:09 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2020-12-10 15:40:09 | INFO | train | epoch 056 | symm_kl 15.848 | loss 4.472 | nll_loss 0.87 | ppl 1.83 | wps 21143.6 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 31416 | lr 1.25e-05 | gnorm 4.605 | train_wall 251 | wall 15544
2020-12-10 15:40:09 | INFO | fairseq.trainer | begin training epoch 57
2020-12-10 15:40:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:40:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:40:49 | INFO | train_inner | epoch 057:     84 / 561 symm_kl=15.415, loss=4.422, nll_loss=0.86, ppl=1.82, wps=14944.9, ups=1.42, wpb=10556.8, bsz=366.4, num_updates=31500, lr=1.25e-05, gnorm=4.427, train_wall=44, wall=15584
2020-12-10 15:41:34 | INFO | train_inner | epoch 057:    184 / 561 symm_kl=15.404, loss=4.427, nll_loss=0.874, ppl=1.83, wps=23390, ups=2.24, wpb=10445.2, bsz=392.2, num_updates=31600, lr=1.25e-05, gnorm=4.524, train_wall=44, wall=15629
2020-12-10 15:42:19 | INFO | train_inner | epoch 057:    284 / 561 symm_kl=16.321, loss=4.521, nll_loss=0.87, ppl=1.83, wps=23384.9, ups=2.23, wpb=10478.3, bsz=359.1, num_updates=31700, lr=1.25e-05, gnorm=4.872, train_wall=45, wall=15673
2020-12-10 15:43:03 | INFO | train_inner | epoch 057:    384 / 561 symm_kl=16.028, loss=4.497, nll_loss=0.874, ppl=1.83, wps=23431.4, ups=2.23, wpb=10493.2, bsz=368.3, num_updates=31800, lr=1.25e-05, gnorm=4.744, train_wall=45, wall=15718
2020-12-10 15:43:48 | INFO | train_inner | epoch 057:    484 / 561 symm_kl=15.928, loss=4.485, nll_loss=0.871, ppl=1.83, wps=23108.1, ups=2.22, wpb=10407.4, bsz=357.6, num_updates=31900, lr=1.25e-05, gnorm=4.644, train_wall=45, wall=15763
2020-12-10 15:44:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:44:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:44:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:44:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:44:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:44:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:44:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:44:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:44:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:44:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:44:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:44:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:44:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:44:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:44:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:44:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:44:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:44:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:44:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:44:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:44:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:44:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:44:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:44:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:44:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:44:42 | INFO | valid | epoch 057 | valid on 'valid' subset | symm_kl 0 | loss 6.781 | nll_loss 4.892 | ppl 29.7 | bleu 21.06 | wps 5108.5 | wpb 7508.5 | bsz 272.7 | num_updates 31977 | best_bleu 21.47
2020-12-10 15:44:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:44:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:44:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:44:45 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 57 @ 31977 updates, score 21.06) (writing took 2.8428004812449217 seconds)
2020-12-10 15:44:45 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2020-12-10 15:44:45 | INFO | train | epoch 057 | symm_kl 15.796 | loss 4.467 | nll_loss 0.87 | ppl 1.83 | wps 21301.7 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 31977 | lr 1.25e-05 | gnorm 4.627 | train_wall 250 | wall 15820
2020-12-10 15:44:45 | INFO | fairseq.trainer | begin training epoch 58
2020-12-10 15:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:44:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:44:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:44:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:44:58 | INFO | train_inner | epoch 058:     23 / 561 symm_kl=15.85, loss=4.478, nll_loss=0.88, ppl=1.84, wps=14924.6, ups=1.44, wpb=10387.5, bsz=370.2, num_updates=32000, lr=1.25e-05, gnorm=4.667, train_wall=44, wall=15833
2020-12-10 15:45:43 | INFO | train_inner | epoch 058:    123 / 561 symm_kl=15.909, loss=4.477, nll_loss=0.865, ppl=1.82, wps=23697.4, ups=2.23, wpb=10622.3, bsz=367.5, num_updates=32100, lr=1.25e-05, gnorm=4.627, train_wall=45, wall=15878
2020-12-10 15:46:28 | INFO | train_inner | epoch 058:    223 / 561 symm_kl=15.672, loss=4.455, nll_loss=0.871, ppl=1.83, wps=22902.7, ups=2.21, wpb=10364.9, bsz=362, num_updates=32200, lr=1.25e-05, gnorm=4.585, train_wall=45, wall=15923
2020-12-10 15:47:13 | INFO | train_inner | epoch 058:    323 / 561 symm_kl=16.463, loss=4.55, nll_loss=0.884, ppl=1.85, wps=23375.9, ups=2.22, wpb=10512, bsz=357.4, num_updates=32300, lr=1.25e-05, gnorm=4.741, train_wall=45, wall=15968
2020-12-10 15:47:58 | INFO | train_inner | epoch 058:    423 / 561 symm_kl=15.146, loss=4.393, nll_loss=0.86, ppl=1.82, wps=23555.3, ups=2.24, wpb=10515.2, bsz=376.7, num_updates=32400, lr=1.25e-05, gnorm=4.427, train_wall=44, wall=16013
2020-12-10 15:48:43 | INFO | train_inner | epoch 058:    523 / 561 symm_kl=14.978, loss=4.387, nll_loss=0.876, ppl=1.84, wps=23397, ups=2.22, wpb=10538.4, bsz=397, num_updates=32500, lr=1.25e-05, gnorm=4.3, train_wall=45, wall=16058
2020-12-10 15:49:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:49:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:49:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:49:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:49:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:49:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:49:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:49:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:49:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:49:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:49:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:49:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:49:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:49:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:49:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:49:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:49:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:49:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:49:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:49:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:49:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:49:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:49:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:49:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:49:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:49:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:49:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:49:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:49:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:49:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:49:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:49:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:49:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:49:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:49:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:49:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:49:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:49:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:49:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:49:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:49:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:49:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:49:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:49:20 | INFO | valid | epoch 058 | valid on 'valid' subset | symm_kl 0 | loss 6.785 | nll_loss 4.897 | ppl 29.79 | bleu 21.03 | wps 4723.7 | wpb 7508.5 | bsz 272.7 | num_updates 32538 | best_bleu 21.47
2020-12-10 15:49:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:49:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:49:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:49:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:49:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:49:23 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 58 @ 32538 updates, score 21.03) (writing took 2.7703746277838945 seconds)
2020-12-10 15:49:23 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2020-12-10 15:49:23 | INFO | train | epoch 058 | symm_kl 15.763 | loss 4.467 | nll_loss 0.873 | ppl 1.83 | wps 21168.1 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 32538 | lr 1.25e-05 | gnorm 4.592 | train_wall 251 | wall 16098
2020-12-10 15:49:23 | INFO | fairseq.trainer | begin training epoch 59
2020-12-10 15:49:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:49:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:49:53 | INFO | train_inner | epoch 059:     62 / 561 symm_kl=16.073, loss=4.501, nll_loss=0.871, ppl=1.83, wps=14821.1, ups=1.42, wpb=10450, bsz=355.6, num_updates=32600, lr=1.25e-05, gnorm=4.699, train_wall=44, wall=16128
2020-12-10 15:50:38 | INFO | train_inner | epoch 059:    162 / 561 symm_kl=15.286, loss=4.414, nll_loss=0.869, ppl=1.83, wps=23216.6, ups=2.23, wpb=10403.5, bsz=374.9, num_updates=32700, lr=1.25e-05, gnorm=4.436, train_wall=45, wall=16173
2020-12-10 15:51:23 | INFO | train_inner | epoch 059:    262 / 561 symm_kl=16.102, loss=4.505, nll_loss=0.879, ppl=1.84, wps=23384.2, ups=2.21, wpb=10558.1, bsz=377.4, num_updates=32800, lr=1.25e-05, gnorm=4.771, train_wall=45, wall=16218
2020-12-10 15:52:08 | INFO | train_inner | epoch 059:    362 / 561 symm_kl=16.065, loss=4.503, nll_loss=0.877, ppl=1.84, wps=23194.1, ups=2.21, wpb=10495.9, bsz=363.3, num_updates=32900, lr=1.25e-05, gnorm=4.691, train_wall=45, wall=16263
2020-12-10 15:52:53 | INFO | train_inner | epoch 059:    462 / 561 symm_kl=15.391, loss=4.43, nll_loss=0.873, ppl=1.83, wps=23549.1, ups=2.24, wpb=10496.3, bsz=373.1, num_updates=33000, lr=1.25e-05, gnorm=4.545, train_wall=44, wall=16308
2020-12-10 15:53:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:53:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:53:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:53:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:53:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:53:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:53:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:53:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:53:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:53:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:53:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:53:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:53:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:53:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:53:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:53:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:53:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:53:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:53:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:53:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:53:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:53:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:53:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:53:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:53:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:53:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:53:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:53:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:53:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:53:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:53:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:53:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:53:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:53:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:53:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:53:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:53:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:53:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:53:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:53:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:53:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:53:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:53:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:53:57 | INFO | valid | epoch 059 | valid on 'valid' subset | symm_kl 0 | loss 6.798 | nll_loss 4.908 | ppl 30.02 | bleu 20.97 | wps 4936.2 | wpb 7508.5 | bsz 272.7 | num_updates 33099 | best_bleu 21.47
2020-12-10 15:53:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:53:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:53:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:54:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:54:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:54:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 59 @ 33099 updates, score 20.97) (writing took 2.947367211803794 seconds)
2020-12-10 15:54:00 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2020-12-10 15:54:00 | INFO | train | epoch 059 | symm_kl 15.699 | loss 4.461 | nll_loss 0.874 | ppl 1.83 | wps 21196.9 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 33099 | lr 1.25e-05 | gnorm 4.62 | train_wall 251 | wall 16375
2020-12-10 15:54:00 | INFO | fairseq.trainer | begin training epoch 60
2020-12-10 15:54:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:54:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:54:04 | INFO | train_inner | epoch 060:      1 / 561 symm_kl=15.623, loss=4.448, nll_loss=0.869, ppl=1.83, wps=14700.3, ups=1.41, wpb=10435.2, bsz=365.5, num_updates=33100, lr=1.25e-05, gnorm=4.69, train_wall=45, wall=16379
2020-12-10 15:54:48 | INFO | train_inner | epoch 060:    101 / 561 symm_kl=14.956, loss=4.387, nll_loss=0.873, ppl=1.83, wps=23705.9, ups=2.26, wpb=10478.6, bsz=375.5, num_updates=33200, lr=1.25e-05, gnorm=4.282, train_wall=44, wall=16423
2020-12-10 15:55:33 | INFO | train_inner | epoch 060:    201 / 561 symm_kl=15.424, loss=4.433, nll_loss=0.876, ppl=1.83, wps=23126, ups=2.22, wpb=10429.5, bsz=377.5, num_updates=33300, lr=1.25e-05, gnorm=4.514, train_wall=45, wall=16468
2020-12-10 15:56:18 | INFO | train_inner | epoch 060:    301 / 561 symm_kl=16.095, loss=4.509, nll_loss=0.88, ppl=1.84, wps=23350.2, ups=2.23, wpb=10464.7, bsz=362.2, num_updates=33400, lr=1.25e-05, gnorm=4.723, train_wall=45, wall=16513
2020-12-10 15:57:03 | INFO | train_inner | epoch 060:    401 / 561 symm_kl=16.299, loss=4.531, nll_loss=0.881, ppl=1.84, wps=23371, ups=2.21, wpb=10587.7, bsz=366.4, num_updates=33500, lr=1.25e-05, gnorm=4.871, train_wall=45, wall=16558
2020-12-10 15:57:48 | INFO | train_inner | epoch 060:    501 / 561 symm_kl=15.251, loss=4.403, nll_loss=0.861, ppl=1.82, wps=23449, ups=2.22, wpb=10549.9, bsz=378.7, num_updates=33600, lr=1.25e-05, gnorm=4.525, train_wall=45, wall=16603
2020-12-10 15:58:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 15:58:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:58:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:58:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:58:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:58:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:58:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:58:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:58:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:58:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:58:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:58:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:58:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:58:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:58:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:58:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:58:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:58:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:58:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:58:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:58:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:58:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:58:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:58:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:58:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:58:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:58:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:58:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:58:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:58:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:58:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:58:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:58:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:58:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:58:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 15:58:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 15:58:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 15:58:35 | INFO | valid | epoch 060 | valid on 'valid' subset | symm_kl 0 | loss 6.801 | nll_loss 4.913 | ppl 30.12 | bleu 21.02 | wps 4875.7 | wpb 7508.5 | bsz 272.7 | num_updates 33660 | best_bleu 21.47
2020-12-10 15:58:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 15:58:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:58:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:58:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 60 @ 33660 updates, score 21.02) (writing took 2.437396114692092 seconds)
2020-12-10 15:58:37 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2020-12-10 15:58:37 | INFO | train | epoch 060 | symm_kl 15.662 | loss 4.459 | nll_loss 0.875 | ppl 1.83 | wps 21229.5 | ups 2.03 | wpb 10483.4 | bsz 369.6 | num_updates 33660 | lr 1.25e-05 | gnorm 4.612 | train_wall 251 | wall 16652
2020-12-10 15:58:37 | INFO | fairseq.trainer | begin training epoch 61
2020-12-10 15:58:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:58:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:58:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 15:58:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 15:58:58 | INFO | train_inner | epoch 061:     40 / 561 symm_kl=15.744, loss=4.471, nll_loss=0.88, ppl=1.84, wps=14920.4, ups=1.44, wpb=10360.3, bsz=361, num_updates=33700, lr=1.25e-05, gnorm=4.728, train_wall=44, wall=16673
2020-12-10 15:59:43 | INFO | train_inner | epoch 061:    140 / 561 symm_kl=15.769, loss=4.472, nll_loss=0.874, ppl=1.83, wps=23430, ups=2.23, wpb=10486.8, bsz=369.2, num_updates=33800, lr=1.25e-05, gnorm=4.659, train_wall=45, wall=16717
2020-12-10 16:00:28 | INFO | train_inner | epoch 061:    240 / 561 symm_kl=15.245, loss=4.421, nll_loss=0.878, ppl=1.84, wps=23553, ups=2.22, wpb=10615.5, bsz=377.6, num_updates=33900, lr=1.25e-05, gnorm=4.391, train_wall=45, wall=16763
2020-12-10 16:01:12 | INFO | train_inner | epoch 061:    340 / 561 symm_kl=15.669, loss=4.466, nll_loss=0.88, ppl=1.84, wps=23508.9, ups=2.25, wpb=10468.7, bsz=367.3, num_updates=34000, lr=1.25e-05, gnorm=4.526, train_wall=44, wall=16807
2020-12-10 16:01:58 | INFO | train_inner | epoch 061:    440 / 561 symm_kl=15.632, loss=4.458, nll_loss=0.877, ppl=1.84, wps=23001.7, ups=2.21, wpb=10426.4, bsz=365.7, num_updates=34100, lr=1.25e-05, gnorm=4.627, train_wall=45, wall=16852
2020-12-10 16:02:43 | INFO | train_inner | epoch 061:    540 / 561 symm_kl=15.757, loss=4.472, nll_loss=0.88, ppl=1.84, wps=23213.4, ups=2.21, wpb=10514.1, bsz=372.9, num_updates=34200, lr=1.25e-05, gnorm=4.61, train_wall=45, wall=16898
2020-12-10 16:02:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:02:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:02:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:02:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:02:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:02:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:02:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:02:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:02:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:02:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:02:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:02:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:02:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:02:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:02:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:02:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:02:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:02:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:02:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:02:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:02:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:02:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:02:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:02:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:02:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:02:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:02:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:02:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:02:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:02:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:02:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:02:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:02:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:02:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:03:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:03:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:03:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:03:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:03:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:03:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:03:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:03:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:03:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:03:12 | INFO | valid | epoch 061 | valid on 'valid' subset | symm_kl 0 | loss 6.791 | nll_loss 4.904 | ppl 29.93 | bleu 20.85 | wps 4778 | wpb 7508.5 | bsz 272.7 | num_updates 34221 | best_bleu 21.47
2020-12-10 16:03:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:03:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:03:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:03:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:03:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:03:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 61 @ 34221 updates, score 20.85) (writing took 2.916446754708886 seconds)
2020-12-10 16:03:15 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2020-12-10 16:03:15 | INFO | train | epoch 061 | symm_kl 15.606 | loss 4.456 | nll_loss 0.878 | ppl 1.84 | wps 21162.7 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 34221 | lr 1.25e-05 | gnorm 4.572 | train_wall 251 | wall 16930
2020-12-10 16:03:15 | INFO | fairseq.trainer | begin training epoch 62
2020-12-10 16:03:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:03:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:03:53 | INFO | train_inner | epoch 062:     79 / 561 symm_kl=16.386, loss=4.543, nll_loss=0.882, ppl=1.84, wps=14871.3, ups=1.42, wpb=10497.8, bsz=353.3, num_updates=34300, lr=1.25e-05, gnorm=4.88, train_wall=44, wall=16968
2020-12-10 16:04:39 | INFO | train_inner | epoch 062:    179 / 561 symm_kl=15.195, loss=4.412, nll_loss=0.875, ppl=1.83, wps=23536.9, ups=2.21, wpb=10633.5, bsz=373, num_updates=34400, lr=1.25e-05, gnorm=4.433, train_wall=45, wall=17013
2020-12-10 16:05:24 | INFO | train_inner | epoch 062:    279 / 561 symm_kl=15.564, loss=4.45, nll_loss=0.874, ppl=1.83, wps=23088.8, ups=2.22, wpb=10412, bsz=372.6, num_updates=34500, lr=1.25e-05, gnorm=4.64, train_wall=45, wall=17059
2020-12-10 16:06:09 | INFO | train_inner | epoch 062:    379 / 561 symm_kl=14.989, loss=4.386, nll_loss=0.871, ppl=1.83, wps=23175.3, ups=2.23, wpb=10412.9, bsz=374.4, num_updates=34600, lr=1.25e-05, gnorm=4.4, train_wall=45, wall=17103
2020-12-10 16:06:54 | INFO | train_inner | epoch 062:    479 / 561 symm_kl=15.475, loss=4.453, nll_loss=0.892, ppl=1.86, wps=23108.8, ups=2.21, wpb=10445.4, bsz=371.2, num_updates=34700, lr=1.25e-05, gnorm=4.436, train_wall=45, wall=17149
2020-12-10 16:07:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:07:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:07:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:07:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:07:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:07:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:07:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:07:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:07:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:07:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:07:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:07:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:07:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:07:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:07:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:07:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:07:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:07:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:07:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:07:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:07:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:07:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:07:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:07:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:07:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:07:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:07:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:07:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:07:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:07:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:07:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:07:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:07:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:07:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:07:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:07:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:07:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:07:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:07:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:07:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:07:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:07:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:07:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:07:52 | INFO | valid | epoch 062 | valid on 'valid' subset | symm_kl 0 | loss 6.795 | nll_loss 4.908 | ppl 30.02 | bleu 20.92 | wps 4618.4 | wpb 7508.5 | bsz 272.7 | num_updates 34782 | best_bleu 21.47
2020-12-10 16:07:52 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:07:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:07:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:07:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 62 @ 34782 updates, score 20.92) (writing took 2.841065203770995 seconds)
2020-12-10 16:07:54 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2020-12-10 16:07:54 | INFO | train | epoch 062 | symm_kl 15.572 | loss 4.454 | nll_loss 0.878 | ppl 1.84 | wps 21069.9 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 34782 | lr 1.25e-05 | gnorm 4.583 | train_wall 251 | wall 17209
2020-12-10 16:07:54 | INFO | fairseq.trainer | begin training epoch 63
2020-12-10 16:07:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:07:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:07:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:07:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:08:05 | INFO | train_inner | epoch 063:     18 / 561 symm_kl=15.49, loss=4.444, nll_loss=0.877, ppl=1.84, wps=14626.3, ups=1.4, wpb=10464.1, bsz=379.4, num_updates=34800, lr=1.25e-05, gnorm=4.611, train_wall=45, wall=17220
2020-12-10 16:08:50 | INFO | train_inner | epoch 063:    118 / 561 symm_kl=16.035, loss=4.502, nll_loss=0.876, ppl=1.84, wps=23696.8, ups=2.25, wpb=10514.5, bsz=360.4, num_updates=34900, lr=1.25e-05, gnorm=4.748, train_wall=44, wall=17265
2020-12-10 16:09:35 | INFO | train_inner | epoch 063:    218 / 561 symm_kl=15.347, loss=4.435, nll_loss=0.884, ppl=1.85, wps=23326, ups=2.23, wpb=10448.6, bsz=367.5, num_updates=35000, lr=1.25e-05, gnorm=4.446, train_wall=45, wall=17309
2020-12-10 16:10:19 | INFO | train_inner | epoch 063:    318 / 561 symm_kl=15.413, loss=4.439, nll_loss=0.879, ppl=1.84, wps=23475.6, ups=2.23, wpb=10523.2, bsz=366.6, num_updates=35100, lr=1.25e-05, gnorm=4.517, train_wall=45, wall=17354
2020-12-10 16:11:04 | INFO | train_inner | epoch 063:    418 / 561 symm_kl=15.906, loss=4.49, nll_loss=0.879, ppl=1.84, wps=23423.3, ups=2.22, wpb=10538, bsz=366.8, num_updates=35200, lr=1.25e-05, gnorm=4.746, train_wall=45, wall=17399
2020-12-10 16:11:49 | INFO | train_inner | epoch 063:    518 / 561 symm_kl=15.67, loss=4.474, nll_loss=0.889, ppl=1.85, wps=23577.6, ups=2.25, wpb=10491.9, bsz=373.4, num_updates=35300, lr=1.25e-05, gnorm=4.711, train_wall=44, wall=17444
2020-12-10 16:12:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:12:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:12:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:12:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:12:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:12:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:12:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:12:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:12:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:12:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:12:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:12:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:12:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:12:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:12:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:12:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:12:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:12:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:12:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:12:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:12:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:12:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:12:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:12:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:12:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:12:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:12:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:12:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:12:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:12:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:12:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:12:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:12:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:12:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:12:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:12:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:12:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:12:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:12:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:12:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:12:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:12:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:12:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:12:29 | INFO | valid | epoch 063 | valid on 'valid' subset | symm_kl 0 | loss 6.804 | nll_loss 4.915 | ppl 30.17 | bleu 20.99 | wps 4606.2 | wpb 7508.5 | bsz 272.7 | num_updates 35343 | best_bleu 21.47
2020-12-10 16:12:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:12:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:12:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:12:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:12:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:12:32 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 63 @ 35343 updates, score 20.99) (writing took 2.8310519699007273 seconds)
2020-12-10 16:12:32 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2020-12-10 16:12:32 | INFO | train | epoch 063 | symm_kl 15.519 | loss 4.45 | nll_loss 0.88 | ppl 1.84 | wps 21198.9 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 35343 | lr 1.25e-05 | gnorm 4.582 | train_wall 250 | wall 17487
2020-12-10 16:12:32 | INFO | fairseq.trainer | begin training epoch 64
2020-12-10 16:12:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:12:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:13:00 | INFO | train_inner | epoch 064:     57 / 561 symm_kl=15.243, loss=4.426, nll_loss=0.883, ppl=1.84, wps=14576.3, ups=1.41, wpb=10339.6, bsz=362.5, num_updates=35400, lr=1.25e-05, gnorm=4.499, train_wall=44, wall=17515
2020-12-10 16:13:45 | INFO | train_inner | epoch 064:    157 / 561 symm_kl=15.581, loss=4.46, nll_loss=0.881, ppl=1.84, wps=23512.5, ups=2.23, wpb=10522.8, bsz=362.6, num_updates=35500, lr=1.25e-05, gnorm=4.633, train_wall=45, wall=17559
2020-12-10 16:14:29 | INFO | train_inner | epoch 064:    257 / 561 symm_kl=16.417, loss=4.551, nll_loss=0.891, ppl=1.85, wps=23112.3, ups=2.22, wpb=10387.9, bsz=367.2, num_updates=35600, lr=1.25e-05, gnorm=4.97, train_wall=45, wall=17604
2020-12-10 16:15:14 | INFO | train_inner | epoch 064:    357 / 561 symm_kl=14.589, loss=4.346, nll_loss=0.871, ppl=1.83, wps=23454.1, ups=2.23, wpb=10513.1, bsz=392.1, num_updates=35700, lr=1.25e-05, gnorm=4.291, train_wall=45, wall=17649
2020-12-10 16:15:59 | INFO | train_inner | epoch 064:    457 / 561 symm_kl=15.307, loss=4.43, nll_loss=0.881, ppl=1.84, wps=23390.8, ups=2.23, wpb=10501.8, bsz=368, num_updates=35800, lr=1.25e-05, gnorm=4.467, train_wall=45, wall=17694
2020-12-10 16:16:44 | INFO | train_inner | epoch 064:    557 / 561 symm_kl=15.517, loss=4.446, nll_loss=0.875, ppl=1.83, wps=23300.7, ups=2.21, wpb=10548.3, bsz=367.3, num_updates=35900, lr=1.25e-05, gnorm=4.628, train_wall=45, wall=17739
2020-12-10 16:16:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:16:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:16:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:16:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:16:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:16:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:16:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:16:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:16:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:16:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:16:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:16:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:16:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:16:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:16:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:16:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:16:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:16:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:16:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:16:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:16:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:16:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:16:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:16:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:16:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:16:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:16:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:16:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:16:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:16:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:16:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:16:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:16:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:16:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:16:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:16:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:16:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:16:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:16:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:16:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:17:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:17:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:17:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:17:06 | INFO | valid | epoch 064 | valid on 'valid' subset | symm_kl 0 | loss 6.794 | nll_loss 4.904 | ppl 29.95 | bleu 21 | wps 4915.1 | wpb 7508.5 | bsz 272.7 | num_updates 35904 | best_bleu 21.47
2020-12-10 16:17:06 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:17:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:17:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:17:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:17:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:17:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 64 @ 35904 updates, score 21.0) (writing took 2.963682521134615 seconds)
2020-12-10 16:17:09 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2020-12-10 16:17:09 | INFO | train | epoch 064 | symm_kl 15.484 | loss 4.449 | nll_loss 0.882 | ppl 1.84 | wps 21215.2 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 35904 | lr 1.25e-05 | gnorm 4.598 | train_wall 250 | wall 17764
2020-12-10 16:17:09 | INFO | fairseq.trainer | begin training epoch 65
2020-12-10 16:17:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:17:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:17:55 | INFO | train_inner | epoch 065:     96 / 561 symm_kl=15.2, loss=4.424, nll_loss=0.888, ppl=1.85, wps=14910.7, ups=1.43, wpb=10442.2, bsz=385.2, num_updates=36000, lr=1.25e-05, gnorm=4.572, train_wall=44, wall=17809
2020-12-10 16:18:40 | INFO | train_inner | epoch 065:    196 / 561 symm_kl=15.397, loss=4.431, nll_loss=0.872, ppl=1.83, wps=22877.3, ups=2.2, wpb=10390.9, bsz=367.8, num_updates=36100, lr=1.25e-05, gnorm=4.601, train_wall=45, wall=17855
2020-12-10 16:19:25 | INFO | train_inner | epoch 065:    296 / 561 symm_kl=15.026, loss=4.393, nll_loss=0.871, ppl=1.83, wps=23742.6, ups=2.23, wpb=10633.9, bsz=373.8, num_updates=36200, lr=1.25e-05, gnorm=4.399, train_wall=45, wall=17900
2020-12-10 16:20:10 | INFO | train_inner | epoch 065:    396 / 561 symm_kl=16.061, loss=4.513, nll_loss=0.887, ppl=1.85, wps=23424.4, ups=2.21, wpb=10589.7, bsz=359.9, num_updates=36300, lr=1.25e-05, gnorm=4.835, train_wall=45, wall=17945
2020-12-10 16:20:55 | INFO | train_inner | epoch 065:    496 / 561 symm_kl=15.152, loss=4.422, nll_loss=0.888, ppl=1.85, wps=23349.5, ups=2.24, wpb=10411.5, bsz=371.7, num_updates=36400, lr=1.25e-05, gnorm=4.437, train_wall=44, wall=17989
2020-12-10 16:21:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:21:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:21:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:21:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:21:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:21:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:21:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:21:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:21:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:21:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:21:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:21:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:21:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:21:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:21:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:21:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:21:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:21:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:21:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:21:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:21:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:21:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:21:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:21:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:21:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:21:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:21:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:21:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:21:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:21:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:21:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:21:45 | INFO | valid | epoch 065 | valid on 'valid' subset | symm_kl 0 | loss 6.798 | nll_loss 4.908 | ppl 30.02 | bleu 20.99 | wps 4440.8 | wpb 7508.5 | bsz 272.7 | num_updates 36465 | best_bleu 21.47
2020-12-10 16:21:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:21:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:21:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:21:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:21:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:21:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 65 @ 36465 updates, score 20.99) (writing took 3.0067296233028173 seconds)
2020-12-10 16:21:48 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2020-12-10 16:21:48 | INFO | train | epoch 065 | symm_kl 15.445 | loss 4.446 | nll_loss 0.883 | ppl 1.84 | wps 21096.9 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 36465 | lr 1.25e-05 | gnorm 4.578 | train_wall 250 | wall 18043
2020-12-10 16:21:48 | INFO | fairseq.trainer | begin training epoch 66
2020-12-10 16:21:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:21:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:22:06 | INFO | train_inner | epoch 066:     35 / 561 symm_kl=15.892, loss=4.502, nll_loss=0.897, ppl=1.86, wps=14576.9, ups=1.4, wpb=10428.6, bsz=361.8, num_updates=36500, lr=1.25e-05, gnorm=4.717, train_wall=44, wall=18061
2020-12-10 16:22:51 | INFO | train_inner | epoch 066:    135 / 561 symm_kl=15.998, loss=4.515, nll_loss=0.893, ppl=1.86, wps=23478.8, ups=2.23, wpb=10551.3, bsz=360.9, num_updates=36600, lr=1.25e-05, gnorm=4.751, train_wall=45, wall=18106
2020-12-10 16:23:36 | INFO | train_inner | epoch 066:    235 / 561 symm_kl=15.174, loss=4.424, nll_loss=0.888, ppl=1.85, wps=23498.3, ups=2.23, wpb=10558.2, bsz=374.5, num_updates=36700, lr=1.25e-05, gnorm=4.466, train_wall=45, wall=18151
2020-12-10 16:24:21 | INFO | train_inner | epoch 066:    335 / 561 symm_kl=15.023, loss=4.391, nll_loss=0.869, ppl=1.83, wps=23101.3, ups=2.23, wpb=10351.5, bsz=377.3, num_updates=36800, lr=1.25e-05, gnorm=4.487, train_wall=45, wall=18196
2020-12-10 16:25:06 | INFO | train_inner | epoch 066:    435 / 561 symm_kl=15.688, loss=4.473, nll_loss=0.884, ppl=1.85, wps=23380.7, ups=2.21, wpb=10574.7, bsz=362.6, num_updates=36900, lr=1.25e-05, gnorm=4.723, train_wall=45, wall=18241
2020-12-10 16:25:51 | INFO | train_inner | epoch 066:    535 / 561 symm_kl=15.076, loss=4.41, nll_loss=0.886, ppl=1.85, wps=23181.5, ups=2.23, wpb=10376.2, bsz=370.6, num_updates=37000, lr=1.25e-05, gnorm=4.462, train_wall=45, wall=18286
2020-12-10 16:26:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:26:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:26:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:26:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:26:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:26:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:26:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:26:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:26:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:26:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:26:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:26:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:26:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:26:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:26:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:26:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:26:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:26:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:26:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:26:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:26:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:26:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:26:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:26:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:26:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:26:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:26:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:26:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:26:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:26:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:26:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:26:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:26:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:26:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:26:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:26:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:26:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:26:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:26:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:26:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:26:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:26:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:26:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:26:25 | INFO | valid | epoch 066 | valid on 'valid' subset | symm_kl 0 | loss 6.802 | nll_loss 4.914 | ppl 30.15 | bleu 21.04 | wps 4102.9 | wpb 7508.5 | bsz 272.7 | num_updates 37026 | best_bleu 21.47
2020-12-10 16:26:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:26:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:26:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:26:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:26:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:26:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 66 @ 37026 updates, score 21.04) (writing took 2.93261837400496 seconds)
2020-12-10 16:26:28 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2020-12-10 16:26:28 | INFO | train | epoch 066 | symm_kl 15.389 | loss 4.442 | nll_loss 0.884 | ppl 1.85 | wps 20990.8 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 37026 | lr 1.25e-05 | gnorm 4.585 | train_wall 251 | wall 18323
2020-12-10 16:26:28 | INFO | fairseq.trainer | begin training epoch 67
2020-12-10 16:26:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:26:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:27:03 | INFO | train_inner | epoch 067:     74 / 561 symm_kl=15.186, loss=4.421, nll_loss=0.883, ppl=1.84, wps=14343.7, ups=1.38, wpb=10408.4, bsz=367.5, num_updates=37100, lr=1.25e-05, gnorm=4.543, train_wall=44, wall=18358
2020-12-10 16:27:48 | INFO | train_inner | epoch 067:    174 / 561 symm_kl=14.432, loss=4.329, nll_loss=0.868, ppl=1.83, wps=23456.9, ups=2.24, wpb=10488.1, bsz=388.6, num_updates=37200, lr=1.25e-05, gnorm=4.266, train_wall=45, wall=18403
2020-12-10 16:28:33 | INFO | train_inner | epoch 067:    274 / 561 symm_kl=15.92, loss=4.497, nll_loss=0.887, ppl=1.85, wps=23178.1, ups=2.21, wpb=10468.2, bsz=370.1, num_updates=37300, lr=1.25e-05, gnorm=4.94, train_wall=45, wall=18448
2020-12-10 16:29:18 | INFO | train_inner | epoch 067:    374 / 561 symm_kl=15.315, loss=4.44, nll_loss=0.892, ppl=1.86, wps=23395.6, ups=2.22, wpb=10527.3, bsz=366.1, num_updates=37400, lr=1.25e-05, gnorm=4.534, train_wall=45, wall=18493
2020-12-10 16:30:03 | INFO | train_inner | epoch 067:    474 / 561 symm_kl=15.657, loss=4.479, nll_loss=0.894, ppl=1.86, wps=23484.5, ups=2.23, wpb=10514.6, bsz=365, num_updates=37500, lr=1.25e-05, gnorm=4.629, train_wall=45, wall=18538
2020-12-10 16:30:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:30:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:30:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:30:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:30:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:30:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:30:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:30:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:30:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:30:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:30:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:30:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:30:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:30:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:30:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:30:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:30:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:30:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:30:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:30:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:30:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:30:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:30:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:30:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:30:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:30:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:30:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:30:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:30:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:30:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:30:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:30:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:30:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:30:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:30:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:30:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:30:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:30:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:30:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:30:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:31:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:31:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:31:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:31:02 | INFO | valid | epoch 067 | valid on 'valid' subset | symm_kl 0 | loss 6.801 | nll_loss 4.913 | ppl 30.12 | bleu 21 | wps 4688.1 | wpb 7508.5 | bsz 272.7 | num_updates 37587 | best_bleu 21.47
2020-12-10 16:31:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:31:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:31:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:31:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:31:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:31:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 67 @ 37587 updates, score 21.0) (writing took 2.88310563005507 seconds)
2020-12-10 16:31:05 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2020-12-10 16:31:05 | INFO | train | epoch 067 | symm_kl 15.35 | loss 4.439 | nll_loss 0.885 | ppl 1.85 | wps 21219.4 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 37587 | lr 1.25e-05 | gnorm 4.591 | train_wall 250 | wall 18600
2020-12-10 16:31:05 | INFO | fairseq.trainer | begin training epoch 68
2020-12-10 16:31:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:31:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:31:14 | INFO | train_inner | epoch 068:     13 / 561 symm_kl=15.489, loss=4.453, nll_loss=0.881, ppl=1.84, wps=14719.3, ups=1.41, wpb=10445.2, bsz=355, num_updates=37600, lr=1.25e-05, gnorm=4.542, train_wall=44, wall=18609
2020-12-10 16:31:58 | INFO | train_inner | epoch 068:    113 / 561 symm_kl=15.163, loss=4.415, nll_loss=0.884, ppl=1.84, wps=23517.6, ups=2.25, wpb=10458.4, bsz=378.4, num_updates=37700, lr=1.25e-05, gnorm=4.561, train_wall=44, wall=18653
2020-12-10 16:32:43 | INFO | train_inner | epoch 068:    213 / 561 symm_kl=15.252, loss=4.436, nll_loss=0.889, ppl=1.85, wps=23471.1, ups=2.24, wpb=10483.5, bsz=364.9, num_updates=37800, lr=1.25e-05, gnorm=4.511, train_wall=45, wall=18698
2020-12-10 16:33:28 | INFO | train_inner | epoch 068:    313 / 561 symm_kl=14.595, loss=4.346, nll_loss=0.87, ppl=1.83, wps=23720.8, ups=2.23, wpb=10660.2, bsz=384.1, num_updates=37900, lr=1.25e-05, gnorm=4.346, train_wall=45, wall=18743
2020-12-10 16:34:13 | INFO | train_inner | epoch 068:    413 / 561 symm_kl=15.641, loss=4.473, nll_loss=0.888, ppl=1.85, wps=23586, ups=2.23, wpb=10576.3, bsz=362.6, num_updates=38000, lr=1.25e-05, gnorm=4.634, train_wall=45, wall=18788
2020-12-10 16:34:57 | INFO | train_inner | epoch 068:    513 / 561 symm_kl=15.496, loss=4.465, nll_loss=0.895, ppl=1.86, wps=23214.5, ups=2.25, wpb=10328.1, bsz=366, num_updates=38100, lr=1.25e-05, gnorm=4.69, train_wall=44, wall=18832
2020-12-10 16:35:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:35:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:35:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:35:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:35:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:35:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:35:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:35:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:35:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:35:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:35:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:35:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:35:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:35:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:35:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:35:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:35:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:35:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:35:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:35:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:35:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:35:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:35:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:35:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:35:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:35:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:35:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:35:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:35:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:35:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:35:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:35:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:35:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:35:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:35:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:35:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:35:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:35:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:35:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:35:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:35:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:35:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:35:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:35:40 | INFO | valid | epoch 068 | valid on 'valid' subset | symm_kl 0 | loss 6.795 | nll_loss 4.909 | ppl 30.05 | bleu 20.97 | wps 4449.4 | wpb 7508.5 | bsz 272.7 | num_updates 38148 | best_bleu 21.47
2020-12-10 16:35:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:35:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:35:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:35:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:35:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:35:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 68 @ 38148 updates, score 20.97) (writing took 2.826191322878003 seconds)
2020-12-10 16:35:43 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2020-12-10 16:35:43 | INFO | train | epoch 068 | symm_kl 15.31 | loss 4.436 | nll_loss 0.886 | ppl 1.85 | wps 21172.5 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 38148 | lr 1.25e-05 | gnorm 4.583 | train_wall 250 | wall 18878
2020-12-10 16:35:43 | INFO | fairseq.trainer | begin training epoch 69
2020-12-10 16:35:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:35:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:36:09 | INFO | train_inner | epoch 069:     52 / 561 symm_kl=15.315, loss=4.433, nll_loss=0.883, ppl=1.84, wps=14679.1, ups=1.4, wpb=10454.2, bsz=367.4, num_updates=38200, lr=1.25e-05, gnorm=4.634, train_wall=44, wall=18903
2020-12-10 16:36:53 | INFO | train_inner | epoch 069:    152 / 561 symm_kl=15.687, loss=4.486, nll_loss=0.899, ppl=1.86, wps=23631.4, ups=2.24, wpb=10531.5, bsz=373, num_updates=38300, lr=1.25e-05, gnorm=4.738, train_wall=44, wall=18948
2020-12-10 16:37:38 | INFO | train_inner | epoch 069:    252 / 561 symm_kl=14.976, loss=4.401, nll_loss=0.882, ppl=1.84, wps=23299.3, ups=2.22, wpb=10472.5, bsz=367.8, num_updates=38400, lr=1.25e-05, gnorm=4.457, train_wall=45, wall=18993
2020-12-10 16:38:23 | INFO | train_inner | epoch 069:    352 / 561 symm_kl=15.409, loss=4.454, nll_loss=0.891, ppl=1.85, wps=23583.5, ups=2.25, wpb=10493.4, bsz=367.2, num_updates=38500, lr=1.25e-05, gnorm=4.558, train_wall=44, wall=19037
2020-12-10 16:39:08 | INFO | train_inner | epoch 069:    452 / 561 symm_kl=14.788, loss=4.375, nll_loss=0.878, ppl=1.84, wps=23435.9, ups=2.22, wpb=10544.1, bsz=381.3, num_updates=38600, lr=1.25e-05, gnorm=4.367, train_wall=45, wall=19082
2020-12-10 16:39:53 | INFO | train_inner | epoch 069:    552 / 561 symm_kl=15.766, loss=4.495, nll_loss=0.9, ppl=1.87, wps=22939.3, ups=2.21, wpb=10394.4, bsz=360.3, num_updates=38700, lr=1.25e-05, gnorm=4.677, train_wall=45, wall=19128
2020-12-10 16:39:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:39:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:39:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:39:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:39:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:39:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:40:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:40:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:40:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:40:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:40:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:40:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:40:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:40:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:40:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:40:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:40:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:40:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:40:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:40:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:40:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:40:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:40:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:40:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:40:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:40:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:40:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:40:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:40:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:40:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:40:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:40:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:40:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:40:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:40:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:40:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:40:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:40:18 | INFO | valid | epoch 069 | valid on 'valid' subset | symm_kl 0 | loss 6.798 | nll_loss 4.909 | ppl 30.04 | bleu 20.94 | wps 4529.2 | wpb 7508.5 | bsz 272.7 | num_updates 38709 | best_bleu 21.47
2020-12-10 16:40:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:40:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:40:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:40:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:40:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:40:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 69 @ 38709 updates, score 20.94) (writing took 3.1330250315368176 seconds)
2020-12-10 16:40:21 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2020-12-10 16:40:21 | INFO | train | epoch 069 | symm_kl 15.258 | loss 4.433 | nll_loss 0.888 | ppl 1.85 | wps 21143.6 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 38709 | lr 1.25e-05 | gnorm 4.541 | train_wall 250 | wall 19156
2020-12-10 16:40:21 | INFO | fairseq.trainer | begin training epoch 70
2020-12-10 16:40:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:40:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:41:04 | INFO | train_inner | epoch 070:     91 / 561 symm_kl=15.288, loss=4.449, nll_loss=0.901, ppl=1.87, wps=14650.9, ups=1.4, wpb=10471.9, bsz=362.6, num_updates=38800, lr=1.25e-05, gnorm=4.53, train_wall=44, wall=19199
2020-12-10 16:41:50 | INFO | train_inner | epoch 070:    191 / 561 symm_kl=15.102, loss=4.415, nll_loss=0.885, ppl=1.85, wps=23177.6, ups=2.21, wpb=10493.2, bsz=368.2, num_updates=38900, lr=1.25e-05, gnorm=4.521, train_wall=45, wall=19244
2020-12-10 16:42:35 | INFO | train_inner | epoch 070:    291 / 561 symm_kl=15.187, loss=4.43, nll_loss=0.893, ppl=1.86, wps=23109.2, ups=2.22, wpb=10413.5, bsz=369.7, num_updates=39000, lr=1.25e-05, gnorm=4.555, train_wall=45, wall=19290
2020-12-10 16:43:20 | INFO | train_inner | epoch 070:    391 / 561 symm_kl=15.048, loss=4.413, nll_loss=0.886, ppl=1.85, wps=23482.4, ups=2.21, wpb=10611.3, bsz=374.6, num_updates=39100, lr=1.25e-05, gnorm=4.443, train_wall=45, wall=19335
2020-12-10 16:44:05 | INFO | train_inner | epoch 070:    491 / 561 symm_kl=15.268, loss=4.428, nll_loss=0.883, ppl=1.84, wps=23263.6, ups=2.23, wpb=10424.4, bsz=374.2, num_updates=39200, lr=1.25e-05, gnorm=4.685, train_wall=45, wall=19380
2020-12-10 16:44:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:44:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:44:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:44:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:44:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:44:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:44:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:44:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:44:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:44:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:44:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:44:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:44:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:44:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:44:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:44:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:44:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:44:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:44:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:44:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:44:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:44:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:44:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:44:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:44:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:44:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:44:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:44:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:44:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:44:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:44:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:44:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:44:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:44:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:44:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:44:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:44:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:44:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:44:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:44:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:44:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:44:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:44:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:44:57 | INFO | valid | epoch 070 | valid on 'valid' subset | symm_kl 0 | loss 6.802 | nll_loss 4.913 | ppl 30.13 | bleu 20.86 | wps 4501.4 | wpb 7508.5 | bsz 272.7 | num_updates 39270 | best_bleu 21.47
2020-12-10 16:44:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:44:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:44:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:45:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:45:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:45:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 70 @ 39270 updates, score 20.86) (writing took 3.090233540162444 seconds)
2020-12-10 16:45:00 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2020-12-10 16:45:00 | INFO | train | epoch 070 | symm_kl 15.208 | loss 4.43 | nll_loss 0.889 | ppl 1.85 | wps 21064.6 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 39270 | lr 1.25e-05 | gnorm 4.552 | train_wall 251 | wall 19435
2020-12-10 16:45:00 | INFO | fairseq.trainer | begin training epoch 71
2020-12-10 16:45:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:45:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:45:16 | INFO | train_inner | epoch 071:     30 / 561 symm_kl=15.272, loss=4.435, nll_loss=0.887, ppl=1.85, wps=14544, ups=1.39, wpb=10441.2, bsz=373.1, num_updates=39300, lr=1.25e-05, gnorm=4.608, train_wall=44, wall=19451
2020-12-10 16:46:01 | INFO | train_inner | epoch 071:    130 / 561 symm_kl=15.218, loss=4.44, nll_loss=0.897, ppl=1.86, wps=23046.5, ups=2.23, wpb=10333.6, bsz=365.5, num_updates=39400, lr=1.25e-05, gnorm=4.512, train_wall=45, wall=19496
2020-12-10 16:46:47 | INFO | train_inner | epoch 071:    230 / 561 symm_kl=14.737, loss=4.384, nll_loss=0.891, ppl=1.85, wps=23382, ups=2.21, wpb=10592.5, bsz=380.7, num_updates=39500, lr=1.25e-05, gnorm=4.362, train_wall=45, wall=19541
2020-12-10 16:47:32 | INFO | train_inner | epoch 071:    330 / 561 symm_kl=15.936, loss=4.518, nll_loss=0.903, ppl=1.87, wps=23374.3, ups=2.21, wpb=10569.6, bsz=348.3, num_updates=39600, lr=1.25e-05, gnorm=4.726, train_wall=45, wall=19587
2020-12-10 16:48:17 | INFO | train_inner | epoch 071:    430 / 561 symm_kl=14.63, loss=4.355, nll_loss=0.875, ppl=1.83, wps=23254.9, ups=2.21, wpb=10513.7, bsz=392, num_updates=39700, lr=1.25e-05, gnorm=4.407, train_wall=45, wall=19632
2020-12-10 16:49:02 | INFO | train_inner | epoch 071:    530 / 561 symm_kl=15.457, loss=4.458, nll_loss=0.892, ppl=1.86, wps=23244, ups=2.23, wpb=10444.7, bsz=359.2, num_updates=39800, lr=1.25e-05, gnorm=4.69, train_wall=45, wall=19677
2020-12-10 16:49:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:49:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:49:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:49:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:49:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:49:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:49:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:49:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:49:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:49:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:49:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:49:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:49:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:49:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:49:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:49:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:49:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:49:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:49:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:49:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:49:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:49:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:49:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:49:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:49:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:49:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:49:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:49:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:49:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:49:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:49:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:49:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:49:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:49:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:49:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:49:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:49:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:49:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:49:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:49:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:49:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:49:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:49:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:49:36 | INFO | valid | epoch 071 | valid on 'valid' subset | symm_kl 0 | loss 6.799 | nll_loss 4.913 | ppl 30.13 | bleu 21.05 | wps 4848.4 | wpb 7508.5 | bsz 272.7 | num_updates 39831 | best_bleu 21.47
2020-12-10 16:49:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:49:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:49:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:49:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:49:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:49:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 71 @ 39831 updates, score 21.05) (writing took 3.093205327168107 seconds)
2020-12-10 16:49:39 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2020-12-10 16:49:39 | INFO | train | epoch 071 | symm_kl 15.142 | loss 4.424 | nll_loss 0.89 | ppl 1.85 | wps 21111.9 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 39831 | lr 1.25e-05 | gnorm 4.534 | train_wall 251 | wall 19714
2020-12-10 16:49:39 | INFO | fairseq.trainer | begin training epoch 72
2020-12-10 16:49:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:49:41 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:50:13 | INFO | train_inner | epoch 072:     69 / 561 symm_kl=15.243, loss=4.437, nll_loss=0.892, ppl=1.86, wps=14896.2, ups=1.42, wpb=10508.7, bsz=355.4, num_updates=39900, lr=1.25e-05, gnorm=4.585, train_wall=44, wall=19747
2020-12-10 16:50:58 | INFO | train_inner | epoch 072:    169 / 561 symm_kl=15.085, loss=4.415, nll_loss=0.887, ppl=1.85, wps=22914.1, ups=2.21, wpb=10350.7, bsz=372.3, num_updates=40000, lr=1.25e-05, gnorm=4.593, train_wall=45, wall=19793
2020-12-10 16:51:43 | INFO | train_inner | epoch 072:    269 / 561 symm_kl=15.417, loss=4.459, nll_loss=0.898, ppl=1.86, wps=23153.7, ups=2.23, wpb=10387, bsz=363.1, num_updates=40100, lr=1.25e-05, gnorm=4.617, train_wall=45, wall=19837
2020-12-10 16:52:28 | INFO | train_inner | epoch 072:    369 / 561 symm_kl=14.362, loss=4.325, nll_loss=0.873, ppl=1.83, wps=23351.6, ups=2.2, wpb=10607, bsz=388.2, num_updates=40200, lr=1.25e-05, gnorm=4.301, train_wall=45, wall=19883
2020-12-10 16:53:13 | INFO | train_inner | epoch 072:    469 / 561 symm_kl=14.786, loss=4.392, nll_loss=0.897, ppl=1.86, wps=23073.4, ups=2.21, wpb=10425.4, bsz=385, num_updates=40300, lr=1.25e-05, gnorm=4.456, train_wall=45, wall=19928
2020-12-10 16:53:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:53:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:53:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:53:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:53:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:53:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:53:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:53:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:53:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:53:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:54:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:54:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:54:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:54:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:54:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:54:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:54:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:54:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:54:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:54:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:54:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:54:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:54:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:54:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:54:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:54:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:54:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:54:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:54:02 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:54:02 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:54:02 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:54:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:54:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:54:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:54:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:54:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:54:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:54:15 | INFO | valid | epoch 072 | valid on 'valid' subset | symm_kl 0 | loss 6.804 | nll_loss 4.915 | ppl 30.17 | bleu 21.01 | wps 4722 | wpb 7508.5 | bsz 272.7 | num_updates 40392 | best_bleu 21.47
2020-12-10 16:54:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:54:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:54:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:54:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:54:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:54:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 72 @ 40392 updates, score 21.01) (writing took 3.3736479356884956 seconds)
2020-12-10 16:54:18 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2020-12-10 16:54:18 | INFO | train | epoch 072 | symm_kl 15.124 | loss 4.424 | nll_loss 0.892 | ppl 1.86 | wps 21046 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 40392 | lr 1.25e-05 | gnorm 4.53 | train_wall 251 | wall 19993
2020-12-10 16:54:18 | INFO | fairseq.trainer | begin training epoch 73
2020-12-10 16:54:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:54:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:54:25 | INFO | train_inner | epoch 073:      8 / 561 symm_kl=15.853, loss=4.511, nll_loss=0.902, ppl=1.87, wps=14660, ups=1.39, wpb=10518.1, bsz=347, num_updates=40400, lr=1.25e-05, gnorm=4.68, train_wall=45, wall=20000
2020-12-10 16:55:09 | INFO | train_inner | epoch 073:    108 / 561 symm_kl=14.745, loss=4.385, nll_loss=0.891, ppl=1.85, wps=23997.4, ups=2.26, wpb=10615.7, bsz=376.9, num_updates=40500, lr=1.25e-05, gnorm=4.442, train_wall=44, wall=20044
2020-12-10 16:55:54 | INFO | train_inner | epoch 073:    208 / 561 symm_kl=14.669, loss=4.365, nll_loss=0.879, ppl=1.84, wps=23322.6, ups=2.21, wpb=10565.9, bsz=374.8, num_updates=40600, lr=1.25e-05, gnorm=4.361, train_wall=45, wall=20089
2020-12-10 16:56:40 | INFO | train_inner | epoch 073:    308 / 561 symm_kl=15.446, loss=4.469, nll_loss=0.902, ppl=1.87, wps=23442.2, ups=2.21, wpb=10629.2, bsz=366.6, num_updates=40700, lr=1.25e-05, gnorm=4.641, train_wall=45, wall=20135
2020-12-10 16:57:24 | INFO | train_inner | epoch 073:    408 / 561 symm_kl=14.921, loss=4.405, nll_loss=0.894, ppl=1.86, wps=23166.4, ups=2.24, wpb=10341.6, bsz=368.4, num_updates=40800, lr=1.25e-05, gnorm=4.487, train_wall=44, wall=20179
2020-12-10 16:58:09 | INFO | train_inner | epoch 073:    508 / 561 symm_kl=15.447, loss=4.466, nll_loss=0.903, ppl=1.87, wps=23034.6, ups=2.22, wpb=10367.5, bsz=359.3, num_updates=40900, lr=1.25e-05, gnorm=4.692, train_wall=45, wall=20224
2020-12-10 16:58:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 16:58:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:58:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:58:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:58:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:58:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:58:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:58:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:58:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:58:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:58:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:58:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:58:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:58:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:58:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:58:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:58:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:58:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:58:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:58:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:58:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:58:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:58:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:58:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:58:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:58:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:58:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:58:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:58:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:58:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:58:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:58:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:58:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:58:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:58:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:58:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:58:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:58:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 16:58:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 16:58:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 16:58:53 | INFO | valid | epoch 073 | valid on 'valid' subset | symm_kl 0 | loss 6.801 | nll_loss 4.913 | ppl 30.13 | bleu 20.97 | wps 4964.7 | wpb 7508.5 | bsz 272.7 | num_updates 40953 | best_bleu 21.47
2020-12-10 16:58:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 16:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:58:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:58:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:58:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:58:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 73 @ 40953 updates, score 20.97) (writing took 3.1706609334796667 seconds)
2020-12-10 16:58:56 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2020-12-10 16:58:56 | INFO | train | epoch 073 | symm_kl 15.087 | loss 4.421 | nll_loss 0.893 | ppl 1.86 | wps 21183.8 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 40953 | lr 1.25e-05 | gnorm 4.557 | train_wall 251 | wall 20271
2020-12-10 16:58:56 | INFO | fairseq.trainer | begin training epoch 74
2020-12-10 16:58:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 16:58:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 16:59:20 | INFO | train_inner | epoch 074:     47 / 561 symm_kl=14.752, loss=4.372, nll_loss=0.877, ppl=1.84, wps=14875.8, ups=1.42, wpb=10444.4, bsz=376.2, num_updates=41000, lr=1.25e-05, gnorm=4.499, train_wall=44, wall=20294
2020-12-10 17:00:05 | INFO | train_inner | epoch 074:    147 / 561 symm_kl=15.196, loss=4.428, nll_loss=0.889, ppl=1.85, wps=23536.4, ups=2.23, wpb=10565, bsz=376.7, num_updates=41100, lr=1.25e-05, gnorm=4.644, train_wall=45, wall=20339
2020-12-10 17:00:50 | INFO | train_inner | epoch 074:    247 / 561 symm_kl=15.175, loss=4.442, nll_loss=0.903, ppl=1.87, wps=23464.5, ups=2.21, wpb=10624.5, bsz=373.2, num_updates=41200, lr=1.25e-05, gnorm=4.609, train_wall=45, wall=20385
2020-12-10 17:01:35 | INFO | train_inner | epoch 074:    347 / 561 symm_kl=15.134, loss=4.43, nll_loss=0.896, ppl=1.86, wps=23046.6, ups=2.22, wpb=10362.3, bsz=363.1, num_updates=41300, lr=1.25e-05, gnorm=4.647, train_wall=45, wall=20430
2020-12-10 17:02:20 | INFO | train_inner | epoch 074:    447 / 561 symm_kl=14.903, loss=4.402, nll_loss=0.892, ppl=1.86, wps=23048.7, ups=2.22, wpb=10359.4, bsz=367.3, num_updates=41400, lr=1.25e-05, gnorm=4.502, train_wall=45, wall=20475
2020-12-10 17:03:05 | INFO | train_inner | epoch 074:    547 / 561 symm_kl=14.999, loss=4.418, nll_loss=0.9, ppl=1.87, wps=23432.6, ups=2.23, wpb=10505.4, bsz=370.6, num_updates=41500, lr=1.25e-05, gnorm=4.463, train_wall=45, wall=20519
2020-12-10 17:03:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:03:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:03:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:03:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:03:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:03:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:03:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:03:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:03:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:03:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:03:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:03:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:03:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:03:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:03:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:03:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:03:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:03:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:03:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:03:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:03:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:03:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:03:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:03:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:03:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:03:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:03:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:03:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:03:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:03:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:03:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:03:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:03:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:03:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:03:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:03:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:03:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:03:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:03:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:03:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:03:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:03:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:03:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:03:32 | INFO | valid | epoch 074 | valid on 'valid' subset | symm_kl 0 | loss 6.801 | nll_loss 4.912 | ppl 30.11 | bleu 20.84 | wps 4635.9 | wpb 7508.5 | bsz 272.7 | num_updates 41514 | best_bleu 21.47
2020-12-10 17:03:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:03:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:03:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:03:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:03:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:03:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 74 @ 41514 updates, score 20.84) (writing took 3.0604724511504173 seconds)
2020-12-10 17:03:35 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2020-12-10 17:03:35 | INFO | train | epoch 074 | symm_kl 15.051 | loss 4.419 | nll_loss 0.894 | ppl 1.86 | wps 21098.4 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 41514 | lr 1.25e-05 | gnorm 4.551 | train_wall 251 | wall 20550
2020-12-10 17:03:35 | INFO | fairseq.trainer | begin training epoch 75
2020-12-10 17:03:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:03:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:04:16 | INFO | train_inner | epoch 075:     86 / 561 symm_kl=14.213, loss=4.323, nll_loss=0.885, ppl=1.85, wps=14685.6, ups=1.4, wpb=10464.7, bsz=388.8, num_updates=41600, lr=1.25e-05, gnorm=4.2, train_wall=44, wall=20591
2020-12-10 17:05:01 | INFO | train_inner | epoch 075:    186 / 561 symm_kl=14.507, loss=4.362, nll_loss=0.896, ppl=1.86, wps=23498.7, ups=2.22, wpb=10574.6, bsz=392.5, num_updates=41700, lr=1.25e-05, gnorm=4.311, train_wall=45, wall=20636
2020-12-10 17:05:46 | INFO | train_inner | epoch 075:    286 / 561 symm_kl=15.363, loss=4.454, nll_loss=0.897, ppl=1.86, wps=23412.6, ups=2.24, wpb=10467.8, bsz=360.9, num_updates=41800, lr=1.25e-05, gnorm=4.616, train_wall=45, wall=20680
2020-12-10 17:06:31 | INFO | train_inner | epoch 075:    386 / 561 symm_kl=15.577, loss=4.485, nll_loss=0.903, ppl=1.87, wps=22947.9, ups=2.19, wpb=10464.8, bsz=347.8, num_updates=41900, lr=1.25e-05, gnorm=4.688, train_wall=45, wall=20726
2020-12-10 17:07:16 | INFO | train_inner | epoch 075:    486 / 561 symm_kl=15.307, loss=4.454, nll_loss=0.898, ppl=1.86, wps=23222, ups=2.22, wpb=10478.3, bsz=357.8, num_updates=42000, lr=1.25e-05, gnorm=4.624, train_wall=45, wall=20771
2020-12-10 17:07:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:07:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:07:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:07:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:07:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:07:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:07:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:07:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:07:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:07:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:07:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:07:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:07:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:07:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:07:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:07:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:07:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:07:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:07:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:07:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:07:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:07:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:07:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:07:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:07:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:07:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:07:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:07:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:07:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:07:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:07:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:08:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:08:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:08:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:08:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:08:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:08:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:08:11 | INFO | valid | epoch 075 | valid on 'valid' subset | symm_kl 0 | loss 6.816 | nll_loss 4.928 | ppl 30.44 | bleu 20.97 | wps 4624.1 | wpb 7508.5 | bsz 272.7 | num_updates 42075 | best_bleu 21.47
2020-12-10 17:08:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:08:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:08:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:08:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:08:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:08:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 75 @ 42075 updates, score 20.97) (writing took 3.3317088913172483 seconds)
2020-12-10 17:08:15 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2020-12-10 17:08:15 | INFO | train | epoch 075 | symm_kl 15.001 | loss 4.416 | nll_loss 0.896 | ppl 1.86 | wps 21010 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 42075 | lr 1.25e-05 | gnorm 4.505 | train_wall 252 | wall 20829
2020-12-10 17:08:15 | INFO | fairseq.trainer | begin training epoch 76
2020-12-10 17:08:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:08:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:08:29 | INFO | train_inner | epoch 076:     25 / 561 symm_kl=15.555, loss=4.477, nll_loss=0.902, ppl=1.87, wps=14328.3, ups=1.38, wpb=10380.1, bsz=367, num_updates=42100, lr=1.25e-05, gnorm=4.855, train_wall=45, wall=20844
2020-12-10 17:09:14 | INFO | train_inner | epoch 076:    125 / 561 symm_kl=14.093, loss=4.309, nll_loss=0.881, ppl=1.84, wps=23372.9, ups=2.23, wpb=10501, bsz=372.2, num_updates=42200, lr=1.25e-05, gnorm=4.165, train_wall=45, wall=20888
2020-12-10 17:09:59 | INFO | train_inner | epoch 076:    225 / 561 symm_kl=14.954, loss=4.405, nll_loss=0.887, ppl=1.85, wps=23238, ups=2.2, wpb=10552.4, bsz=362.9, num_updates=42300, lr=1.25e-05, gnorm=4.544, train_wall=45, wall=20934
2020-12-10 17:10:44 | INFO | train_inner | epoch 076:    325 / 561 symm_kl=14.944, loss=4.417, nll_loss=0.904, ppl=1.87, wps=23228.3, ups=2.21, wpb=10497.6, bsz=376, num_updates=42400, lr=1.25e-05, gnorm=4.543, train_wall=45, wall=20979
2020-12-10 17:11:29 | INFO | train_inner | epoch 076:    425 / 561 symm_kl=15.698, loss=4.497, nll_loss=0.902, ppl=1.87, wps=23225.8, ups=2.23, wpb=10433.3, bsz=351.4, num_updates=42500, lr=1.25e-05, gnorm=4.752, train_wall=45, wall=21024
2020-12-10 17:12:14 | INFO | train_inner | epoch 076:    525 / 561 symm_kl=14.755, loss=4.403, nll_loss=0.912, ppl=1.88, wps=23401.1, ups=2.21, wpb=10580.1, bsz=382.4, num_updates=42600, lr=1.25e-05, gnorm=4.352, train_wall=45, wall=21069
2020-12-10 17:12:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:12:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:12:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:12:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:12:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:12:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:12:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:12:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:12:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:12:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:12:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:12:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:12:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:12:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:12:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:12:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:12:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:12:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:12:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:12:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:12:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:12:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:12:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:12:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:12:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:12:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:12:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:12:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:12:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:12:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:12:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:12:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:12:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:12:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:12:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:12:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:12:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:12:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:12:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:12:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:12:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:12:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:12:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:12:51 | INFO | valid | epoch 076 | valid on 'valid' subset | symm_kl 0 | loss 6.806 | nll_loss 4.92 | ppl 30.27 | bleu 20.94 | wps 4542.3 | wpb 7508.5 | bsz 272.7 | num_updates 42636 | best_bleu 21.47
2020-12-10 17:12:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:12:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:12:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:12:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:12:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:12:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 76 @ 42636 updates, score 20.94) (writing took 3.165910292416811 seconds)
2020-12-10 17:12:55 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2020-12-10 17:12:55 | INFO | train | epoch 076 | symm_kl 14.937 | loss 4.411 | nll_loss 0.897 | ppl 1.86 | wps 21004.3 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 42636 | lr 1.25e-05 | gnorm 4.522 | train_wall 252 | wall 21109
2020-12-10 17:12:55 | INFO | fairseq.trainer | begin training epoch 77
2020-12-10 17:12:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:12:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:13:26 | INFO | train_inner | epoch 077:     64 / 561 symm_kl=15.207, loss=4.442, nll_loss=0.902, ppl=1.87, wps=14418, ups=1.4, wpb=10305.6, bsz=368.2, num_updates=42700, lr=1.25e-05, gnorm=4.691, train_wall=44, wall=21141
2020-12-10 17:14:11 | INFO | train_inner | epoch 077:    164 / 561 symm_kl=14.968, loss=4.417, nll_loss=0.898, ppl=1.86, wps=23319.3, ups=2.22, wpb=10521, bsz=369.5, num_updates=42800, lr=1.25e-05, gnorm=4.49, train_wall=45, wall=21186
2020-12-10 17:14:56 | INFO | train_inner | epoch 077:    264 / 561 symm_kl=15.373, loss=4.464, nll_loss=0.905, ppl=1.87, wps=23115.7, ups=2.2, wpb=10506.2, bsz=360.5, num_updates=42900, lr=1.25e-05, gnorm=4.656, train_wall=45, wall=21231
2020-12-10 17:15:42 | INFO | train_inner | epoch 077:    364 / 561 symm_kl=14.662, loss=4.371, nll_loss=0.884, ppl=1.85, wps=23427.6, ups=2.21, wpb=10604.1, bsz=379.8, num_updates=43000, lr=1.25e-05, gnorm=4.527, train_wall=45, wall=21277
2020-12-10 17:16:27 | INFO | train_inner | epoch 077:    464 / 561 symm_kl=14.536, loss=4.371, nll_loss=0.899, ppl=1.87, wps=23215, ups=2.22, wpb=10478.7, bsz=373.4, num_updates=43100, lr=1.25e-05, gnorm=4.322, train_wall=45, wall=21322
2020-12-10 17:17:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:17:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:17:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:17:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:17:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:17:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:17:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:17:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:17:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:17:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:17:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:17:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:17:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:17:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:17:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:17:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:17:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:17:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:17:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:17:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:17:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:17:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:17:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:17:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:17:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:17:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:17:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:17:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:17:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:17:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:17:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:17:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:17:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:17:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:17:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:17:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:17:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:17:32 | INFO | valid | epoch 077 | valid on 'valid' subset | symm_kl 0 | loss 6.811 | nll_loss 4.925 | ppl 30.37 | bleu 20.93 | wps 4504.7 | wpb 7508.5 | bsz 272.7 | num_updates 43197 | best_bleu 21.47
2020-12-10 17:17:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:17:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:17:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:17:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 77 @ 43197 updates, score 20.93) (writing took 3.192911807447672 seconds)
2020-12-10 17:17:35 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2020-12-10 17:17:35 | INFO | train | epoch 077 | symm_kl 14.932 | loss 4.411 | nll_loss 0.898 | ppl 1.86 | wps 20974.7 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 43197 | lr 1.25e-05 | gnorm 4.523 | train_wall 251 | wall 21390
2020-12-10 17:17:35 | INFO | fairseq.trainer | begin training epoch 78
2020-12-10 17:17:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:17:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:17:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:17:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:17:40 | INFO | train_inner | epoch 078:      3 / 561 symm_kl=14.932, loss=4.409, nll_loss=0.897, ppl=1.86, wps=14263.6, ups=1.37, wpb=10374.8, bsz=364.6, num_updates=43200, lr=1.25e-05, gnorm=4.546, train_wall=45, wall=21394
2020-12-10 17:18:23 | INFO | train_inner | epoch 078:    103 / 561 symm_kl=15.341, loss=4.468, nll_loss=0.912, ppl=1.88, wps=23695.6, ups=2.28, wpb=10394.3, bsz=363, num_updates=43300, lr=1.25e-05, gnorm=4.725, train_wall=44, wall=21438
2020-12-10 17:19:09 | INFO | train_inner | epoch 078:    203 / 561 symm_kl=14.86, loss=4.404, nll_loss=0.897, ppl=1.86, wps=23101, ups=2.2, wpb=10520.7, bsz=365.6, num_updates=43400, lr=1.25e-05, gnorm=4.482, train_wall=45, wall=21484
2020-12-10 17:19:55 | INFO | train_inner | epoch 078:    303 / 561 symm_kl=14.325, loss=4.336, nll_loss=0.887, ppl=1.85, wps=23295.6, ups=2.19, wpb=10635.8, bsz=384.4, num_updates=43500, lr=1.25e-05, gnorm=4.32, train_wall=45, wall=21529
2020-12-10 17:20:40 | INFO | train_inner | epoch 078:    403 / 561 symm_kl=15.372, loss=4.462, nll_loss=0.903, ppl=1.87, wps=23073.2, ups=2.22, wpb=10394.3, bsz=358.9, num_updates=43600, lr=1.25e-05, gnorm=4.668, train_wall=45, wall=21575
2020-12-10 17:21:25 | INFO | train_inner | epoch 078:    503 / 561 symm_kl=14.712, loss=4.391, nll_loss=0.9, ppl=1.87, wps=23169.8, ups=2.21, wpb=10475.4, bsz=375.7, num_updates=43700, lr=1.25e-05, gnorm=4.514, train_wall=45, wall=21620
2020-12-10 17:21:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:21:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:21:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:21:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:21:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:21:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:21:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:21:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:21:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:21:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:21:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:21:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:21:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:21:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:21:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:21:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:21:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:21:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:21:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:21:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:21:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:21:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:21:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:21:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:21:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:21:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:21:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:21:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:21:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:21:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:21:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:21:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:21:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:21:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:22:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:22:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:22:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:22:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:22:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:22:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:22:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:22:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:22:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:22:12 | INFO | valid | epoch 078 | valid on 'valid' subset | symm_kl 0 | loss 6.806 | nll_loss 4.918 | ppl 30.22 | bleu 20.94 | wps 4850.3 | wpb 7508.5 | bsz 272.7 | num_updates 43758 | best_bleu 21.47
2020-12-10 17:22:12 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:22:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:22:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:22:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:22:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:22:15 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 78 @ 43758 updates, score 20.94) (writing took 3.262088142335415 seconds)
2020-12-10 17:22:15 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2020-12-10 17:22:15 | INFO | train | epoch 078 | symm_kl 14.906 | loss 4.41 | nll_loss 0.899 | ppl 1.87 | wps 21018.5 | ups 2 | wpb 10483.4 | bsz 369.6 | num_updates 43758 | lr 1.25e-05 | gnorm 4.535 | train_wall 252 | wall 21670
2020-12-10 17:22:15 | INFO | fairseq.trainer | begin training epoch 79
2020-12-10 17:22:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:22:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:22:36 | INFO | train_inner | epoch 079:     42 / 561 symm_kl=14.575, loss=4.365, nll_loss=0.889, ppl=1.85, wps=14634.5, ups=1.4, wpb=10444, bsz=375.5, num_updates=43800, lr=1.25e-05, gnorm=4.437, train_wall=45, wall=21691
2020-12-10 17:23:21 | INFO | train_inner | epoch 079:    142 / 561 symm_kl=15.21, loss=4.446, nll_loss=0.905, ppl=1.87, wps=23348.5, ups=2.22, wpb=10515.1, bsz=370.6, num_updates=43900, lr=1.25e-05, gnorm=4.684, train_wall=45, wall=21736
2020-12-10 17:24:07 | INFO | train_inner | epoch 079:    242 / 561 symm_kl=14.635, loss=4.384, nll_loss=0.902, ppl=1.87, wps=23104, ups=2.21, wpb=10473, bsz=369.1, num_updates=44000, lr=1.25e-05, gnorm=4.465, train_wall=45, wall=21781
2020-12-10 17:24:52 | INFO | train_inner | epoch 079:    342 / 561 symm_kl=14.879, loss=4.41, nll_loss=0.902, ppl=1.87, wps=23470.1, ups=2.2, wpb=10677.2, bsz=375.4, num_updates=44100, lr=1.25e-05, gnorm=4.437, train_wall=45, wall=21827
2020-12-10 17:25:37 | INFO | train_inner | epoch 079:    442 / 561 symm_kl=15.153, loss=4.443, nll_loss=0.904, ppl=1.87, wps=23081.1, ups=2.22, wpb=10398.5, bsz=357.7, num_updates=44200, lr=1.25e-05, gnorm=4.589, train_wall=45, wall=21872
2020-12-10 17:26:22 | INFO | train_inner | epoch 079:    542 / 561 symm_kl=14.431, loss=4.353, nll_loss=0.891, ppl=1.85, wps=23164.2, ups=2.21, wpb=10479.9, bsz=375.9, num_updates=44300, lr=1.25e-05, gnorm=4.349, train_wall=45, wall=21917
2020-12-10 17:26:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:26:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:26:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:26:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:26:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:26:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:26:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:26:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:26:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:26:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:26:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:26:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:26:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:26:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:26:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:26:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:26:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:26:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:26:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:26:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:26:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:26:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:26:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:26:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:26:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:26:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:26:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:26:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:26:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:26:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:26:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:26:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:26:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:26:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:26:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:26:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:26:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:26:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:26:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:26:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:26:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:26:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:26:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:26:51 | INFO | valid | epoch 079 | valid on 'valid' subset | symm_kl 0 | loss 6.802 | nll_loss 4.916 | ppl 30.19 | bleu 20.97 | wps 4778.5 | wpb 7508.5 | bsz 272.7 | num_updates 44319 | best_bleu 21.47
2020-12-10 17:26:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:26:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:26:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:26:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:26:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:26:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 79 @ 44319 updates, score 20.97) (writing took 3.236094595864415 seconds)
2020-12-10 17:26:54 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2020-12-10 17:26:54 | INFO | train | epoch 079 | symm_kl 14.85 | loss 4.406 | nll_loss 0.9 | ppl 1.87 | wps 21036.1 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 44319 | lr 1.25e-05 | gnorm 4.517 | train_wall 252 | wall 21949
2020-12-10 17:26:54 | INFO | fairseq.trainer | begin training epoch 80
2020-12-10 17:26:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:26:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:27:33 | INFO | train_inner | epoch 080:     81 / 561 symm_kl=14.792, loss=4.407, nll_loss=0.908, ppl=1.88, wps=14817.2, ups=1.42, wpb=10460.9, bsz=371.8, num_updates=44400, lr=1.25e-05, gnorm=4.609, train_wall=44, wall=21988
2020-12-10 17:28:18 | INFO | train_inner | epoch 080:    181 / 561 symm_kl=14.819, loss=4.41, nll_loss=0.906, ppl=1.87, wps=23221.2, ups=2.22, wpb=10457.5, bsz=368.6, num_updates=44500, lr=1.25e-05, gnorm=4.434, train_wall=45, wall=22033
2020-12-10 17:29:03 | INFO | train_inner | epoch 080:    281 / 561 symm_kl=14.662, loss=4.384, nll_loss=0.898, ppl=1.86, wps=22761.6, ups=2.2, wpb=10345, bsz=372.7, num_updates=44600, lr=1.25e-05, gnorm=4.612, train_wall=45, wall=22078
2020-12-10 17:29:49 | INFO | train_inner | epoch 080:    381 / 561 symm_kl=14.646, loss=4.385, nll_loss=0.9, ppl=1.87, wps=23440.6, ups=2.21, wpb=10605.2, bsz=369.9, num_updates=44700, lr=1.25e-05, gnorm=4.386, train_wall=45, wall=22124
2020-12-10 17:30:34 | INFO | train_inner | epoch 080:    481 / 561 symm_kl=15.103, loss=4.433, nll_loss=0.904, ppl=1.87, wps=23417.6, ups=2.23, wpb=10511.1, bsz=370.2, num_updates=44800, lr=1.25e-05, gnorm=4.608, train_wall=45, wall=22168
2020-12-10 17:31:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:31:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:31:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:31:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:31:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:31:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:31:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:31:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:31:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:31:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:31:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:31:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:31:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:31:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:31:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:31:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:31:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:31:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:31:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:31:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:31:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:31:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:31:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:31:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:31:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:31:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:31:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:31:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:31:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:31:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:31:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:31:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:31:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:31:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:31:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:31:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:31:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:31:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:31:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:31:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:31:28 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:31:28 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:31:28 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:31:30 | INFO | valid | epoch 080 | valid on 'valid' subset | symm_kl 0 | loss 6.803 | nll_loss 4.917 | ppl 30.2 | bleu 21 | wps 4979.9 | wpb 7508.5 | bsz 272.7 | num_updates 44880 | best_bleu 21.47
2020-12-10 17:31:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:31:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:31:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:31:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:31:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:31:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 80 @ 44880 updates, score 21.0) (writing took 3.242003358900547 seconds)
2020-12-10 17:31:33 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2020-12-10 17:31:33 | INFO | train | epoch 080 | symm_kl 14.821 | loss 4.405 | nll_loss 0.902 | ppl 1.87 | wps 21126.1 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 44880 | lr 1.25e-05 | gnorm 4.531 | train_wall 251 | wall 22228
2020-12-10 17:31:33 | INFO | fairseq.trainer | begin training epoch 81
2020-12-10 17:31:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:31:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:31:45 | INFO | train_inner | epoch 081:     20 / 561 symm_kl=15.042, loss=4.428, nll_loss=0.901, ppl=1.87, wps=14698.1, ups=1.41, wpb=10443.5, bsz=355.5, num_updates=44900, lr=1.25e-05, gnorm=4.582, train_wall=45, wall=22240
2020-12-10 17:32:29 | INFO | train_inner | epoch 081:    120 / 561 symm_kl=15.108, loss=4.441, nll_loss=0.908, ppl=1.88, wps=23579.7, ups=2.24, wpb=10512.5, bsz=368, num_updates=45000, lr=1.25e-05, gnorm=4.546, train_wall=44, wall=22284
2020-12-10 17:33:14 | INFO | train_inner | epoch 081:    220 / 561 symm_kl=15.39, loss=4.474, nll_loss=0.914, ppl=1.88, wps=23315.5, ups=2.24, wpb=10417.8, bsz=369.2, num_updates=45100, lr=1.25e-05, gnorm=4.925, train_wall=45, wall=22329
2020-12-10 17:33:59 | INFO | train_inner | epoch 081:    320 / 561 symm_kl=14.781, loss=4.398, nll_loss=0.897, ppl=1.86, wps=23521.9, ups=2.21, wpb=10657.1, bsz=360.6, num_updates=45200, lr=1.25e-05, gnorm=4.418, train_wall=45, wall=22374
2020-12-10 17:34:44 | INFO | train_inner | epoch 081:    420 / 561 symm_kl=14.639, loss=4.398, nll_loss=0.915, ppl=1.89, wps=22940.9, ups=2.22, wpb=10333.5, bsz=368.4, num_updates=45300, lr=1.25e-05, gnorm=4.452, train_wall=45, wall=22419
2020-12-10 17:35:29 | INFO | train_inner | epoch 081:    520 / 561 symm_kl=14.091, loss=4.314, nll_loss=0.889, ppl=1.85, wps=23449.9, ups=2.22, wpb=10577, bsz=390.2, num_updates=45400, lr=1.25e-05, gnorm=4.281, train_wall=45, wall=22464
2020-12-10 17:35:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:35:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:35:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:35:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:35:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:35:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:35:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:35:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:35:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:35:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:35:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:35:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:35:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:35:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:35:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:35:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:35:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:35:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:35:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:35:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:35:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:35:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:35:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:35:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:35:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:35:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:35:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:35:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:35:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:35:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:35:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:35:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:35:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:35:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:35:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:35:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:35:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:35:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:35:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:35:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:36:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:36:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:36:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:36:09 | INFO | valid | epoch 081 | valid on 'valid' subset | symm_kl 0 | loss 6.806 | nll_loss 4.923 | ppl 30.34 | bleu 21.04 | wps 4610.9 | wpb 7508.5 | bsz 272.7 | num_updates 45441 | best_bleu 21.47
2020-12-10 17:36:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:36:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:36:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:36:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:36:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:36:12 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 81 @ 45441 updates, score 21.04) (writing took 3.0815763119608164 seconds)
2020-12-10 17:36:12 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2020-12-10 17:36:12 | INFO | train | epoch 081 | symm_kl 14.773 | loss 4.401 | nll_loss 0.903 | ppl 1.87 | wps 21086 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 45441 | lr 1.25e-05 | gnorm 4.503 | train_wall 251 | wall 22507
2020-12-10 17:36:12 | INFO | fairseq.trainer | begin training epoch 82
2020-12-10 17:36:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:36:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:36:41 | INFO | train_inner | epoch 082:     59 / 561 symm_kl=14.611, loss=4.384, nll_loss=0.904, ppl=1.87, wps=14524.9, ups=1.4, wpb=10343, bsz=363.6, num_updates=45500, lr=1.25e-05, gnorm=4.344, train_wall=44, wall=22535
2020-12-10 17:37:25 | INFO | train_inner | epoch 082:    159 / 561 symm_kl=15.015, loss=4.437, nll_loss=0.914, ppl=1.88, wps=23167.5, ups=2.24, wpb=10356.6, bsz=365.3, num_updates=45600, lr=1.25e-05, gnorm=4.63, train_wall=45, wall=22580
2020-12-10 17:38:11 | INFO | train_inner | epoch 082:    259 / 561 symm_kl=14.673, loss=4.394, nll_loss=0.903, ppl=1.87, wps=23454.9, ups=2.21, wpb=10608.7, bsz=365, num_updates=45700, lr=1.25e-05, gnorm=4.459, train_wall=45, wall=22625
2020-12-10 17:38:56 | INFO | train_inner | epoch 082:    359 / 561 symm_kl=14.613, loss=4.391, nll_loss=0.912, ppl=1.88, wps=23179.5, ups=2.22, wpb=10444.1, bsz=373, num_updates=45800, lr=1.25e-05, gnorm=4.468, train_wall=45, wall=22670
2020-12-10 17:39:41 | INFO | train_inner | epoch 082:    459 / 561 symm_kl=14.567, loss=4.372, nll_loss=0.895, ppl=1.86, wps=23239, ups=2.21, wpb=10493.7, bsz=377.4, num_updates=45900, lr=1.25e-05, gnorm=4.623, train_wall=45, wall=22716
2020-12-10 17:40:26 | INFO | train_inner | epoch 082:    559 / 561 symm_kl=14.52, loss=4.37, nll_loss=0.896, ppl=1.86, wps=23423.1, ups=2.2, wpb=10640.1, bsz=375.8, num_updates=46000, lr=1.25e-05, gnorm=4.483, train_wall=45, wall=22761
2020-12-10 17:40:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:40:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:40:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:40:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:40:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:40:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:40:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:40:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:40:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:40:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:40:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:40:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:40:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:40:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:40:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:40:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:40:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:40:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:40:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:40:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:40:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:40:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:40:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:40:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:40:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:40:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:40:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:40:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:40:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:40:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:40:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:40:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:40:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:40:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:40:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:40:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:40:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:40:37 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:40:37 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:40:37 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:40:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:40:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:40:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:40:47 | INFO | valid | epoch 082 | valid on 'valid' subset | symm_kl 0 | loss 6.81 | nll_loss 4.926 | ppl 30.39 | bleu 21.08 | wps 4743.8 | wpb 7508.5 | bsz 272.7 | num_updates 46002 | best_bleu 21.47
2020-12-10 17:40:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:40:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:40:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:40:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:40:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:40:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 82 @ 46002 updates, score 21.08) (writing took 3.0924865305423737 seconds)
2020-12-10 17:40:50 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2020-12-10 17:40:50 | INFO | train | epoch 082 | symm_kl 14.698 | loss 4.395 | nll_loss 0.905 | ppl 1.87 | wps 21097.5 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 46002 | lr 1.25e-05 | gnorm 4.521 | train_wall 251 | wall 22785
2020-12-10 17:40:50 | INFO | fairseq.trainer | begin training epoch 83
2020-12-10 17:40:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:40:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:41:37 | INFO | train_inner | epoch 083:     98 / 561 symm_kl=14.788, loss=4.405, nll_loss=0.906, ppl=1.87, wps=14665.4, ups=1.41, wpb=10408.4, bsz=368.8, num_updates=46100, lr=1.25e-05, gnorm=4.637, train_wall=44, wall=22832
2020-12-10 17:42:22 | INFO | train_inner | epoch 083:    198 / 561 symm_kl=15.13, loss=4.456, nll_loss=0.923, ppl=1.9, wps=23129.6, ups=2.22, wpb=10412.4, bsz=355.8, num_updates=46200, lr=1.25e-05, gnorm=4.634, train_wall=45, wall=22877
2020-12-10 17:43:07 | INFO | train_inner | epoch 083:    298 / 561 symm_kl=14.683, loss=4.397, nll_loss=0.907, ppl=1.87, wps=23234.3, ups=2.23, wpb=10401, bsz=371.8, num_updates=46300, lr=1.25e-05, gnorm=4.473, train_wall=45, wall=22922
2020-12-10 17:43:52 | INFO | train_inner | epoch 083:    398 / 561 symm_kl=14.101, loss=4.322, nll_loss=0.891, ppl=1.85, wps=23356.3, ups=2.22, wpb=10539.4, bsz=384.4, num_updates=46400, lr=1.25e-05, gnorm=4.414, train_wall=45, wall=22967
2020-12-10 17:44:37 | INFO | train_inner | epoch 083:    498 / 561 symm_kl=14.739, loss=4.399, nll_loss=0.905, ppl=1.87, wps=23523.1, ups=2.21, wpb=10649.7, bsz=367.7, num_updates=46500, lr=1.25e-05, gnorm=4.46, train_wall=45, wall=23012
2020-12-10 17:45:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:45:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:45:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:45:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:45:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:45:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:45:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:45:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:45:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:45:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:45:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:45:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:45:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:45:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:45:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:45:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:45:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:45:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:45:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:45:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:45:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:45:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:45:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:45:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:45:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:45:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:45:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:45:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:45:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:45:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:45:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:45:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:45:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:45:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:45:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:45:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:45:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:45:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:45:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:45:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:45:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:45:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:45:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:45:26 | INFO | valid | epoch 083 | valid on 'valid' subset | symm_kl 0 | loss 6.801 | nll_loss 4.914 | ppl 30.15 | bleu 21.09 | wps 4760.3 | wpb 7508.5 | bsz 272.7 | num_updates 46563 | best_bleu 21.47
2020-12-10 17:45:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:45:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:45:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:45:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:45:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:45:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 83 @ 46563 updates, score 21.09) (writing took 3.252570979297161 seconds)
2020-12-10 17:45:29 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2020-12-10 17:45:29 | INFO | train | epoch 083 | symm_kl 14.671 | loss 4.394 | nll_loss 0.906 | ppl 1.87 | wps 21092.5 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 46563 | lr 1.25e-05 | gnorm 4.505 | train_wall 251 | wall 23064
2020-12-10 17:45:29 | INFO | fairseq.trainer | begin training epoch 84
2020-12-10 17:45:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:45:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:45:49 | INFO | train_inner | epoch 084:     37 / 561 symm_kl=13.985, loss=4.306, nll_loss=0.891, ppl=1.85, wps=14681.7, ups=1.4, wpb=10474.8, bsz=382.2, num_updates=46600, lr=1.25e-05, gnorm=4.241, train_wall=44, wall=23084
2020-12-10 17:46:34 | INFO | train_inner | epoch 084:    137 / 561 symm_kl=14.313, loss=4.353, nll_loss=0.901, ppl=1.87, wps=23465.8, ups=2.23, wpb=10538.2, bsz=371.7, num_updates=46700, lr=1.25e-05, gnorm=4.392, train_wall=45, wall=23128
2020-12-10 17:47:19 | INFO | train_inner | epoch 084:    237 / 561 symm_kl=14.63, loss=4.39, nll_loss=0.907, ppl=1.88, wps=23045.6, ups=2.21, wpb=10430.9, bsz=371.7, num_updates=46800, lr=1.25e-05, gnorm=4.561, train_wall=45, wall=23174
2020-12-10 17:48:04 | INFO | train_inner | epoch 084:    337 / 561 symm_kl=14.824, loss=4.416, nll_loss=0.913, ppl=1.88, wps=23327.3, ups=2.21, wpb=10570, bsz=363.5, num_updates=46900, lr=1.25e-05, gnorm=4.502, train_wall=45, wall=23219
2020-12-10 17:48:50 | INFO | train_inner | epoch 084:    437 / 561 symm_kl=14.66, loss=4.394, nll_loss=0.907, ppl=1.87, wps=23165.1, ups=2.21, wpb=10505.2, bsz=367, num_updates=47000, lr=1.25e-05, gnorm=4.394, train_wall=45, wall=23264
2020-12-10 17:49:34 | INFO | train_inner | epoch 084:    537 / 561 symm_kl=15.291, loss=4.477, nll_loss=0.924, ppl=1.9, wps=23059.9, ups=2.23, wpb=10352.2, bsz=362.8, num_updates=47100, lr=1.25e-05, gnorm=4.811, train_wall=45, wall=23309
2020-12-10 17:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:49:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:49:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:49:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:49:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:49:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:49:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:49:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:49:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:49:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:49:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:49:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:49:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:49:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:49:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:49:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:49:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:49:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:49:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:49:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:49:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:49:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:49:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:49:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:49:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:49:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:49:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:49:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:49:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:49:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:49:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:49:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:49:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:49:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:49:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:49:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:49:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:49:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:49:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:49:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:50:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:50:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:50:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:50:05 | INFO | valid | epoch 084 | valid on 'valid' subset | symm_kl 0 | loss 6.809 | nll_loss 4.923 | ppl 30.34 | bleu 20.9 | wps 4809.3 | wpb 7508.5 | bsz 272.7 | num_updates 47124 | best_bleu 21.47
2020-12-10 17:50:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:50:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:50:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:50:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:50:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:50:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 84 @ 47124 updates, score 20.9) (writing took 3.1303476188331842 seconds)
2020-12-10 17:50:08 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2020-12-10 17:50:08 | INFO | train | epoch 084 | symm_kl 14.644 | loss 4.392 | nll_loss 0.907 | ppl 1.87 | wps 21070.7 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 47124 | lr 1.25e-05 | gnorm 4.509 | train_wall 251 | wall 23343
2020-12-10 17:50:08 | INFO | fairseq.trainer | begin training epoch 85
2020-12-10 17:50:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:50:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:50:45 | INFO | train_inner | epoch 085:     76 / 561 symm_kl=14.968, loss=4.43, nll_loss=0.908, ppl=1.88, wps=14863.7, ups=1.42, wpb=10479.2, bsz=360.5, num_updates=47200, lr=1.25e-05, gnorm=4.605, train_wall=44, wall=23380
2020-12-10 17:51:30 | INFO | train_inner | epoch 085:    176 / 561 symm_kl=14.626, loss=4.381, nll_loss=0.896, ppl=1.86, wps=23189.2, ups=2.2, wpb=10519.6, bsz=365.3, num_updates=47300, lr=1.25e-05, gnorm=4.546, train_wall=45, wall=23425
2020-12-10 17:52:15 | INFO | train_inner | epoch 085:    276 / 561 symm_kl=14.198, loss=4.348, nll_loss=0.912, ppl=1.88, wps=23074.4, ups=2.22, wpb=10411.3, bsz=378.6, num_updates=47400, lr=1.25e-05, gnorm=4.338, train_wall=45, wall=23470
2020-12-10 17:53:01 | INFO | train_inner | epoch 085:    376 / 561 symm_kl=15.091, loss=4.451, nll_loss=0.917, ppl=1.89, wps=23181.4, ups=2.21, wpb=10493.3, bsz=350.9, num_updates=47500, lr=1.25e-05, gnorm=4.542, train_wall=45, wall=23516
2020-12-10 17:53:46 | INFO | train_inner | epoch 085:    476 / 561 symm_kl=14.165, loss=4.335, nll_loss=0.901, ppl=1.87, wps=23330.1, ups=2.21, wpb=10573.5, bsz=385.6, num_updates=47600, lr=1.25e-05, gnorm=4.39, train_wall=45, wall=23561
2020-12-10 17:54:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:54:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:54:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:54:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:54:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:54:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:54:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:54:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:54:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:54:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:54:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:54:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:54:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:54:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:54:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:54:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:54:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:54:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:54:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:54:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:54:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:54:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:54:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:54:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:54:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:54:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:54:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:54:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:54:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:54:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:54:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:54:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:54:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:54:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:54:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:54:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:54:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:54:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:54:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:54:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:54:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:54:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:54:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:54:44 | INFO | valid | epoch 085 | valid on 'valid' subset | symm_kl 0 | loss 6.816 | nll_loss 4.927 | ppl 30.42 | bleu 20.98 | wps 4898.7 | wpb 7508.5 | bsz 272.7 | num_updates 47685 | best_bleu 21.47
2020-12-10 17:54:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:54:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:54:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:54:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:54:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:54:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 85 @ 47685 updates, score 20.98) (writing took 3.180449817329645 seconds)
2020-12-10 17:54:47 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2020-12-10 17:54:47 | INFO | train | epoch 085 | symm_kl 14.616 | loss 4.39 | nll_loss 0.908 | ppl 1.88 | wps 21086.2 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 47685 | lr 1.25e-05 | gnorm 4.487 | train_wall 252 | wall 23622
2020-12-10 17:54:47 | INFO | fairseq.trainer | begin training epoch 86
2020-12-10 17:54:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:54:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:54:57 | INFO | train_inner | epoch 086:     15 / 561 symm_kl=14.815, loss=4.416, nll_loss=0.912, ppl=1.88, wps=14606.8, ups=1.41, wpb=10358.5, bsz=370.7, num_updates=47700, lr=1.25e-05, gnorm=4.593, train_wall=45, wall=23632
2020-12-10 17:55:41 | INFO | train_inner | epoch 086:    115 / 561 symm_kl=14.524, loss=4.379, nll_loss=0.905, ppl=1.87, wps=23466.6, ups=2.25, wpb=10432.3, bsz=368.6, num_updates=47800, lr=1.25e-05, gnorm=4.488, train_wall=44, wall=23676
2020-12-10 17:56:27 | INFO | train_inner | epoch 086:    215 / 561 symm_kl=14.371, loss=4.363, nll_loss=0.905, ppl=1.87, wps=22996.5, ups=2.21, wpb=10421.5, bsz=370.7, num_updates=47900, lr=1.25e-05, gnorm=4.394, train_wall=45, wall=23722
2020-12-10 17:57:12 | INFO | train_inner | epoch 086:    315 / 561 symm_kl=15.101, loss=4.453, nll_loss=0.919, ppl=1.89, wps=23363.1, ups=2.2, wpb=10629.7, bsz=359.5, num_updates=48000, lr=1.25e-05, gnorm=4.66, train_wall=45, wall=23767
2020-12-10 17:57:57 | INFO | train_inner | epoch 086:    415 / 561 symm_kl=14.735, loss=4.404, nll_loss=0.91, ppl=1.88, wps=23247.5, ups=2.21, wpb=10517.7, bsz=371.1, num_updates=48100, lr=1.25e-05, gnorm=4.685, train_wall=45, wall=23812
2020-12-10 17:58:43 | INFO | train_inner | epoch 086:    515 / 561 symm_kl=14.12, loss=4.336, nll_loss=0.903, ppl=1.87, wps=23105.7, ups=2.21, wpb=10447.3, bsz=380.6, num_updates=48200, lr=1.25e-05, gnorm=4.304, train_wall=45, wall=23857
2020-12-10 17:59:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 17:59:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:59:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:59:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:59:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:59:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:59:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:59:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:59:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:59:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:59:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:59:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:59:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:59:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:59:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:59:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:59:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:59:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:59:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:59:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:59:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:59:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:59:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:59:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:59:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:59:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:59:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:59:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:59:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:59:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:59:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:59:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:59:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:59:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:59:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:59:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:59:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:59:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:59:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:59:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:59:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 17:59:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 17:59:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 17:59:23 | INFO | valid | epoch 086 | valid on 'valid' subset | symm_kl 0 | loss 6.806 | nll_loss 4.922 | ppl 30.33 | bleu 20.9 | wps 4836.4 | wpb 7508.5 | bsz 272.7 | num_updates 48246 | best_bleu 21.47
2020-12-10 17:59:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 17:59:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:59:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:59:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:59:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:59:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 86 @ 48246 updates, score 20.9) (writing took 3.1126778312027454 seconds)
2020-12-10 17:59:27 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2020-12-10 17:59:27 | INFO | train | epoch 086 | symm_kl 14.569 | loss 4.387 | nll_loss 0.909 | ppl 1.88 | wps 21062.5 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 48246 | lr 1.25e-05 | gnorm 4.505 | train_wall 252 | wall 23901
2020-12-10 17:59:27 | INFO | fairseq.trainer | begin training epoch 87
2020-12-10 17:59:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 17:59:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 17:59:53 | INFO | train_inner | epoch 087:     54 / 561 symm_kl=14.176, loss=4.342, nll_loss=0.907, ppl=1.88, wps=14722.2, ups=1.42, wpb=10389, bsz=372.3, num_updates=48300, lr=1.25e-05, gnorm=4.345, train_wall=44, wall=23928
2020-12-10 18:00:39 | INFO | train_inner | epoch 087:    154 / 561 symm_kl=15.017, loss=4.439, nll_loss=0.91, ppl=1.88, wps=23419.4, ups=2.2, wpb=10643.2, bsz=353.4, num_updates=48400, lr=1.25e-05, gnorm=4.636, train_wall=45, wall=23973
2020-12-10 18:01:24 | INFO | train_inner | epoch 087:    254 / 561 symm_kl=13.618, loss=4.275, nll_loss=0.898, ppl=1.86, wps=22908.2, ups=2.22, wpb=10314.6, bsz=387.7, num_updates=48500, lr=1.25e-05, gnorm=4.141, train_wall=45, wall=24019
2020-12-10 18:02:09 | INFO | train_inner | epoch 087:    354 / 561 symm_kl=14.684, loss=4.402, nll_loss=0.913, ppl=1.88, wps=23430, ups=2.2, wpb=10633.7, bsz=373.8, num_updates=48600, lr=1.25e-05, gnorm=4.541, train_wall=45, wall=24064
2020-12-10 18:02:54 | INFO | train_inner | epoch 087:    454 / 561 symm_kl=15.409, loss=4.486, nll_loss=0.921, ppl=1.89, wps=22869.8, ups=2.21, wpb=10338.3, bsz=352.2, num_updates=48700, lr=1.25e-05, gnorm=4.894, train_wall=45, wall=24109
2020-12-10 18:03:39 | INFO | train_inner | epoch 087:    554 / 561 symm_kl=14.387, loss=4.364, nll_loss=0.908, ppl=1.88, wps=23563.8, ups=2.21, wpb=10650.1, bsz=381.9, num_updates=48800, lr=1.25e-05, gnorm=4.323, train_wall=45, wall=24154
2020-12-10 18:03:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:03:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:03:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:03:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:03:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:03:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:03:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:03:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:03:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:03:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:03:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:03:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:03:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:03:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:03:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:03:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:03:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:03:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:03:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:03:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:03:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:03:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:03:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:03:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:03:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:03:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:03:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:03:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:03:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:03:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:03:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:03:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:03:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:03:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:03:50 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:03:50 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:03:50 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:03:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:03:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:03:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:04:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:04:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:04:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:04:02 | INFO | valid | epoch 087 | valid on 'valid' subset | symm_kl 0 | loss 6.807 | nll_loss 4.923 | ppl 30.33 | bleu 21.01 | wps 4983.8 | wpb 7508.5 | bsz 272.7 | num_updates 48807 | best_bleu 21.47
2020-12-10 18:04:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:04:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:04:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:04:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:04:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:04:05 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 87 @ 48807 updates, score 21.01) (writing took 3.1804972793906927 seconds)
2020-12-10 18:04:05 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2020-12-10 18:04:05 | INFO | train | epoch 087 | symm_kl 14.555 | loss 4.386 | nll_loss 0.909 | ppl 1.88 | wps 21095 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 48807 | lr 1.25e-05 | gnorm 4.478 | train_wall 252 | wall 24180
2020-12-10 18:04:05 | INFO | fairseq.trainer | begin training epoch 88
2020-12-10 18:04:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:04:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:04:50 | INFO | train_inner | epoch 088:     93 / 561 symm_kl=14.21, loss=4.35, nll_loss=0.906, ppl=1.87, wps=14752.4, ups=1.42, wpb=10402.3, bsz=368.2, num_updates=48900, lr=1.25e-05, gnorm=4.39, train_wall=44, wall=24225
2020-12-10 18:05:35 | INFO | train_inner | epoch 088:    193 / 561 symm_kl=13.965, loss=4.314, nll_loss=0.898, ppl=1.86, wps=23058.8, ups=2.21, wpb=10450.4, bsz=376.6, num_updates=49000, lr=1.25e-05, gnorm=4.302, train_wall=45, wall=24270
2020-12-10 18:06:20 | INFO | train_inner | epoch 088:    293 / 561 symm_kl=14.761, loss=4.423, nll_loss=0.925, ppl=1.9, wps=23508.4, ups=2.23, wpb=10561.8, bsz=370.5, num_updates=49100, lr=1.25e-05, gnorm=4.64, train_wall=45, wall=24315
2020-12-10 18:07:05 | INFO | train_inner | epoch 088:    393 / 561 symm_kl=14.891, loss=4.424, nll_loss=0.912, ppl=1.88, wps=23261, ups=2.22, wpb=10472.8, bsz=353.8, num_updates=49200, lr=1.25e-05, gnorm=4.534, train_wall=45, wall=24360
2020-12-10 18:07:50 | INFO | train_inner | epoch 088:    493 / 561 symm_kl=14.736, loss=4.408, nll_loss=0.909, ppl=1.88, wps=23483.6, ups=2.22, wpb=10560.6, bsz=365.3, num_updates=49300, lr=1.25e-05, gnorm=4.606, train_wall=45, wall=24405
2020-12-10 18:08:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:08:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:08:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:08:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:08:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:08:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:08:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:08:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:08:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:08:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:08:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:08:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:08:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:08:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:08:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:08:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:08:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:08:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:08:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:08:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:08:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:08:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:08:26 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:08:26 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:08:26 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:08:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:08:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:08:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:08:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:08:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:08:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:08:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:08:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:08:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:08:29 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:08:29 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:08:29 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:08:30 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:08:30 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:08:30 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:08:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:08:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:08:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:08:40 | INFO | valid | epoch 088 | valid on 'valid' subset | symm_kl 0 | loss 6.814 | nll_loss 4.929 | ppl 30.47 | bleu 20.88 | wps 4951.9 | wpb 7508.5 | bsz 272.7 | num_updates 49368 | best_bleu 21.47
2020-12-10 18:08:40 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:08:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:08:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:08:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:08:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:08:43 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 88 @ 49368 updates, score 20.88) (writing took 3.044062804430723 seconds)
2020-12-10 18:08:43 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2020-12-10 18:08:43 | INFO | train | epoch 088 | symm_kl 14.492 | loss 4.381 | nll_loss 0.91 | ppl 1.88 | wps 21165 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 49368 | lr 1.25e-05 | gnorm 4.488 | train_wall 251 | wall 24458
2020-12-10 18:08:43 | INFO | fairseq.trainer | begin training epoch 89
2020-12-10 18:08:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:08:46 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:09:00 | INFO | train_inner | epoch 089:     32 / 561 symm_kl=14.612, loss=4.39, nll_loss=0.911, ppl=1.88, wps=14834.6, ups=1.43, wpb=10396.9, bsz=376.5, num_updates=49400, lr=1.25e-05, gnorm=4.551, train_wall=44, wall=24475
2020-12-10 18:09:45 | INFO | train_inner | epoch 089:    132 / 561 symm_kl=14.971, loss=4.436, nll_loss=0.91, ppl=1.88, wps=23332.3, ups=2.22, wpb=10517.2, bsz=344.6, num_updates=49500, lr=1.25e-05, gnorm=4.575, train_wall=45, wall=24520
2020-12-10 18:10:30 | INFO | train_inner | epoch 089:    232 / 561 symm_kl=14.28, loss=4.357, nll_loss=0.907, ppl=1.88, wps=23301.3, ups=2.22, wpb=10489.7, bsz=373.4, num_updates=49600, lr=1.25e-05, gnorm=4.503, train_wall=45, wall=24565
2020-12-10 18:11:16 | INFO | train_inner | epoch 089:    332 / 561 symm_kl=14.186, loss=4.348, nll_loss=0.912, ppl=1.88, wps=23409.5, ups=2.21, wpb=10595.3, bsz=392.5, num_updates=49700, lr=1.25e-05, gnorm=4.306, train_wall=45, wall=24611
2020-12-10 18:12:01 | INFO | train_inner | epoch 089:    432 / 561 symm_kl=14.841, loss=4.431, nll_loss=0.926, ppl=1.9, wps=22792.5, ups=2.23, wpb=10225.3, bsz=354.1, num_updates=49800, lr=1.25e-05, gnorm=4.599, train_wall=45, wall=24655
2020-12-10 18:12:46 | INFO | train_inner | epoch 089:    532 / 561 symm_kl=13.997, loss=4.324, nll_loss=0.907, ppl=1.88, wps=23279.6, ups=2.2, wpb=10602.9, bsz=387.8, num_updates=49900, lr=1.25e-05, gnorm=4.33, train_wall=45, wall=24701
2020-12-10 18:12:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:13:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:13:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:13:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:13:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:13:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:13:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:13:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:13:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:13:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:13:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:13:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:13:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:13:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:13:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:13:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:13:05 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:13:05 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:13:05 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:13:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:13:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:13:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:13:07 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:13:07 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:13:07 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:13:08 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:13:08 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:13:08 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:13:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:13:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:13:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:13:19 | INFO | valid | epoch 089 | valid on 'valid' subset | symm_kl 0 | loss 6.81 | nll_loss 4.924 | ppl 30.37 | bleu 20.95 | wps 4843.3 | wpb 7508.5 | bsz 272.7 | num_updates 49929 | best_bleu 21.47
2020-12-10 18:13:19 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:13:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:13:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:13:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:13:22 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 89 @ 49929 updates, score 20.95) (writing took 3.0433689672499895 seconds)
2020-12-10 18:13:22 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2020-12-10 18:13:22 | INFO | train | epoch 089 | symm_kl 14.492 | loss 4.382 | nll_loss 0.912 | ppl 1.88 | wps 21076.9 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 49929 | lr 1.25e-05 | gnorm 4.483 | train_wall 252 | wall 24737
2020-12-10 18:13:22 | INFO | fairseq.trainer | begin training epoch 90
2020-12-10 18:13:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:13:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:13:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:13:57 | INFO | train_inner | epoch 090:     71 / 561 symm_kl=14.234, loss=4.353, nll_loss=0.908, ppl=1.88, wps=14850.9, ups=1.42, wpb=10487, bsz=371.6, num_updates=50000, lr=1.25e-05, gnorm=4.368, train_wall=44, wall=24772
2020-12-10 18:14:42 | INFO | train_inner | epoch 090:    171 / 561 symm_kl=14.463, loss=4.378, nll_loss=0.912, ppl=1.88, wps=23204, ups=2.22, wpb=10439.7, bsz=368.8, num_updates=50100, lr=1.25e-05, gnorm=4.456, train_wall=45, wall=24817
2020-12-10 18:15:27 | INFO | train_inner | epoch 090:    271 / 561 symm_kl=14.476, loss=4.391, nll_loss=0.924, ppl=1.9, wps=23111.9, ups=2.22, wpb=10397.9, bsz=370.4, num_updates=50200, lr=1.25e-05, gnorm=4.514, train_wall=45, wall=24862
2020-12-10 18:16:12 | INFO | train_inner | epoch 090:    371 / 561 symm_kl=14.267, loss=4.351, nll_loss=0.903, ppl=1.87, wps=23289.8, ups=2.2, wpb=10569.2, bsz=377, num_updates=50300, lr=1.25e-05, gnorm=4.425, train_wall=45, wall=24907
2020-12-10 18:16:57 | INFO | train_inner | epoch 090:    471 / 561 symm_kl=14.781, loss=4.412, nll_loss=0.91, ppl=1.88, wps=23417.4, ups=2.21, wpb=10597.1, bsz=362.5, num_updates=50400, lr=1.25e-05, gnorm=4.521, train_wall=45, wall=24952
2020-12-10 18:17:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:17:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:17:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:17:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:17:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:17:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:17:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:17:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:17:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:17:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:17:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:17:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:17:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:17:42 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:17:42 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:17:42 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:17:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:17:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:17:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:17:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:17:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:17:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:17:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:17:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:17:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:17:44 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:17:44 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:17:44 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:17:45 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:17:45 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:17:45 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:17:47 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:17:47 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:17:47 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:17:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:17:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:17:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:17:58 | INFO | valid | epoch 090 | valid on 'valid' subset | symm_kl 0 | loss 6.821 | nll_loss 4.935 | ppl 30.58 | bleu 20.89 | wps 4654.8 | wpb 7508.5 | bsz 272.7 | num_updates 50490 | best_bleu 21.47
2020-12-10 18:17:58 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:17:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:17:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:18:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 90 @ 50490 updates, score 20.89) (writing took 2.423624634742737 seconds)
2020-12-10 18:18:00 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2020-12-10 18:18:00 | INFO | train | epoch 090 | symm_kl 14.442 | loss 4.379 | nll_loss 0.913 | ppl 1.88 | wps 21135.3 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 50490 | lr 1.25e-05 | gnorm 4.452 | train_wall 251 | wall 25015
2020-12-10 18:18:00 | INFO | fairseq.trainer | begin training epoch 91
2020-12-10 18:18:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:18:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:18:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:18:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:18:08 | INFO | train_inner | epoch 091:     10 / 561 symm_kl=14.632, loss=4.409, nll_loss=0.921, ppl=1.89, wps=14606.4, ups=1.42, wpb=10318.9, bsz=359, num_updates=50500, lr=1.25e-05, gnorm=4.509, train_wall=44, wall=25023
2020-12-10 18:18:52 | INFO | train_inner | epoch 091:    110 / 561 symm_kl=14.309, loss=4.365, nll_loss=0.913, ppl=1.88, wps=23787.1, ups=2.26, wpb=10514.9, bsz=375.8, num_updates=50600, lr=1.25e-05, gnorm=4.387, train_wall=44, wall=25067
2020-12-10 18:19:37 | INFO | train_inner | epoch 091:    210 / 561 symm_kl=14.869, loss=4.436, nll_loss=0.925, ppl=1.9, wps=23243.3, ups=2.21, wpb=10532.3, bsz=355.8, num_updates=50700, lr=1.25e-05, gnorm=4.51, train_wall=45, wall=25112
2020-12-10 18:20:22 | INFO | train_inner | epoch 091:    310 / 561 symm_kl=14.428, loss=4.375, nll_loss=0.907, ppl=1.88, wps=23317.4, ups=2.23, wpb=10469.8, bsz=373.2, num_updates=50800, lr=1.25e-05, gnorm=4.6, train_wall=45, wall=25157
2020-12-10 18:21:08 | INFO | train_inner | epoch 091:    410 / 561 symm_kl=14.158, loss=4.348, nll_loss=0.913, ppl=1.88, wps=23278.5, ups=2.21, wpb=10546.4, bsz=380.1, num_updates=50900, lr=1.25e-05, gnorm=4.337, train_wall=45, wall=25203
2020-12-10 18:21:53 | INFO | train_inner | epoch 091:    510 / 561 symm_kl=14.052, loss=4.331, nll_loss=0.909, ppl=1.88, wps=23185.7, ups=2.23, wpb=10393.4, bsz=372.7, num_updates=51000, lr=1.25e-05, gnorm=4.375, train_wall=45, wall=25247
2020-12-10 18:22:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:22:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:22:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:22:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:22:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:22:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:22:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:22:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:22:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:22:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:22:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:22:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:22:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:22:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:22:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:22:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:22:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:22:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:22:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:22:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:22:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:22:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:22:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:22:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:22:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:22:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:22:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:22:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:22:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:22:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:22:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:22:22 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:22:22 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:22:22 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:22:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:22:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:22:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:22:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:22:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:22:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:22:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:22:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:22:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:22:36 | INFO | valid | epoch 091 | valid on 'valid' subset | symm_kl 0 | loss 6.816 | nll_loss 4.929 | ppl 30.46 | bleu 20.86 | wps 4690.8 | wpb 7508.5 | bsz 272.7 | num_updates 51051 | best_bleu 21.47
2020-12-10 18:22:36 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:22:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:22:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:22:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:22:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:22:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 91 @ 51051 updates, score 20.86) (writing took 2.943990968167782 seconds)
2020-12-10 18:22:39 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2020-12-10 18:22:39 | INFO | train | epoch 091 | symm_kl 14.43 | loss 4.378 | nll_loss 0.914 | ppl 1.88 | wps 21111.8 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 51051 | lr 1.25e-05 | gnorm 4.463 | train_wall 251 | wall 25294
2020-12-10 18:22:39 | INFO | fairseq.trainer | begin training epoch 92
2020-12-10 18:22:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:22:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:23:03 | INFO | train_inner | epoch 092:     49 / 561 symm_kl=14.252, loss=4.352, nll_loss=0.906, ppl=1.87, wps=14772.4, ups=1.41, wpb=10462.3, bsz=368.4, num_updates=51100, lr=1.25e-05, gnorm=4.429, train_wall=44, wall=25318
2020-12-10 18:23:48 | INFO | train_inner | epoch 092:    149 / 561 symm_kl=13.752, loss=4.298, nll_loss=0.905, ppl=1.87, wps=23119.4, ups=2.22, wpb=10431.5, bsz=384.6, num_updates=51200, lr=1.25e-05, gnorm=4.279, train_wall=45, wall=25363
2020-12-10 18:24:34 | INFO | train_inner | epoch 092:    249 / 561 symm_kl=14.691, loss=4.409, nll_loss=0.913, ppl=1.88, wps=23140.3, ups=2.21, wpb=10451.8, bsz=357.5, num_updates=51300, lr=1.25e-05, gnorm=4.553, train_wall=45, wall=25408
2020-12-10 18:25:19 | INFO | train_inner | epoch 092:    349 / 561 symm_kl=14.676, loss=4.415, nll_loss=0.925, ppl=1.9, wps=22893.6, ups=2.21, wpb=10363.4, bsz=366.9, num_updates=51400, lr=1.25e-05, gnorm=4.646, train_wall=45, wall=25454
2020-12-10 18:26:04 | INFO | train_inner | epoch 092:    449 / 561 symm_kl=14.825, loss=4.433, nll_loss=0.926, ppl=1.9, wps=23535.6, ups=2.21, wpb=10632.7, bsz=356.2, num_updates=51500, lr=1.25e-05, gnorm=4.539, train_wall=45, wall=25499
2020-12-10 18:26:49 | INFO | train_inner | epoch 092:    549 / 561 symm_kl=14.233, loss=4.357, nll_loss=0.915, ppl=1.89, wps=23512.1, ups=2.22, wpb=10610.3, bsz=383, num_updates=51600, lr=1.25e-05, gnorm=4.392, train_wall=45, wall=25544
2020-12-10 18:26:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:26:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:26:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:26:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:26:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:26:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:26:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:26:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:26:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:26:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:26:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:26:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:26:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:26:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:26:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:26:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:27:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:27:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:27:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:27:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:27:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:27:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:27:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:27:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:27:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:27:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:27:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:27:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:27:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:27:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:27:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:27:01 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:27:01 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:27:01 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:27:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:27:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:27:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:27:04 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:27:04 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:27:04 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:27:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:27:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:27:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:27:15 | INFO | valid | epoch 092 | valid on 'valid' subset | symm_kl 0 | loss 6.809 | nll_loss 4.924 | ppl 30.35 | bleu 20.86 | wps 4618.4 | wpb 7508.5 | bsz 272.7 | num_updates 51612 | best_bleu 21.47
2020-12-10 18:27:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:27:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:27:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:27:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:27:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:27:18 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 92 @ 51612 updates, score 20.86) (writing took 3.0734495650976896 seconds)
2020-12-10 18:27:18 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2020-12-10 18:27:18 | INFO | train | epoch 092 | symm_kl 14.358 | loss 4.372 | nll_loss 0.914 | ppl 1.88 | wps 21062.4 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 51612 | lr 1.25e-05 | gnorm 4.46 | train_wall 251 | wall 25573
2020-12-10 18:27:18 | INFO | fairseq.trainer | begin training epoch 93
2020-12-10 18:27:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:27:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:28:00 | INFO | train_inner | epoch 093:     88 / 561 symm_kl=14.508, loss=4.375, nll_loss=0.897, ppl=1.86, wps=14773.9, ups=1.41, wpb=10497.8, bsz=359.2, num_updates=51700, lr=1.25e-05, gnorm=4.572, train_wall=44, wall=25615
2020-12-10 18:28:45 | INFO | train_inner | epoch 093:    188 / 561 symm_kl=14.017, loss=4.337, nll_loss=0.915, ppl=1.89, wps=23242.5, ups=2.22, wpb=10487.5, bsz=369.7, num_updates=51800, lr=1.25e-05, gnorm=4.314, train_wall=45, wall=25660
2020-12-10 18:29:31 | INFO | train_inner | epoch 093:    288 / 561 symm_kl=14.252, loss=4.363, nll_loss=0.915, ppl=1.89, wps=23576.1, ups=2.21, wpb=10653.3, bsz=373.3, num_updates=51900, lr=1.25e-05, gnorm=4.478, train_wall=45, wall=25705
2020-12-10 18:30:16 | INFO | train_inner | epoch 093:    388 / 561 symm_kl=14.647, loss=4.419, nll_loss=0.93, ppl=1.91, wps=23201.5, ups=2.22, wpb=10432.2, bsz=361.2, num_updates=52000, lr=1.25e-05, gnorm=4.473, train_wall=45, wall=25750
2020-12-10 18:31:01 | INFO | train_inner | epoch 093:    488 / 561 symm_kl=14.262, loss=4.371, nll_loss=0.927, ppl=1.9, wps=23027.7, ups=2.22, wpb=10376.9, bsz=374, num_updates=52100, lr=1.25e-05, gnorm=4.377, train_wall=45, wall=25795
2020-12-10 18:31:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:31:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:31:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:31:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:31:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:31:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:31:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:31:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:31:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:31:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:31:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:31:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:31:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:31:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:31:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:31:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:31:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:31:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:31:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:31:40 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:31:40 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:31:40 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:31:41 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:31:41 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:31:41 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:31:43 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:31:43 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:31:43 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:31:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:31:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:31:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:31:54 | INFO | valid | epoch 093 | valid on 'valid' subset | symm_kl 0 | loss 6.805 | nll_loss 4.923 | ppl 30.34 | bleu 20.89 | wps 4761.4 | wpb 7508.5 | bsz 272.7 | num_updates 52173 | best_bleu 21.47
2020-12-10 18:31:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:31:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:31:54 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:31:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:31:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:31:57 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 93 @ 52173 updates, score 20.89) (writing took 3.088900551199913 seconds)
2020-12-10 18:31:57 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2020-12-10 18:31:57 | INFO | train | epoch 093 | symm_kl 14.334 | loss 4.371 | nll_loss 0.916 | ppl 1.89 | wps 21131.7 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 52173 | lr 1.25e-05 | gnorm 4.467 | train_wall 251 | wall 25851
2020-12-10 18:31:57 | INFO | fairseq.trainer | begin training epoch 94
2020-12-10 18:31:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:31:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:32:12 | INFO | train_inner | epoch 094:     27 / 561 symm_kl=13.963, loss=4.32, nll_loss=0.904, ppl=1.87, wps=14736.5, ups=1.41, wpb=10461.7, bsz=386.4, num_updates=52200, lr=1.25e-05, gnorm=4.472, train_wall=44, wall=25866
2020-12-10 18:32:56 | INFO | train_inner | epoch 094:    127 / 561 symm_kl=14.931, loss=4.441, nll_loss=0.918, ppl=1.89, wps=23362.6, ups=2.25, wpb=10402.3, bsz=353, num_updates=52300, lr=1.25e-05, gnorm=4.642, train_wall=44, wall=25911
2020-12-10 18:33:41 | INFO | train_inner | epoch 094:    227 / 561 symm_kl=14.526, loss=4.395, nll_loss=0.919, ppl=1.89, wps=22917.2, ups=2.22, wpb=10335.7, bsz=373.4, num_updates=52400, lr=1.25e-05, gnorm=4.585, train_wall=45, wall=25956
2020-12-10 18:34:26 | INFO | train_inner | epoch 094:    327 / 561 symm_kl=14.143, loss=4.346, nll_loss=0.91, ppl=1.88, wps=23494.1, ups=2.22, wpb=10578.3, bsz=367.3, num_updates=52500, lr=1.25e-05, gnorm=4.324, train_wall=45, wall=26001
2020-12-10 18:35:12 | INFO | train_inner | epoch 094:    427 / 561 symm_kl=13.758, loss=4.302, nll_loss=0.908, ppl=1.88, wps=23488.3, ups=2.21, wpb=10632.9, bsz=381, num_updates=52600, lr=1.25e-05, gnorm=4.192, train_wall=45, wall=26046
2020-12-10 18:35:57 | INFO | train_inner | epoch 094:    527 / 561 symm_kl=14.212, loss=4.366, nll_loss=0.926, ppl=1.9, wps=23122.7, ups=2.21, wpb=10443.3, bsz=371.7, num_updates=52700, lr=1.25e-05, gnorm=4.411, train_wall=45, wall=26092
2020-12-10 18:36:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:36:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:36:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:36:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:36:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:36:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:36:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:36:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:36:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:36:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:36:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:36:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:36:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:36:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:36:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:36:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:36:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:36:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:36:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:36:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:36:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:36:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:36:18 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:36:18 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:36:18 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:36:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:36:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:36:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:36:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:36:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:36:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:36:19 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:36:19 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:36:19 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:36:20 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:36:20 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:36:20 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:36:21 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:36:21 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:36:21 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:36:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:36:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:36:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:36:32 | INFO | valid | epoch 094 | valid on 'valid' subset | symm_kl 0 | loss 6.814 | nll_loss 4.931 | ppl 30.51 | bleu 20.85 | wps 4897.6 | wpb 7508.5 | bsz 272.7 | num_updates 52734 | best_bleu 21.47
2020-12-10 18:36:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:36:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:36:33 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:36:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:36:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:36:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 94 @ 52734 updates, score 20.85) (writing took 3.0934944543987513 seconds)
2020-12-10 18:36:35 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2020-12-10 18:36:35 | INFO | train | epoch 094 | symm_kl 14.302 | loss 4.369 | nll_loss 0.917 | ppl 1.89 | wps 21121.1 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 52734 | lr 1.25e-05 | gnorm 4.419 | train_wall 251 | wall 26130
2020-12-10 18:36:35 | INFO | fairseq.trainer | begin training epoch 95
2020-12-10 18:36:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:36:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:37:07 | INFO | train_inner | epoch 095:     66 / 561 symm_kl=14.706, loss=4.427, nll_loss=0.932, ppl=1.91, wps=14937.1, ups=1.42, wpb=10510.1, bsz=358.2, num_updates=52800, lr=1.25e-05, gnorm=4.456, train_wall=44, wall=26162
2020-12-10 18:37:52 | INFO | train_inner | epoch 095:    166 / 561 symm_kl=13.926, loss=4.32, nll_loss=0.904, ppl=1.87, wps=23494.8, ups=2.22, wpb=10593.3, bsz=381.3, num_updates=52900, lr=1.25e-05, gnorm=4.434, train_wall=45, wall=26207
2020-12-10 18:38:37 | INFO | train_inner | epoch 095:    266 / 561 symm_kl=13.557, loss=4.29, nll_loss=0.916, ppl=1.89, wps=22882.5, ups=2.22, wpb=10327.4, bsz=383.2, num_updates=53000, lr=1.25e-05, gnorm=4.19, train_wall=45, wall=26252
2020-12-10 18:39:22 | INFO | train_inner | epoch 095:    366 / 561 symm_kl=14.806, loss=4.419, nll_loss=0.916, ppl=1.89, wps=23416.3, ups=2.21, wpb=10593.3, bsz=362.1, num_updates=53100, lr=1.25e-05, gnorm=4.658, train_wall=45, wall=26297
2020-12-10 18:40:07 | INFO | train_inner | epoch 095:    466 / 561 symm_kl=14.735, loss=4.43, nll_loss=0.93, ppl=1.91, wps=23352.6, ups=2.23, wpb=10466.8, bsz=358.6, num_updates=53200, lr=1.25e-05, gnorm=4.693, train_wall=45, wall=26342
2020-12-10 18:40:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:40:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:40:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:40:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:40:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:40:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:40:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:40:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:40:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:40:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:40:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:40:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:40:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:40:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:40:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:40:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:40:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:40:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:40:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:40:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:40:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:40:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:40:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:40:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:40:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:40:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:40:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:40:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:40:58 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:40:58 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:40:58 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:41:00 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:41:00 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:41:00 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:41:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:41:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:41:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:41:10 | INFO | valid | epoch 095 | valid on 'valid' subset | symm_kl 0 | loss 6.81 | nll_loss 4.926 | ppl 30.41 | bleu 20.9 | wps 4761.4 | wpb 7508.5 | bsz 272.7 | num_updates 53295 | best_bleu 21.47
2020-12-10 18:41:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:41:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:41:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:41:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:41:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:41:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 95 @ 53295 updates, score 20.9) (writing took 3.0927191860973835 seconds)
2020-12-10 18:41:14 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2020-12-10 18:41:14 | INFO | train | epoch 095 | symm_kl 14.272 | loss 4.367 | nll_loss 0.918 | ppl 1.89 | wps 21115.8 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 53295 | lr 1.25e-05 | gnorm 4.47 | train_wall 251 | wall 26408
2020-12-10 18:41:14 | INFO | fairseq.trainer | begin training epoch 96
2020-12-10 18:41:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:41:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:41:19 | INFO | train_inner | epoch 096:      5 / 561 symm_kl=13.984, loss=4.331, nll_loss=0.914, ppl=1.88, wps=14521, ups=1.4, wpb=10401.9, bsz=370.6, num_updates=53300, lr=1.25e-05, gnorm=4.346, train_wall=45, wall=26414
2020-12-10 18:42:03 | INFO | train_inner | epoch 096:    105 / 561 symm_kl=14.037, loss=4.354, nll_loss=0.93, ppl=1.91, wps=23852.8, ups=2.24, wpb=10625.5, bsz=382.3, num_updates=53400, lr=1.25e-05, gnorm=4.316, train_wall=44, wall=26458
2020-12-10 18:42:49 | INFO | train_inner | epoch 096:    205 / 561 symm_kl=14.321, loss=4.375, nll_loss=0.921, ppl=1.89, wps=22956.7, ups=2.22, wpb=10348.6, bsz=367, num_updates=53500, lr=1.25e-05, gnorm=4.499, train_wall=45, wall=26503
2020-12-10 18:43:34 | INFO | train_inner | epoch 096:    305 / 561 symm_kl=14.053, loss=4.345, nll_loss=0.919, ppl=1.89, wps=23257.4, ups=2.2, wpb=10569.5, bsz=371.9, num_updates=53600, lr=1.25e-05, gnorm=4.361, train_wall=45, wall=26549
2020-12-10 18:44:19 | INFO | train_inner | epoch 096:    405 / 561 symm_kl=14.318, loss=4.373, nll_loss=0.914, ppl=1.88, wps=23315.8, ups=2.22, wpb=10487.9, bsz=358, num_updates=53700, lr=1.25e-05, gnorm=4.523, train_wall=45, wall=26594
2020-12-10 18:45:04 | INFO | train_inner | epoch 096:    505 / 561 symm_kl=14.285, loss=4.364, nll_loss=0.916, ppl=1.89, wps=23117.3, ups=2.21, wpb=10456.6, bsz=374.6, num_updates=53800, lr=1.25e-05, gnorm=4.464, train_wall=45, wall=26639
2020-12-10 18:45:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:45:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:45:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:45:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:45:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:45:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:45:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:45:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:45:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:45:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:45:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:45:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:45:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:45:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:45:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:45:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:45:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:45:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:45:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:45:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:45:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:45:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:45:35 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:45:35 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:45:35 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:45:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:45:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:45:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:45:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:45:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:45:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:45:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:45:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:45:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:45:38 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:45:38 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:45:38 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:45:39 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:45:39 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:45:39 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:45:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:45:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:45:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:45:49 | INFO | valid | epoch 096 | valid on 'valid' subset | symm_kl 0 | loss 6.814 | nll_loss 4.931 | ppl 30.5 | bleu 20.95 | wps 5056.1 | wpb 7508.5 | bsz 272.7 | num_updates 53856 | best_bleu 21.47
2020-12-10 18:45:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:45:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:45:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:45:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:45:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:45:52 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 96 @ 53856 updates, score 20.95) (writing took 3.0194309521466494 seconds)
2020-12-10 18:45:52 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2020-12-10 18:45:52 | INFO | train | epoch 096 | symm_kl 14.236 | loss 4.364 | nll_loss 0.919 | ppl 1.89 | wps 21110.2 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 53856 | lr 1.25e-05 | gnorm 4.476 | train_wall 252 | wall 26687
2020-12-10 18:45:52 | INFO | fairseq.trainer | begin training epoch 97
2020-12-10 18:45:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:45:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:46:15 | INFO | train_inner | epoch 097:     44 / 561 symm_kl=13.892, loss=4.313, nll_loss=0.902, ppl=1.87, wps=14797.7, ups=1.42, wpb=10400.5, bsz=368.9, num_updates=53900, lr=1.25e-05, gnorm=4.459, train_wall=44, wall=26709
2020-12-10 18:46:59 | INFO | train_inner | epoch 097:    144 / 561 symm_kl=14.586, loss=4.404, nll_loss=0.925, ppl=1.9, wps=23285.6, ups=2.22, wpb=10469.8, bsz=365.3, num_updates=54000, lr=1.25e-05, gnorm=4.57, train_wall=45, wall=26754
2020-12-10 18:47:45 | INFO | train_inner | epoch 097:    244 / 561 symm_kl=13.812, loss=4.318, nll_loss=0.917, ppl=1.89, wps=22948.3, ups=2.21, wpb=10403.3, bsz=384.1, num_updates=54100, lr=1.25e-05, gnorm=4.335, train_wall=45, wall=26800
2020-12-10 18:48:30 | INFO | train_inner | epoch 097:    344 / 561 symm_kl=14.262, loss=4.378, nll_loss=0.929, ppl=1.9, wps=23338.9, ups=2.23, wpb=10487.3, bsz=359.1, num_updates=54200, lr=1.25e-05, gnorm=4.363, train_wall=45, wall=26845
2020-12-10 18:49:15 | INFO | train_inner | epoch 097:    444 / 561 symm_kl=14.33, loss=4.369, nll_loss=0.914, ppl=1.88, wps=23239.9, ups=2.21, wpb=10524.2, bsz=372.2, num_updates=54300, lr=1.25e-05, gnorm=4.57, train_wall=45, wall=26890
2020-12-10 18:50:00 | INFO | train_inner | epoch 097:    544 / 561 symm_kl=14.61, loss=4.411, nll_loss=0.926, ppl=1.9, wps=23408.3, ups=2.22, wpb=10566.6, bsz=364.7, num_updates=54400, lr=1.25e-05, gnorm=4.544, train_wall=45, wall=26935
2020-12-10 18:50:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:50:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:50:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:50:09 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:50:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:50:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:50:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:50:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:50:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:50:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:50:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:50:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:50:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:50:12 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:50:12 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:50:12 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:50:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:50:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:50:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:50:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:50:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:50:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:50:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:50:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:50:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:50:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:50:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:50:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:50:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:50:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:50:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:50:15 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:50:15 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:50:15 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:50:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:50:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:50:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:50:17 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:50:17 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:50:17 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:50:27 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:50:27 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:50:27 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:50:28 | INFO | valid | epoch 097 | valid on 'valid' subset | symm_kl 0 | loss 6.827 | nll_loss 4.941 | ppl 30.72 | bleu 20.98 | wps 4876.4 | wpb 7508.5 | bsz 272.7 | num_updates 54417 | best_bleu 21.47
2020-12-10 18:50:28 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:50:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:50:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:50:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:50:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 97 @ 54417 updates, score 20.98) (writing took 3.0953249521553516 seconds)
2020-12-10 18:50:31 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2020-12-10 18:50:31 | INFO | train | epoch 097 | symm_kl 14.214 | loss 4.363 | nll_loss 0.92 | ppl 1.89 | wps 21106.3 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 54417 | lr 1.25e-05 | gnorm 4.43 | train_wall 252 | wall 26966
2020-12-10 18:50:31 | INFO | fairseq.trainer | begin training epoch 98
2020-12-10 18:50:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:50:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:51:11 | INFO | train_inner | epoch 098:     83 / 561 symm_kl=14.042, loss=4.344, nll_loss=0.919, ppl=1.89, wps=14790.6, ups=1.42, wpb=10408.8, bsz=366.3, num_updates=54500, lr=1.25e-05, gnorm=4.403, train_wall=44, wall=27005
2020-12-10 18:51:56 | INFO | train_inner | epoch 098:    183 / 561 symm_kl=13.77, loss=4.315, nll_loss=0.917, ppl=1.89, wps=23120.6, ups=2.21, wpb=10461.8, bsz=380.1, num_updates=54600, lr=1.25e-05, gnorm=4.299, train_wall=45, wall=27051
2020-12-10 18:52:41 | INFO | train_inner | epoch 098:    283 / 561 symm_kl=14.413, loss=4.387, nll_loss=0.923, ppl=1.9, wps=23233.4, ups=2.21, wpb=10514.2, bsz=364.8, num_updates=54700, lr=1.25e-05, gnorm=4.605, train_wall=45, wall=27096
2020-12-10 18:53:26 | INFO | train_inner | epoch 098:    383 / 561 symm_kl=14.606, loss=4.412, nll_loss=0.926, ppl=1.9, wps=23426.6, ups=2.22, wpb=10550.9, bsz=352.4, num_updates=54800, lr=1.25e-05, gnorm=4.524, train_wall=45, wall=27141
2020-12-10 18:54:12 | INFO | train_inner | epoch 098:    483 / 561 symm_kl=14.447, loss=4.399, nll_loss=0.932, ppl=1.91, wps=22961.6, ups=2.2, wpb=10435.6, bsz=377.7, num_updates=54900, lr=1.25e-05, gnorm=4.567, train_wall=45, wall=27186
2020-12-10 18:54:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:54:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:54:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:54:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:54:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:54:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:54:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:54:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:54:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:54:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:54:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:54:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:54:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:54:51 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:54:51 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:54:51 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:54:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:54:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:54:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:54:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:54:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:54:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:54:52 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:54:52 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:54:52 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:54:53 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:54:53 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:54:53 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:54:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:54:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:54:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:54:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:54:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:54:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:54:55 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:54:55 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:54:55 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:54:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:54:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:54:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:55:06 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:55:06 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:55:06 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:55:07 | INFO | valid | epoch 098 | valid on 'valid' subset | symm_kl 0 | loss 6.812 | nll_loss 4.929 | ppl 30.46 | bleu 20.88 | wps 4690.6 | wpb 7508.5 | bsz 272.7 | num_updates 54978 | best_bleu 21.47
2020-12-10 18:55:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:55:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:55:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:55:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 98 @ 54978 updates, score 20.88) (writing took 3.124102460220456 seconds)
2020-12-10 18:55:10 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2020-12-10 18:55:10 | INFO | train | epoch 098 | symm_kl 14.2 | loss 4.363 | nll_loss 0.921 | ppl 1.89 | wps 21034 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 54978 | lr 1.25e-05 | gnorm 4.467 | train_wall 252 | wall 27245
2020-12-10 18:55:10 | INFO | fairseq.trainer | begin training epoch 99
2020-12-10 18:55:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:55:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:55:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:55:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:55:23 | INFO | train_inner | epoch 099:     22 / 561 symm_kl=13.77, loss=4.306, nll_loss=0.91, ppl=1.88, wps=14608.6, ups=1.4, wpb=10456.6, bsz=376, num_updates=55000, lr=1.25e-05, gnorm=4.308, train_wall=45, wall=27258
2020-12-10 18:56:08 | INFO | train_inner | epoch 099:    122 / 561 symm_kl=13.496, loss=4.276, nll_loss=0.904, ppl=1.87, wps=23793.5, ups=2.24, wpb=10600.2, bsz=383.8, num_updates=55100, lr=1.25e-05, gnorm=4.202, train_wall=44, wall=27303
2020-12-10 18:56:53 | INFO | train_inner | epoch 099:    222 / 561 symm_kl=14.052, loss=4.341, nll_loss=0.912, ppl=1.88, wps=23196.7, ups=2.2, wpb=10522.3, bsz=373.4, num_updates=55200, lr=1.25e-05, gnorm=4.398, train_wall=45, wall=27348
2020-12-10 18:57:38 | INFO | train_inner | epoch 099:    322 / 561 symm_kl=14.264, loss=4.373, nll_loss=0.919, ppl=1.89, wps=23347.4, ups=2.21, wpb=10573.9, bsz=357.8, num_updates=55300, lr=1.25e-05, gnorm=4.382, train_wall=45, wall=27393
2020-12-10 18:58:23 | INFO | train_inner | epoch 099:    422 / 561 symm_kl=14.233, loss=4.377, nll_loss=0.933, ppl=1.91, wps=23197.8, ups=2.22, wpb=10468.3, bsz=367.5, num_updates=55400, lr=1.25e-05, gnorm=4.488, train_wall=45, wall=27438
2020-12-10 18:59:09 | INFO | train_inner | epoch 099:    522 / 561 symm_kl=14.343, loss=4.387, nll_loss=0.934, ppl=1.91, wps=22771.3, ups=2.21, wpb=10318.6, bsz=372.3, num_updates=55500, lr=1.25e-05, gnorm=4.665, train_wall=45, wall=27484
2020-12-10 18:59:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 18:59:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:59:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:59:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:59:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:59:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:59:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:59:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:59:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:59:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:59:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:59:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:59:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:59:31 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:59:31 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:59:31 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:59:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:59:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:59:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:59:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:59:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:59:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:59:32 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:59:32 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:59:32 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:59:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:59:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:59:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:59:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:59:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:59:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:59:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:59:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:59:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:59:34 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:59:34 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:59:34 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:59:36 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:59:36 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:59:36 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:59:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 18:59:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 18:59:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 18:59:47 | INFO | valid | epoch 099 | valid on 'valid' subset | symm_kl 0 | loss 6.825 | nll_loss 4.938 | ppl 30.65 | bleu 20.9 | wps 4745.4 | wpb 7508.5 | bsz 272.7 | num_updates 55539 | best_bleu 21.47
2020-12-10 18:59:47 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 18:59:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:59:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:59:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:59:49 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 99 @ 55539 updates, score 20.9) (writing took 2.648856358602643 seconds)
2020-12-10 18:59:49 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2020-12-10 18:59:49 | INFO | train | epoch 099 | symm_kl 14.13 | loss 4.358 | nll_loss 0.922 | ppl 1.89 | wps 21076 | ups 2.01 | wpb 10483.4 | bsz 369.6 | num_updates 55539 | lr 1.25e-05 | gnorm 4.437 | train_wall 252 | wall 27524
2020-12-10 18:59:49 | INFO | fairseq.trainer | begin training epoch 100
2020-12-10 18:59:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 18:59:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 18:59:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 19:00:19 | INFO | train_inner | epoch 100:     61 / 561 symm_kl=14.198, loss=4.363, nll_loss=0.921, ppl=1.89, wps=14818.3, ups=1.42, wpb=10436.2, bsz=367.3, num_updates=55600, lr=1.25e-05, gnorm=4.46, train_wall=44, wall=27554
2020-12-10 19:01:04 | INFO | train_inner | epoch 100:    161 / 561 symm_kl=14.198, loss=4.366, nll_loss=0.922, ppl=1.89, wps=23124.8, ups=2.23, wpb=10389, bsz=359.3, num_updates=55700, lr=1.25e-05, gnorm=4.47, train_wall=45, wall=27599
2020-12-10 19:01:49 | INFO | train_inner | epoch 100:    261 / 561 symm_kl=14.385, loss=4.399, nll_loss=0.938, ppl=1.92, wps=23424.6, ups=2.21, wpb=10594.4, bsz=373.2, num_updates=55800, lr=1.25e-05, gnorm=4.619, train_wall=45, wall=27644
2020-12-10 19:02:35 | INFO | train_inner | epoch 100:    361 / 561 symm_kl=14.224, loss=4.366, nll_loss=0.919, ppl=1.89, wps=23291.8, ups=2.21, wpb=10532.3, bsz=360.5, num_updates=55900, lr=1.25e-05, gnorm=4.497, train_wall=45, wall=27689
2020-12-10 19:03:20 | INFO | train_inner | epoch 100:    461 / 561 symm_kl=13.742, loss=4.312, nll_loss=0.917, ppl=1.89, wps=23361.6, ups=2.22, wpb=10534.4, bsz=382.5, num_updates=56000, lr=1.25e-05, gnorm=4.142, train_wall=45, wall=27735
2020-12-10 19:04:05 | INFO | train_inner | epoch 100:    561 / 561 symm_kl=14.233, loss=4.376, nll_loss=0.929, ppl=1.9, wps=22993.7, ups=2.21, wpb=10381.9, bsz=366.9, num_updates=56100, lr=1.25e-05, gnorm=4.454, train_wall=45, wall=27780
2020-12-10 19:04:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-10 19:04:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 19:04:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 19:04:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-10 19:04:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 19:04:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 19:04:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-10 19:04:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 19:04:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 19:04:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 19:04:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 19:04:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 19:04:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 19:04:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 19:04:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 19:04:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 19:04:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 19:04:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 19:04:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 19:04:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 19:04:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 19:04:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 19:04:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 19:04:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 19:04:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 19:04:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 19:04:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 19:04:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 19:04:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 19:04:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 19:04:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 19:04:11 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 19:04:11 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 19:04:11 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 19:04:13 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 19:04:13 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 19:04:13 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 19:04:14 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 19:04:14 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 19:04:14 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 19:04:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-10 19:04:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-10 19:04:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-10 19:04:24 | INFO | valid | epoch 100 | valid on 'valid' subset | symm_kl 0 | loss 6.821 | nll_loss 4.937 | ppl 30.62 | bleu 20.84 | wps 5123.4 | wpb 7508.5 | bsz 272.7 | num_updates 56100 | best_bleu 21.47
2020-12-10 19:04:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-10 19:04:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/entr/bash/../checkpoints/closer_gap/checkpoint_last.pt (epoch 100 @ 56100 updates, score 20.84) (writing took 2.712877832353115 seconds)
2020-12-10 19:04:27 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2020-12-10 19:04:27 | INFO | train | epoch 100 | symm_kl 14.094 | loss 4.355 | nll_loss 0.923 | ppl 1.9 | wps 21208.6 | ups 2.02 | wpb 10483.4 | bsz 369.6 | num_updates 56100 | lr 1.25e-05 | gnorm 4.418 | train_wall 251 | wall 27802
2020-12-10 19:04:27 | INFO | fairseq_cli.train | done training in 27800.7 seconds
/home/rcduan/miniconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 600 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
