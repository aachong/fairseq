2020-12-01 20:54:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 20:55:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 20:55:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 20:55:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 20:55:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 20:55:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 20:55:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 20:55:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 20:55:09 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:15001
2020-12-01 20:55:09 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15001
2020-12-01 20:55:09 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15001
2020-12-01 20:55:10 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-01 20:55:10 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-01 20:55:10 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-01 20:55:16 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='closer_dropout', attention_dropout=0.0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='r3f_closer_dropout', cross_self_attention=False, curriculum=0, data='./examples/transformer_enzh/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:15001', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=3024, max_tokens_valid=3024, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.05, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='./examples/transformer_enzh/bash/../checkpoints/closer', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='ch', stop_time_hours=0, target_lang='en', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-12-01 20:55:16 | INFO | fairseq.tasks.translation | [ch] dictionary: 27920 types
2020-12-01 20:55:16 | INFO | fairseq.tasks.translation | [en] dictionary: 19376 types
2020-12-01 20:55:16 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/transformer_enzh/bash/../data-bin/valid.ch-en.ch
2020-12-01 20:55:16 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/transformer_enzh/bash/../data-bin/valid.ch-en.en
2020-12-01 20:55:16 | INFO | fairseq.tasks.translation | ./examples/transformer_enzh/bash/../data-bin valid ch-en 1664 examples
2020-12-01 20:55:20 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(27920, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19376, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19376, bias=False)
  )
)
2020-12-01 20:55:20 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-01 20:55:20 | INFO | fairseq_cli.train | model: closer_dropout (TransformerModel)
2020-12-01 20:55:20 | INFO | fairseq_cli.train | criterion: r3f_closer_dropout (R3fCloserDropout)
2020-12-01 20:55:20 | INFO | fairseq_cli.train | num. model params: 78274560 (num. trained: 78274560)
2020-12-01 20:55:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-01 20:55:20 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-01 20:55:20 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-01 20:55:20 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-01 20:55:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-01 20:55:20 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-12-01 20:55:20 | INFO | fairseq_cli.train | max tokens per GPU = 3024 and max sentences per GPU = None
2020-12-01 20:55:21 | INFO | fairseq.trainer | loaded checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 41 @ 0 updates)
2020-12-01 20:55:21 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-01 20:55:21 | INFO | fairseq.trainer | loading train data for epoch 41
2020-12-01 20:55:21 | INFO | fairseq.data.data_utils | loaded 200000 examples from: ./examples/transformer_enzh/bash/../data-bin/train.ch-en.ch
2020-12-01 20:55:21 | INFO | fairseq.data.data_utils | loaded 200000 examples from: ./examples/transformer_enzh/bash/../data-bin/train.ch-en.en
2020-12-01 20:55:21 | INFO | fairseq.tasks.translation | ./examples/transformer_enzh/bash/../data-bin train ch-en 200000 examples
111
111
2020-12-01 20:55:22 | INFO | fairseq.trainer | begin training epoch 41
111
2020-12-01 20:55:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 20:55:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 20:55:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 20:55:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 20:55:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 20:55:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:00:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 21:00:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:00:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:00:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:00:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:00:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:00:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:00:56 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-01 21:00:56 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-01 21:00:56 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-01 21:00:57 | INFO | valid | epoch 041 | valid on 'valid' subset | symm_mse 0 | loss 9.757 | nll_loss 8.725 | ppl 423.09 | bleu 11.76 | wps 3450.4 | wpb 4629 | bsz 118.9 | num_updates 719
2020-12-01 21:00:57 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 21:00:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:00:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:01:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:01:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:01:04 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint41.pt (epoch 41 @ 719 updates, score 9.757) (writing took 7.436116307973862 seconds)
2020-12-01 21:01:04 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2020-12-01 21:01:04 | INFO | train | epoch 041 | loss 3.357 | nll_loss 1.557 | symm_mse 1.054 | ppl 2.94 | wps 16566.1 | ups 2.03 | wpb 8159.9 | bsz 278.2 | num_updates 719 | lr 1.8057e-05 | gnorm 0.758 | train_wall 630 | wall 0
111
2020-12-01 21:01:04 | INFO | fairseq.trainer | begin training epoch 42
2020-12-01 21:01:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:01:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:06:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 21:06:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:06:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:06:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:06:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:06:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:06:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:06:24 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-01 21:06:24 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-01 21:06:24 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-01 21:06:25 | INFO | valid | epoch 042 | valid on 'valid' subset | symm_mse 0 | loss 9.756 | nll_loss 8.729 | ppl 424.18 | bleu 11.85 | wps 3534.2 | wpb 4629 | bsz 118.9 | num_updates 1438 | best_loss 9.756
2020-12-01 21:06:25 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 21:06:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:06:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:06:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:06:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:06:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint42.pt (epoch 42 @ 1438 updates, score 9.756) (writing took 7.629692645743489 seconds)
2020-12-01 21:06:33 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2020-12-01 21:06:33 | INFO | train | epoch 042 | loss 3.343 | nll_loss 1.543 | symm_mse 1.04 | ppl 2.91 | wps 17841.2 | ups 2.19 | wpb 8159.9 | bsz 278.2 | num_updates 1438 | lr 3.60141e-05 | gnorm 0.76 | train_wall 294 | wall 0
111
2020-12-01 21:06:33 | INFO | fairseq.trainer | begin training epoch 43
2020-12-01 21:06:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:06:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:11:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 21:11:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:11:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:11:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:11:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:11:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:11:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:11:54 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-01 21:11:54 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-01 21:11:54 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-01 21:11:55 | INFO | valid | epoch 043 | valid on 'valid' subset | symm_mse 0 | loss 9.742 | nll_loss 8.715 | ppl 420.21 | bleu 11.85 | wps 3445.6 | wpb 4629 | bsz 118.9 | num_updates 2157 | best_loss 9.742
2020-12-01 21:11:55 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 21:11:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:11:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:11:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:11:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:12:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint43.pt (epoch 43 @ 2157 updates, score 9.742) (writing took 7.170524561777711 seconds)
2020-12-01 21:12:02 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2020-12-01 21:12:02 | INFO | train | epoch 043 | loss 3.343 | nll_loss 1.547 | symm_mse 1.03 | ppl 2.92 | wps 17832.4 | ups 2.19 | wpb 8159.9 | bsz 278.2 | num_updates 2157 | lr 5.39711e-05 | gnorm 0.772 | train_wall 294 | wall 0
111
2020-12-01 21:12:02 | INFO | fairseq.trainer | begin training epoch 44
2020-12-01 21:12:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:12:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:17:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 21:17:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:17:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:17:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:17:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:17:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:17:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:17:23 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-01 21:17:23 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-01 21:17:23 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-01 21:17:24 | INFO | valid | epoch 044 | valid on 'valid' subset | symm_mse 0 | loss 9.775 | nll_loss 8.754 | ppl 431.8 | bleu 11.71 | wps 3433.3 | wpb 4629 | bsz 118.9 | num_updates 2876 | best_loss 9.742
2020-12-01 21:17:24 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 21:17:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:17:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:17:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:17:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:17:29 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint44.pt (epoch 44 @ 2876 updates, score 9.775) (writing took 4.895731607452035 seconds)
2020-12-01 21:17:29 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2020-12-01 21:17:29 | INFO | train | epoch 044 | loss 3.343 | nll_loss 1.549 | symm_mse 1.022 | ppl 2.93 | wps 17945.8 | ups 2.2 | wpb 8159.9 | bsz 278.2 | num_updates 2876 | lr 7.19281e-05 | gnorm 0.781 | train_wall 294 | wall 0
111
2020-12-01 21:17:29 | INFO | fairseq.trainer | begin training epoch 45
2020-12-01 21:17:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:17:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:22:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 21:22:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:22:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:22:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:22:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:22:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:22:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:22:48 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')
2020-12-01 21:22:48 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.
2020-12-01 21:22:48 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
2020-12-01 21:22:49 | INFO | valid | epoch 045 | valid on 'valid' subset | symm_mse 0 | loss 9.777 | nll_loss 8.756 | ppl 432.45 | bleu 11.69 | wps 3601.8 | wpb 4629 | bsz 118.9 | num_updates 3595 | best_loss 9.742
2020-12-01 21:22:49 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 21:22:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:22:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:22:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:22:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:22:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint45.pt (epoch 45 @ 3595 updates, score 9.777) (writing took 4.950438655912876 seconds)
2020-12-01 21:22:54 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2020-12-01 21:22:54 | INFO | train | epoch 045 | loss 3.344 | nll_loss 1.552 | symm_mse 1.015 | ppl 2.93 | wps 18048.4 | ups 2.21 | wpb 8159.9 | bsz 278.2 | num_updates 3595 | lr 8.98851e-05 | gnorm 0.793 | train_wall 293 | wall 0
111
2020-12-01 21:22:54 | INFO | fairseq.trainer | begin training epoch 46
2020-12-01 21:22:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:22:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:25:13 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:25:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:25:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:25:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:25:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:25:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:25:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:25:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:25:19 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:11914
2020-12-01 21:25:19 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:11914
2020-12-01 21:25:19 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-01 21:25:20 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:11914
2020-12-01 21:25:20 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-01 21:25:20 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-01 21:25:24 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='closer_dropout', attention_dropout=0.0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='r3f_closer_dropout', cross_self_attention=False, curriculum=0, data='./examples/transformer_enzh/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:11914', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=3024, max_tokens_valid=3024, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.05, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='./examples/transformer_enzh/bash/../checkpoints/closer', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='ch', stop_time_hours=0, target_lang='en', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-12-01 21:25:24 | INFO | fairseq.tasks.translation | [ch] dictionary: 27920 types
2020-12-01 21:25:24 | INFO | fairseq.tasks.translation | [en] dictionary: 19376 types
2020-12-01 21:25:24 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/transformer_enzh/bash/../data-bin/valid.ch-en.ch
2020-12-01 21:25:24 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/transformer_enzh/bash/../data-bin/valid.ch-en.en
2020-12-01 21:25:24 | INFO | fairseq.tasks.translation | ./examples/transformer_enzh/bash/../data-bin valid ch-en 1664 examples
2020-12-01 21:25:26 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(27920, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19376, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19376, bias=False)
  )
)
2020-12-01 21:25:26 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-01 21:25:26 | INFO | fairseq_cli.train | model: closer_dropout (TransformerModel)
2020-12-01 21:25:26 | INFO | fairseq_cli.train | criterion: r3f_closer_dropout (R3fCloserDropout)
2020-12-01 21:25:26 | INFO | fairseq_cli.train | num. model params: 78274560 (num. trained: 78274560)
2020-12-01 21:25:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-01 21:25:26 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-01 21:25:26 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-01 21:25:26 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-01 21:25:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-01 21:25:26 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-12-01 21:25:26 | INFO | fairseq_cli.train | max tokens per GPU = 3024 and max sentences per GPU = None
2020-12-01 21:25:27 | INFO | fairseq.trainer | loaded checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 46 @ 0 updates)
2020-12-01 21:25:27 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-01 21:25:27 | INFO | fairseq.trainer | loading train data for epoch 46
2020-12-01 21:25:27 | INFO | fairseq.data.data_utils | loaded 200000 examples from: ./examples/transformer_enzh/bash/../data-bin/train.ch-en.ch
2020-12-01 21:25:27 | INFO | fairseq.data.data_utils | loaded 200000 examples from: ./examples/transformer_enzh/bash/../data-bin/train.ch-en.en
2020-12-01 21:25:27 | INFO | fairseq.tasks.translation | ./examples/transformer_enzh/bash/../data-bin train ch-en 200000 examples
111
111
2020-12-01 21:25:28 | INFO | fairseq.trainer | begin training epoch 46
111
2020-12-01 21:25:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:25:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:25:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:25:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:25:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:25:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:30:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 21:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:30:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:30:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:30:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:30:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:30:49 | INFO | valid | epoch 046 | valid on 'valid' subset | symm_mse 0 | loss 9.775 | nll_loss 8.754 | ppl 431.8 | bleu 11.87 | wps 3573.9 | wpb 4629 | bsz 118.9 | num_updates 719
2020-12-01 21:30:49 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 21:30:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:30:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:30:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:30:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:30:56 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_best.pt (epoch 46 @ 719 updates, score 9.775) (writing took 6.943225111812353 seconds)
2020-12-01 21:30:56 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2020-12-01 21:30:56 | INFO | train | epoch 046 | loss 3.319 | nll_loss 1.523 | symm_mse 1.014 | ppl 2.87 | wps 18067.5 | ups 2.21 | wpb 8159.9 | bsz 278.2 | num_updates 719 | lr 1.8057e-05 | gnorm 0.778 | train_wall 588 | wall 0
111
2020-12-01 21:30:56 | INFO | fairseq.trainer | begin training epoch 47
2020-12-01 21:30:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:30:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:34:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:34:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:34:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:34:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:34:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:34:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:34:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:34:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:34:25 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:19137
2020-12-01 21:34:25 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:19137
2020-12-01 21:34:25 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:19137
2020-12-01 21:34:25 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-01 21:34:26 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-01 21:34:26 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-01 21:34:30 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='closer_dropout', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='r3f_closer_dropout', cross_self_attention=False, curriculum=0, data='./examples/transformer_enzh/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:19137', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=3024, max_tokens_valid=3024, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.05, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='./examples/transformer_enzh/bash/../checkpoints/closer', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='ch', stop_time_hours=0, target_lang='en', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-12-01 21:34:30 | INFO | fairseq.tasks.translation | [ch] dictionary: 27920 types
2020-12-01 21:34:30 | INFO | fairseq.tasks.translation | [en] dictionary: 19376 types
2020-12-01 21:34:30 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/transformer_enzh/bash/../data-bin/valid.ch-en.ch
2020-12-01 21:34:30 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/transformer_enzh/bash/../data-bin/valid.ch-en.en
2020-12-01 21:34:30 | INFO | fairseq.tasks.translation | ./examples/transformer_enzh/bash/../data-bin valid ch-en 1664 examples
2020-12-01 21:34:32 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(27920, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19376, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19376, bias=False)
  )
)
2020-12-01 21:34:32 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-01 21:34:32 | INFO | fairseq_cli.train | model: closer_dropout (TransformerModel)
2020-12-01 21:34:32 | INFO | fairseq_cli.train | criterion: r3f_closer_dropout (R3fCloserDropout)
2020-12-01 21:34:32 | INFO | fairseq_cli.train | num. model params: 78274560 (num. trained: 78274560)
2020-12-01 21:34:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-01 21:34:32 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-01 21:34:32 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-01 21:34:32 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-01 21:34:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-01 21:34:32 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-12-01 21:34:32 | INFO | fairseq_cli.train | max tokens per GPU = 3024 and max sentences per GPU = None
2020-12-01 21:34:33 | INFO | fairseq.trainer | loaded checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 47 @ 0 updates)
2020-12-01 21:34:33 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-01 21:34:33 | INFO | fairseq.trainer | loading train data for epoch 47
2020-12-01 21:34:33 | INFO | fairseq.data.data_utils | loaded 200000 examples from: ./examples/transformer_enzh/bash/../data-bin/train.ch-en.ch
2020-12-01 21:34:33 | INFO | fairseq.data.data_utils | loaded 200000 examples from: ./examples/transformer_enzh/bash/../data-bin/train.ch-en.en
2020-12-01 21:34:33 | INFO | fairseq.tasks.translation | ./examples/transformer_enzh/bash/../data-bin train ch-en 200000 examples
111
111
111
2020-12-01 21:34:33 | INFO | fairseq.trainer | begin training epoch 47
2020-12-01 21:34:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:34:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:34:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:34:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:34:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:34:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:39:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 21:39:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:39:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:39:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:39:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:39:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:39:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:39:56 | INFO | valid | epoch 047 | valid on 'valid' subset | symm_mse 0 | loss 9.784 | nll_loss 8.766 | ppl 435.37 | bleu 11.7 | wps 3413.4 | wpb 4629 | bsz 118.9 | num_updates 719
2020-12-01 21:39:56 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 21:39:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:39:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:39:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:39:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:40:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_best.pt (epoch 47 @ 719 updates, score 11.7) (writing took 6.638414425775409 seconds)
2020-12-01 21:40:03 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2020-12-01 21:40:03 | INFO | train | epoch 047 | loss 3.305 | nll_loss 1.508 | symm_mse 1.014 | ppl 2.84 | wps 18102.8 | ups 2.22 | wpb 8159.9 | bsz 278.2 | num_updates 719 | lr 1.8057e-05 | gnorm 0.772 | train_wall 882 | wall 0
111
2020-12-01 21:40:03 | INFO | fairseq.trainer | begin training epoch 48
2020-12-01 21:40:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:40:05 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:45:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 21:45:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:45:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:45:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:45:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:45:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:45:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:45:24 | INFO | valid | epoch 048 | valid on 'valid' subset | symm_mse 0 | loss 9.777 | nll_loss 8.758 | ppl 432.97 | bleu 11.89 | wps 3569.5 | wpb 4629 | bsz 118.9 | num_updates 1438 | best_bleu 11.89
2020-12-01 21:45:24 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 21:45:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:45:25 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:45:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:45:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:45:31 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_best.pt (epoch 48 @ 1438 updates, score 11.89) (writing took 6.552708489820361 seconds)
2020-12-01 21:45:31 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2020-12-01 21:45:31 | INFO | train | epoch 048 | loss 3.283 | nll_loss 1.484 | symm_mse 1.013 | ppl 2.8 | wps 17890.6 | ups 2.19 | wpb 8159.9 | bsz 278.2 | num_updates 1438 | lr 3.60141e-05 | gnorm 0.767 | train_wall 294 | wall 0
111
2020-12-01 21:45:31 | INFO | fairseq.trainer | begin training epoch 49
2020-12-01 21:45:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:45:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:47:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:47:28 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:47:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:47:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:47:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:47:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:47:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:47:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:47:32 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15157
2020-12-01 21:47:32 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15157
2020-12-01 21:47:32 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 1
2020-12-01 21:47:33 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:15157
2020-12-01 21:47:33 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 2
2020-12-01 21:47:33 | INFO | fairseq.distributed_utils | initialized host inspur129 as rank 0
2020-12-01 21:47:37 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='closer_dropout', attention_dropout=0.0, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='r3f_closer_dropout', cross_self_attention=False, curriculum=0, data='./examples/transformer_enzh/bash/../data-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:15157', distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, distributed_wrapper='DDP', dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eps=1e-05, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=100, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=3024, max_tokens_valid=3024, max_update=0, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=True, no_token_positional_embeddings=False, noise_type='normal', nprocs_per_node=3, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, r3f_lambda=0.5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='./examples/transformer_enzh/bash/../checkpoints/closer', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='ch', stop_time_hours=0, target_lang='en', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0, zero_sharding='none')
2020-12-01 21:47:37 | INFO | fairseq.tasks.translation | [ch] dictionary: 27920 types
2020-12-01 21:47:37 | INFO | fairseq.tasks.translation | [en] dictionary: 19376 types
2020-12-01 21:47:37 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/transformer_enzh/bash/../data-bin/valid.ch-en.ch
2020-12-01 21:47:37 | INFO | fairseq.data.data_utils | loaded 1664 examples from: ./examples/transformer_enzh/bash/../data-bin/valid.ch-en.en
2020-12-01 21:47:37 | INFO | fairseq.tasks.translation | ./examples/transformer_enzh/bash/../data-bin valid ch-en 1664 examples
2020-12-01 21:47:39 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(27920, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(19376, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=19376, bias=False)
  )
)
2020-12-01 21:47:39 | INFO | fairseq_cli.train | task: translation (TranslationTask)
2020-12-01 21:47:39 | INFO | fairseq_cli.train | model: closer_dropout (TransformerModel)
2020-12-01 21:47:39 | INFO | fairseq_cli.train | criterion: r3f_closer_dropout (R3fCloserDropout)
2020-12-01 21:47:39 | INFO | fairseq_cli.train | num. model params: 78274560 (num. trained: 78274560)
2020-12-01 21:47:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-01 21:47:39 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-01 21:47:39 | INFO | fairseq.utils | rank   1: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-01 21:47:39 | INFO | fairseq.utils | rank   2: capabilities =  6.1  ; total memory = 10.917 GB ; name = GeForce GTX 1080 Ti                     
2020-12-01 21:47:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 3 workers***********************
2020-12-01 21:47:39 | INFO | fairseq_cli.train | training on 3 devices (GPUs/TPUs)
2020-12-01 21:47:39 | INFO | fairseq_cli.train | max tokens per GPU = 3024 and max sentences per GPU = None
2020-12-01 21:47:40 | INFO | fairseq.trainer | loaded checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 49 @ 0 updates)
2020-12-01 21:47:40 | INFO | fairseq.optim.adam | using FusedAdam
2020-12-01 21:47:40 | INFO | fairseq.trainer | loading train data for epoch 49
2020-12-01 21:47:40 | INFO | fairseq.data.data_utils | loaded 200000 examples from: ./examples/transformer_enzh/bash/../data-bin/train.ch-en.ch
2020-12-01 21:47:40 | INFO | fairseq.data.data_utils | loaded 200000 examples from: ./examples/transformer_enzh/bash/../data-bin/train.ch-en.en
2020-12-01 21:47:40 | INFO | fairseq.tasks.translation | ./examples/transformer_enzh/bash/../data-bin train ch-en 200000 examples
111
2020-12-01 21:47:40 | INFO | fairseq.trainer | begin training epoch 49
111
111
2020-12-01 21:47:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:47:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:47:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:47:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:47:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:47:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:52:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 21:52:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:52:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:52:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:52:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:52:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:52:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:53:03 | INFO | valid | epoch 049 | valid on 'valid' subset | symm_mse 0 | loss 9.409 | nll_loss 8.443 | ppl 348.05 | bleu 12.4 | wps 3341.1 | wpb 4629 | bsz 118.9 | num_updates 719
2020-12-01 21:53:03 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 21:53:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:53:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:53:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:53:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:53:10 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_best.pt (epoch 49 @ 719 updates, score 12.4) (writing took 6.854849174618721 seconds)
2020-12-01 21:53:10 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2020-12-01 21:53:10 | INFO | train | epoch 049 | loss 3.554 | nll_loss 1.549 | symm_mse 0.894 | ppl 2.93 | wps 18003.6 | ups 2.21 | wpb 8159.9 | bsz 278.2 | num_updates 719 | lr 1.8057e-05 | gnorm 0.776 | train_wall 589 | wall 0
111
2020-12-01 21:53:10 | INFO | fairseq.trainer | begin training epoch 50
2020-12-01 21:53:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:53:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:58:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 21:58:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:58:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:58:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:58:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:58:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:58:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:58:37 | INFO | valid | epoch 050 | valid on 'valid' subset | symm_mse 0 | loss 9.364 | nll_loss 8.396 | ppl 336.75 | bleu 12.18 | wps 3085.5 | wpb 4629 | bsz 118.9 | num_updates 1438 | best_bleu 12.4
2020-12-01 21:58:37 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 21:58:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:58:38 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:58:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:58:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 21:58:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 50 @ 1438 updates, score 12.18) (writing took 4.313698736950755 seconds)
2020-12-01 21:58:41 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2020-12-01 21:58:41 | INFO | train | epoch 050 | loss 3.764 | nll_loss 1.667 | symm_mse 0.641 | ppl 3.18 | wps 17723 | ups 2.17 | wpb 8159.9 | bsz 278.2 | num_updates 1438 | lr 3.60141e-05 | gnorm 0.714 | train_wall 297 | wall 0
111
2020-12-01 21:58:41 | INFO | fairseq.trainer | begin training epoch 51
2020-12-01 21:58:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 21:58:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:03:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 22:03:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:03:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:03:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:03:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:03:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:03:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:04:04 | INFO | valid | epoch 051 | valid on 'valid' subset | symm_mse 0 | loss 9.341 | nll_loss 8.374 | ppl 331.72 | bleu 12.16 | wps 3516.3 | wpb 4629 | bsz 118.9 | num_updates 2157 | best_bleu 12.4
2020-12-01 22:04:04 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 22:04:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:04:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:04:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:04:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:04:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 51 @ 2157 updates, score 12.16) (writing took 3.7430192083120346 seconds)
2020-12-01 22:04:07 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2020-12-01 22:04:07 | INFO | train | epoch 051 | loss 3.731 | nll_loss 1.679 | symm_mse 0.579 | ppl 3.2 | wps 17999.4 | ups 2.21 | wpb 8159.9 | bsz 278.2 | num_updates 2157 | lr 5.39711e-05 | gnorm 0.723 | train_wall 295 | wall 0
111
2020-12-01 22:04:07 | INFO | fairseq.trainer | begin training epoch 52
2020-12-01 22:04:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:04:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:09:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 22:09:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:09:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:09:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:09:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:09:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:09:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:09:29 | INFO | valid | epoch 052 | valid on 'valid' subset | symm_mse 0 | loss 9.341 | nll_loss 8.372 | ppl 331.38 | bleu 12.37 | wps 3304.2 | wpb 4629 | bsz 118.9 | num_updates 2876 | best_bleu 12.4
2020-12-01 22:09:29 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 22:09:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:09:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:09:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:09:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:09:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 52 @ 2876 updates, score 12.37) (writing took 3.7523569352924824 seconds)
2020-12-01 22:09:33 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2020-12-01 22:09:33 | INFO | train | epoch 052 | loss 3.703 | nll_loss 1.684 | symm_mse 0.531 | ppl 3.21 | wps 18030.1 | ups 2.21 | wpb 8159.9 | bsz 278.2 | num_updates 2876 | lr 7.19281e-05 | gnorm 0.738 | train_wall 294 | wall 0
111
2020-12-01 22:09:33 | INFO | fairseq.trainer | begin training epoch 53
2020-12-01 22:09:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:09:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:14:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 22:14:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:14:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:14:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:14:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:14:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:14:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:14:57 | INFO | valid | epoch 053 | valid on 'valid' subset | symm_mse 0 | loss 9.365 | nll_loss 8.39 | ppl 335.37 | bleu 12.23 | wps 3252.6 | wpb 4629 | bsz 118.9 | num_updates 3595 | best_bleu 12.4
2020-12-01 22:14:57 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 22:14:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:14:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:15:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:15:00 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:15:01 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 53 @ 3595 updates, score 12.23) (writing took 4.211794635280967 seconds)
2020-12-01 22:15:01 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2020-12-01 22:15:01 | INFO | train | epoch 053 | loss 3.678 | nll_loss 1.686 | symm_mse 0.492 | ppl 3.22 | wps 17875.4 | ups 2.19 | wpb 8159.9 | bsz 278.2 | num_updates 3595 | lr 8.98851e-05 | gnorm 0.753 | train_wall 295 | wall 0
111
2020-12-01 22:15:01 | INFO | fairseq.trainer | begin training epoch 54
2020-12-01 22:15:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:15:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:20:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 22:20:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:20:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:20:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:20:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:20:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:20:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:20:23 | INFO | valid | epoch 054 | valid on 'valid' subset | symm_mse 0 | loss 9.378 | nll_loss 8.405 | ppl 338.91 | bleu 12.23 | wps 3385 | wpb 4629 | bsz 118.9 | num_updates 4314 | best_bleu 12.4
2020-12-01 22:20:23 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 22:20:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:20:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:20:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:20:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:20:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 54 @ 4314 updates, score 12.23) (writing took 4.013868249952793 seconds)
2020-12-01 22:20:27 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2020-12-01 22:20:27 | INFO | train | epoch 054 | loss 3.656 | nll_loss 1.687 | symm_mse 0.458 | ppl 3.22 | wps 17967.1 | ups 2.2 | wpb 8159.9 | bsz 278.2 | num_updates 4314 | lr 9.62919e-05 | gnorm 0.764 | train_wall 295 | wall 0
111
2020-12-01 22:20:27 | INFO | fairseq.trainer | begin training epoch 55
2020-12-01 22:20:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:20:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:25:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 22:25:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:25:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:25:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:25:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:25:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:25:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:25:50 | INFO | valid | epoch 055 | valid on 'valid' subset | symm_mse 0 | loss 9.324 | nll_loss 8.353 | ppl 326.89 | bleu 12.21 | wps 3495.1 | wpb 4629 | bsz 118.9 | num_updates 5033 | best_bleu 12.4
2020-12-01 22:25:50 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 22:25:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:25:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:25:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:25:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:25:54 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 55 @ 5033 updates, score 12.21) (writing took 4.084249207749963 seconds)
2020-12-01 22:25:54 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2020-12-01 22:25:54 | INFO | train | epoch 055 | loss 3.623 | nll_loss 1.669 | symm_mse 0.431 | ppl 3.18 | wps 17961.2 | ups 2.2 | wpb 8159.9 | bsz 278.2 | num_updates 5033 | lr 8.9149e-05 | gnorm 0.769 | train_wall 295 | wall 0
111
2020-12-01 22:25:54 | INFO | fairseq.trainer | begin training epoch 56
2020-12-01 22:25:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:25:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:30:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 22:30:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:30:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:30:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:30:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:30:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:30:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:31:16 | INFO | valid | epoch 056 | valid on 'valid' subset | symm_mse 0 | loss 9.341 | nll_loss 8.367 | ppl 330.08 | bleu 12.25 | wps 3492.2 | wpb 4629 | bsz 118.9 | num_updates 5752 | best_bleu 12.4
2020-12-01 22:31:16 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 22:31:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:31:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:31:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:31:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:31:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 56 @ 5752 updates, score 12.25) (writing took 4.120411643758416 seconds)
2020-12-01 22:31:21 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2020-12-01 22:31:21 | INFO | train | epoch 056 | loss 3.588 | nll_loss 1.644 | symm_mse 0.412 | ppl 3.13 | wps 17975.4 | ups 2.2 | wpb 8159.9 | bsz 278.2 | num_updates 5752 | lr 8.33913e-05 | gnorm 0.768 | train_wall 295 | wall 0
111
2020-12-01 22:31:21 | INFO | fairseq.trainer | begin training epoch 57
2020-12-01 22:31:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:31:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:36:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 22:36:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:36:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:36:21 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:36:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:36:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:36:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:36:42 | INFO | valid | epoch 057 | valid on 'valid' subset | symm_mse 0 | loss 9.375 | nll_loss 8.404 | ppl 338.73 | bleu 12.17 | wps 3531.6 | wpb 4629 | bsz 118.9 | num_updates 6471 | best_bleu 12.4
2020-12-01 22:36:42 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 22:36:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:36:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:36:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:36:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:36:46 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 57 @ 6471 updates, score 12.17) (writing took 4.074216924607754 seconds)
2020-12-01 22:36:46 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2020-12-01 22:36:46 | INFO | train | epoch 057 | loss 3.56 | nll_loss 1.623 | symm_mse 0.397 | ppl 3.08 | wps 18020.3 | ups 2.21 | wpb 8159.9 | bsz 278.2 | num_updates 6471 | lr 7.8622e-05 | gnorm 0.769 | train_wall 294 | wall 0
111
2020-12-01 22:36:46 | INFO | fairseq.trainer | begin training epoch 58
2020-12-01 22:36:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:36:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:41:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 22:41:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:41:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:41:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:41:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:41:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:41:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:42:09 | INFO | valid | epoch 058 | valid on 'valid' subset | symm_mse 0 | loss 9.44 | nll_loss 8.469 | ppl 354.38 | bleu 12.17 | wps 3534.5 | wpb 4629 | bsz 118.9 | num_updates 7190 | best_bleu 12.4
2020-12-01 22:42:09 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 22:42:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:42:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:42:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:42:12 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:42:13 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 58 @ 7190 updates, score 12.17) (writing took 4.128575844690204 seconds)
2020-12-01 22:42:13 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2020-12-01 22:42:13 | INFO | train | epoch 058 | loss 3.534 | nll_loss 1.603 | symm_mse 0.385 | ppl 3.04 | wps 17953 | ups 2.2 | wpb 8159.9 | bsz 278.2 | num_updates 7190 | lr 7.45874e-05 | gnorm 0.769 | train_wall 295 | wall 0
111
2020-12-01 22:42:13 | INFO | fairseq.trainer | begin training epoch 59
2020-12-01 22:42:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:42:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:47:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 22:47:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:47:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:47:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:47:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:47:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:47:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:47:35 | INFO | valid | epoch 059 | valid on 'valid' subset | symm_mse 0 | loss 9.413 | nll_loss 8.442 | ppl 347.77 | bleu 12.15 | wps 3524.9 | wpb 4629 | bsz 118.9 | num_updates 7909 | best_bleu 12.4
2020-12-01 22:47:35 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 22:47:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:47:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:47:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:47:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:47:39 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 59 @ 7909 updates, score 12.15) (writing took 4.236019721254706 seconds)
2020-12-01 22:47:39 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2020-12-01 22:47:39 | INFO | train | epoch 059 | loss 3.512 | nll_loss 1.585 | symm_mse 0.375 | ppl 3 | wps 17987.6 | ups 2.2 | wpb 8159.9 | bsz 278.2 | num_updates 7909 | lr 7.11163e-05 | gnorm 0.771 | train_wall 294 | wall 0
111
2020-12-01 22:47:39 | INFO | fairseq.trainer | begin training epoch 60
2020-12-01 22:47:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:47:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:52:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 22:52:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:52:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:52:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:52:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:52:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:52:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:53:03 | INFO | valid | epoch 060 | valid on 'valid' subset | symm_mse 0 | loss 9.43 | nll_loss 8.453 | ppl 350.48 | bleu 12.23 | wps 3297 | wpb 4629 | bsz 118.9 | num_updates 8628 | best_bleu 12.4
2020-12-01 22:53:03 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 22:53:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:53:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:53:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:53:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:53:07 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 60 @ 8628 updates, score 12.23) (writing took 4.12744159437716 seconds)
2020-12-01 22:53:07 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2020-12-01 22:53:07 | INFO | train | epoch 060 | loss 3.492 | nll_loss 1.568 | symm_mse 0.367 | ppl 2.97 | wps 17868.1 | ups 2.19 | wpb 8159.9 | bsz 278.2 | num_updates 8628 | lr 6.80887e-05 | gnorm 0.773 | train_wall 296 | wall 0
111
2020-12-01 22:53:07 | INFO | fairseq.trainer | begin training epoch 61
2020-12-01 22:53:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:53:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:58:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 22:58:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:58:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:58:08 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:58:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:58:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:58:10 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:58:29 | INFO | valid | epoch 061 | valid on 'valid' subset | symm_mse 0 | loss 9.472 | nll_loss 8.5 | ppl 362.07 | bleu 11.95 | wps 3499.2 | wpb 4629 | bsz 118.9 | num_updates 9347 | best_bleu 12.4
2020-12-01 22:58:29 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 22:58:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:58:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:58:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:58:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 61 @ 9347 updates, score 11.95) (writing took 4.292814217507839 seconds)
2020-12-01 22:58:33 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2020-12-01 22:58:33 | INFO | train | epoch 061 | loss 3.474 | nll_loss 1.553 | symm_mse 0.359 | ppl 2.94 | wps 18015.5 | ups 2.21 | wpb 8159.9 | bsz 278.2 | num_updates 9347 | lr 6.54175e-05 | gnorm 0.776 | train_wall 294 | wall 0
111
2020-12-01 22:58:33 | INFO | fairseq.trainer | begin training epoch 62
2020-12-01 22:58:34 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 22:58:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 22:58:36 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:03:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 23:03:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:03:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:03:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:03:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:03:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:03:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:03:56 | INFO | valid | epoch 062 | valid on 'valid' subset | symm_mse 0 | loss 9.428 | nll_loss 8.453 | ppl 350.55 | bleu 12.34 | wps 3497.1 | wpb 4629 | bsz 118.9 | num_updates 10066 | best_bleu 12.4
2020-12-01 23:03:56 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 23:03:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:03:57 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:03:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:03:59 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:04:00 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 62 @ 10066 updates, score 12.34) (writing took 4.246222918853164 seconds)
2020-12-01 23:04:00 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2020-12-01 23:04:00 | INFO | train | epoch 062 | loss 3.456 | nll_loss 1.538 | symm_mse 0.353 | ppl 2.9 | wps 17932 | ups 2.2 | wpb 8159.9 | bsz 278.2 | num_updates 10066 | lr 6.30379e-05 | gnorm 0.777 | train_wall 295 | wall 0
111
2020-12-01 23:04:00 | INFO | fairseq.trainer | begin training epoch 63
2020-12-01 23:04:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:04:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:09:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 23:09:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:09:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:09:02 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:09:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:09:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:09:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:09:23 | INFO | valid | epoch 063 | valid on 'valid' subset | symm_mse 0 | loss 9.484 | nll_loss 8.512 | ppl 365.01 | bleu 12.17 | wps 3437.9 | wpb 4629 | bsz 118.9 | num_updates 10785 | best_bleu 12.4
2020-12-01 23:09:23 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 23:09:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:09:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:09:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:09:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:09:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 63 @ 10785 updates, score 12.17) (writing took 4.226629573851824 seconds)
2020-12-01 23:09:27 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2020-12-01 23:09:27 | INFO | train | epoch 063 | loss 3.441 | nll_loss 1.524 | symm_mse 0.347 | ppl 2.88 | wps 17934.6 | ups 2.2 | wpb 8159.9 | bsz 278.2 | num_updates 10785 | lr 6.09004e-05 | gnorm 0.779 | train_wall 295 | wall 0
111
2020-12-01 23:09:27 | INFO | fairseq.trainer | begin training epoch 64
2020-12-01 23:09:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:09:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:14:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 23:14:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:14:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:14:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:14:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:14:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:14:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:14:50 | INFO | valid | epoch 064 | valid on 'valid' subset | symm_mse 0 | loss 9.496 | nll_loss 8.53 | ppl 369.56 | bleu 12.09 | wps 3454.9 | wpb 4629 | bsz 118.9 | num_updates 11504 | best_bleu 12.4
2020-12-01 23:14:50 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 23:14:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:14:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:14:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:14:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:14:55 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 64 @ 11504 updates, score 12.09) (writing took 4.288520812988281 seconds)
2020-12-01 23:14:55 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2020-12-01 23:14:55 | INFO | train | epoch 064 | loss 3.427 | nll_loss 1.512 | symm_mse 0.342 | ppl 2.85 | wps 17928.4 | ups 2.2 | wpb 8159.9 | bsz 278.2 | num_updates 11504 | lr 5.89665e-05 | gnorm 0.783 | train_wall 295 | wall 0
111
2020-12-01 23:14:55 | INFO | fairseq.trainer | begin training epoch 65
2020-12-01 23:14:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:14:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:19:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 23:19:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:19:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:19:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:19:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:19:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:19:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:20:17 | INFO | valid | epoch 065 | valid on 'valid' subset | symm_mse 0 | loss 9.471 | nll_loss 8.498 | ppl 361.6 | bleu 12.38 | wps 3522.3 | wpb 4629 | bsz 118.9 | num_updates 12223 | best_bleu 12.4
2020-12-01 23:20:17 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 23:20:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:20:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:20:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:20:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:20:21 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 65 @ 12223 updates, score 12.38) (writing took 3.9350977428257465 seconds)
2020-12-01 23:20:21 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2020-12-01 23:20:21 | INFO | train | epoch 065 | loss 3.413 | nll_loss 1.5 | symm_mse 0.337 | ppl 2.83 | wps 17986.6 | ups 2.2 | wpb 8159.9 | bsz 278.2 | num_updates 12223 | lr 5.72059e-05 | gnorm 0.785 | train_wall 295 | wall 0
111
2020-12-01 23:20:21 | INFO | fairseq.trainer | begin training epoch 66
2020-12-01 23:20:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:20:24 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:25:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 23:25:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:25:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:25:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:25:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:25:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:25:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:25:58 | INFO | valid | epoch 066 | valid on 'valid' subset | symm_mse 0 | loss 9.5 | nll_loss 8.531 | ppl 370.01 | bleu 12.25 | wps 2615.5 | wpb 4629 | bsz 118.9 | num_updates 12942 | best_bleu 12.4
2020-12-01 23:25:58 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 23:25:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:25:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:26:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:26:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:26:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 66 @ 12942 updates, score 12.25) (writing took 5.360321290791035 seconds)
2020-12-01 23:26:03 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2020-12-01 23:26:03 | INFO | train | epoch 066 | loss 3.4 | nll_loss 1.487 | symm_mse 0.333 | ppl 2.8 | wps 17147.8 | ups 2.1 | wpb 8159.9 | bsz 278.2 | num_updates 12942 | lr 5.55942e-05 | gnorm 0.783 | train_wall 301 | wall 0
111
2020-12-01 23:26:03 | INFO | fairseq.trainer | begin training epoch 67
2020-12-01 23:26:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:26:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:31:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 23:31:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:31:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:31:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:31:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:31:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:31:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:32:09 | INFO | valid | epoch 067 | valid on 'valid' subset | symm_mse 0 | loss 9.504 | nll_loss 8.539 | ppl 372 | bleu 12.34 | wps 2528.6 | wpb 4629 | bsz 118.9 | num_updates 13661 | best_bleu 12.4
2020-12-01 23:32:09 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 23:32:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:32:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:32:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:32:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:32:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 67 @ 13661 updates, score 12.34) (writing took 4.798175577074289 seconds)
2020-12-01 23:32:14 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2020-12-01 23:32:14 | INFO | train | epoch 067 | loss 3.388 | nll_loss 1.477 | symm_mse 0.329 | ppl 2.78 | wps 15822.8 | ups 1.94 | wpb 8159.9 | bsz 278.2 | num_updates 13661 | lr 5.41114e-05 | gnorm 0.787 | train_wall 326 | wall 0
111
2020-12-01 23:32:14 | INFO | fairseq.trainer | begin training epoch 68
2020-12-01 23:32:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:32:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:37:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 23:37:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:37:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:37:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:37:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:37:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:37:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:38:20 | INFO | valid | epoch 068 | valid on 'valid' subset | symm_mse 0 | loss 9.528 | nll_loss 8.56 | ppl 377.42 | bleu 12.11 | wps 2513.9 | wpb 4629 | bsz 118.9 | num_updates 14380 | best_bleu 12.4
2020-12-01 23:38:20 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 23:38:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:38:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:38:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:38:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:38:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 68 @ 14380 updates, score 12.11) (writing took 5.436845114454627 seconds)
2020-12-01 23:38:26 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2020-12-01 23:38:26 | INFO | train | epoch 068 | loss 3.377 | nll_loss 1.467 | symm_mse 0.325 | ppl 2.76 | wps 15771.9 | ups 1.93 | wpb 8159.9 | bsz 278.2 | num_updates 14380 | lr 5.27413e-05 | gnorm 0.788 | train_wall 326 | wall 0
111
2020-12-01 23:38:26 | INFO | fairseq.trainer | begin training epoch 69
2020-12-01 23:38:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:38:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:43:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 23:43:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:43:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:43:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:44:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:44:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:44:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:44:29 | INFO | valid | epoch 069 | valid on 'valid' subset | symm_mse 0 | loss 9.515 | nll_loss 8.548 | ppl 374.26 | bleu 12.3 | wps 2492.6 | wpb 4629 | bsz 118.9 | num_updates 15099 | best_bleu 12.4
2020-12-01 23:44:29 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 23:44:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:44:30 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:44:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:44:33 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 69 @ 15099 updates, score 12.3) (writing took 4.5526768285781145 seconds)
2020-12-01 23:44:33 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2020-12-01 23:44:33 | INFO | train | epoch 069 | loss 3.365 | nll_loss 1.456 | symm_mse 0.322 | ppl 2.74 | wps 15973.8 | ups 1.96 | wpb 8159.9 | bsz 278.2 | num_updates 15099 | lr 5.14702e-05 | gnorm 0.79 | train_wall 322 | wall 0
111
2020-12-01 23:44:33 | INFO | fairseq.trainer | begin training epoch 70
2020-12-01 23:44:33 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:44:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:44:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:50:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 23:50:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:50:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:50:12 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:50:15 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:50:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:50:16 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:50:42 | INFO | valid | epoch 070 | valid on 'valid' subset | symm_mse 0 | loss 9.531 | nll_loss 8.563 | ppl 378.27 | bleu 12.17 | wps 2454.4 | wpb 4629 | bsz 118.9 | num_updates 15818 | best_bleu 12.4
2020-12-01 23:50:42 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 23:50:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:50:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:50:47 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 70 @ 15818 updates, score 12.17) (writing took 4.904849607497454 seconds)
2020-12-01 23:50:47 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2020-12-01 23:50:47 | INFO | train | epoch 070 | loss 3.355 | nll_loss 1.447 | symm_mse 0.318 | ppl 2.73 | wps 15701.5 | ups 1.92 | wpb 8159.9 | bsz 278.2 | num_updates 15818 | lr 5.02868e-05 | gnorm 0.793 | train_wall 328 | wall 0
111
2020-12-01 23:50:47 | INFO | fairseq.trainer | begin training epoch 71
2020-12-01 23:50:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:50:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:50:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:50:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:56:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-01 23:56:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:56:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:56:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:56:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:56:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:56:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:56:53 | INFO | valid | epoch 071 | valid on 'valid' subset | symm_mse 0 | loss 9.549 | nll_loss 8.581 | ppl 382.93 | bleu 12.28 | wps 2382.1 | wpb 4629 | bsz 118.9 | num_updates 16537 | best_bleu 12.4
2020-12-01 23:56:53 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-01 23:56:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:56:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:56:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:56:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-01 23:56:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 71 @ 16537 updates, score 12.28) (writing took 5.394830033183098 seconds)
2020-12-01 23:56:59 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2020-12-01 23:56:59 | INFO | train | epoch 071 | loss 3.346 | nll_loss 1.438 | symm_mse 0.315 | ppl 2.71 | wps 15774.6 | ups 1.93 | wpb 8159.9 | bsz 278.2 | num_updates 16537 | lr 4.91815e-05 | gnorm 0.795 | train_wall 325 | wall 0
111
2020-12-01 23:56:59 | INFO | fairseq.trainer | begin training epoch 72
2020-12-01 23:57:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-01 23:57:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:02:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 00:02:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:02:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:02:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:02:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:02:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:02:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:03:03 | INFO | valid | epoch 072 | valid on 'valid' subset | symm_mse 0 | loss 9.557 | nll_loss 8.591 | ppl 385.64 | bleu 12.05 | wps 2592.4 | wpb 4629 | bsz 118.9 | num_updates 17256 | best_bleu 12.4
2020-12-02 00:03:03 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 00:03:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:03:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:03:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:03:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:03:08 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 72 @ 17256 updates, score 12.05) (writing took 5.112164873629808 seconds)
2020-12-02 00:03:08 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2020-12-02 00:03:08 | INFO | train | epoch 072 | loss 3.336 | nll_loss 1.429 | symm_mse 0.313 | ppl 2.69 | wps 15870.6 | ups 1.94 | wpb 8159.9 | bsz 278.2 | num_updates 17256 | lr 4.8146e-05 | gnorm 0.8 | train_wall 325 | wall 0
111
2020-12-02 00:03:08 | INFO | fairseq.trainer | begin training epoch 73
2020-12-02 00:03:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:03:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:08:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 00:08:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:08:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:08:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:08:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:08:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:08:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:09:15 | INFO | valid | epoch 073 | valid on 'valid' subset | symm_mse 0 | loss 9.553 | nll_loss 8.588 | ppl 384.87 | bleu 12.11 | wps 2523.9 | wpb 4629 | bsz 118.9 | num_updates 17975 | best_bleu 12.4
2020-12-02 00:09:15 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 00:09:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:09:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:09:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:09:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:09:20 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 73 @ 17975 updates, score 12.11) (writing took 5.541271358728409 seconds)
2020-12-02 00:09:20 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2020-12-02 00:09:20 | INFO | train | epoch 073 | loss 3.327 | nll_loss 1.421 | symm_mse 0.31 | ppl 2.68 | wps 15765.6 | ups 1.93 | wpb 8159.9 | bsz 278.2 | num_updates 17975 | lr 4.71732e-05 | gnorm 0.799 | train_wall 326 | wall 0
111
2020-12-02 00:09:20 | INFO | fairseq.trainer | begin training epoch 74
2020-12-02 00:09:22 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:09:25 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:14:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 00:15:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:15:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:15:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:15:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:15:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:15:04 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:15:30 | INFO | valid | epoch 074 | valid on 'valid' subset | symm_mse 0 | loss 9.573 | nll_loss 8.607 | ppl 390.04 | bleu 11.86 | wps 2447.2 | wpb 4629 | bsz 118.9 | num_updates 18694 | best_bleu 12.4
2020-12-02 00:15:30 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 00:15:31 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:15:32 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:15:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:15:35 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:15:35 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 74 @ 18694 updates, score 11.86) (writing took 5.024154253304005 seconds)
2020-12-02 00:15:35 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2020-12-02 00:15:35 | INFO | train | epoch 074 | loss 3.318 | nll_loss 1.412 | symm_mse 0.307 | ppl 2.66 | wps 15652.4 | ups 1.92 | wpb 8159.9 | bsz 278.2 | num_updates 18694 | lr 4.62572e-05 | gnorm 0.8 | train_wall 329 | wall 0
111
2020-12-02 00:15:35 | INFO | fairseq.trainer | begin training epoch 75
2020-12-02 00:15:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:15:39 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:21:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 00:21:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:21:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:21:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:21:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:21:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:21:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:21:45 | INFO | valid | epoch 075 | valid on 'valid' subset | symm_mse 0 | loss 9.584 | nll_loss 8.62 | ppl 393.3 | bleu 12.01 | wps 2436.1 | wpb 4629 | bsz 118.9 | num_updates 19413 | best_bleu 12.4
2020-12-02 00:21:45 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 00:21:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:21:46 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:21:49 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:21:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:21:50 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 75 @ 19413 updates, score 12.01) (writing took 5.324454685673118 seconds)
2020-12-02 00:21:50 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2020-12-02 00:21:50 | INFO | train | epoch 075 | loss 3.309 | nll_loss 1.404 | symm_mse 0.305 | ppl 2.65 | wps 15650.4 | ups 1.92 | wpb 8159.9 | bsz 278.2 | num_updates 19413 | lr 4.53925e-05 | gnorm 0.803 | train_wall 329 | wall 0
111
2020-12-02 00:21:50 | INFO | fairseq.trainer | begin training epoch 76
2020-12-02 00:21:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:21:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:27:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 00:27:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:27:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:27:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:27:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:27:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:27:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:27:56 | INFO | valid | epoch 076 | valid on 'valid' subset | symm_mse 0 | loss 9.599 | nll_loss 8.635 | ppl 397.68 | bleu 12.24 | wps 2699.6 | wpb 4629 | bsz 118.9 | num_updates 20132 | best_bleu 12.4
2020-12-02 00:27:56 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 00:27:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:27:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:28:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:28:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:28:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 76 @ 20132 updates, score 12.24) (writing took 5.29367639683187 seconds)
2020-12-02 00:28:02 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2020-12-02 00:28:02 | INFO | train | epoch 076 | loss 3.302 | nll_loss 1.397 | symm_mse 0.303 | ppl 2.63 | wps 15788.1 | ups 1.93 | wpb 8159.9 | bsz 278.2 | num_updates 20132 | lr 4.45745e-05 | gnorm 0.804 | train_wall 327 | wall 0
111
2020-12-02 00:28:02 | INFO | fairseq.trainer | begin training epoch 77
2020-12-02 00:28:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:28:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:33:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 00:33:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:33:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:33:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:33:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:33:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:33:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:34:09 | INFO | valid | epoch 077 | valid on 'valid' subset | symm_mse 0 | loss 9.575 | nll_loss 8.609 | ppl 390.42 | bleu 12.1 | wps 2464.7 | wpb 4629 | bsz 118.9 | num_updates 20851 | best_bleu 12.4
2020-12-02 00:34:09 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 00:34:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:34:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:34:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:34:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:34:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 77 @ 20851 updates, score 12.1) (writing took 4.93949256837368 seconds)
2020-12-02 00:34:14 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2020-12-02 00:34:14 | INFO | train | epoch 077 | loss 3.294 | nll_loss 1.389 | symm_mse 0.301 | ppl 2.62 | wps 15760.7 | ups 1.93 | wpb 8159.9 | bsz 278.2 | num_updates 20851 | lr 4.37992e-05 | gnorm 0.804 | train_wall 327 | wall 0
111
2020-12-02 00:34:14 | INFO | fairseq.trainer | begin training epoch 78
2020-12-02 00:34:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:34:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:39:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 00:39:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:39:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:39:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:39:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:39:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:39:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:40:23 | INFO | valid | epoch 078 | valid on 'valid' subset | symm_mse 0 | loss 9.649 | nll_loss 8.688 | ppl 412.45 | bleu 12 | wps 2362 | wpb 4629 | bsz 118.9 | num_updates 21570 | best_bleu 12.4
2020-12-02 00:40:23 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 00:40:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:40:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:40:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:40:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:40:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 78 @ 21570 updates, score 12.0) (writing took 5.083603385835886 seconds)
2020-12-02 00:40:28 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2020-12-02 00:40:28 | INFO | train | epoch 078 | loss 3.286 | nll_loss 1.381 | symm_mse 0.299 | ppl 2.61 | wps 15703.8 | ups 1.92 | wpb 8159.9 | bsz 278.2 | num_updates 21570 | lr 4.30631e-05 | gnorm 0.806 | train_wall 326 | wall 0
111
2020-12-02 00:40:28 | INFO | fairseq.trainer | begin training epoch 79
2020-12-02 00:40:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:40:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:46:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 00:46:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:46:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:46:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:46:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:46:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:46:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:46:33 | INFO | valid | epoch 079 | valid on 'valid' subset | symm_mse 0 | loss 9.617 | nll_loss 8.654 | ppl 402.77 | bleu 11.87 | wps 2546.4 | wpb 4629 | bsz 118.9 | num_updates 22289 | best_bleu 12.4
2020-12-02 00:46:33 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 00:46:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:46:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:46:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:46:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:46:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 79 @ 22289 updates, score 11.87) (writing took 4.881507551297545 seconds)
2020-12-02 00:46:37 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2020-12-02 00:46:37 | INFO | train | epoch 079 | loss 3.279 | nll_loss 1.375 | symm_mse 0.297 | ppl 2.59 | wps 15863.8 | ups 1.94 | wpb 8159.9 | bsz 278.2 | num_updates 22289 | lr 4.23628e-05 | gnorm 0.809 | train_wall 325 | wall 0
111
2020-12-02 00:46:37 | INFO | fairseq.trainer | begin training epoch 80
2020-12-02 00:46:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:46:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:52:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 00:52:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:52:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:52:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:52:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:52:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:52:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:52:43 | INFO | valid | epoch 080 | valid on 'valid' subset | symm_mse 0 | loss 9.613 | nll_loss 8.647 | ppl 400.99 | bleu 11.96 | wps 2557.1 | wpb 4629 | bsz 118.9 | num_updates 23008 | best_bleu 12.4
2020-12-02 00:52:43 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 00:52:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:52:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:52:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:52:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:52:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 80 @ 23008 updates, score 11.96) (writing took 5.014961684122682 seconds)
2020-12-02 00:52:48 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2020-12-02 00:52:49 | INFO | train | epoch 080 | loss 3.272 | nll_loss 1.368 | symm_mse 0.295 | ppl 2.58 | wps 15811.3 | ups 1.94 | wpb 8159.9 | bsz 278.2 | num_updates 23008 | lr 4.16956e-05 | gnorm 0.81 | train_wall 327 | wall 0
111
2020-12-02 00:52:49 | INFO | fairseq.trainer | begin training epoch 81
2020-12-02 00:52:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:52:53 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:58:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 00:58:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:58:26 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:58:27 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:58:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:58:29 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:58:30 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:58:57 | INFO | valid | epoch 081 | valid on 'valid' subset | symm_mse 0 | loss 9.614 | nll_loss 8.651 | ppl 401.95 | bleu 12.15 | wps 2396.3 | wpb 4629 | bsz 118.9 | num_updates 23727 | best_bleu 12.4
2020-12-02 00:58:57 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 00:58:58 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:58:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:59:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 81 @ 23727 updates, score 12.15) (writing took 4.887339416891336 seconds)
2020-12-02 00:59:02 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2020-12-02 00:59:02 | INFO | train | epoch 081 | loss 3.265 | nll_loss 1.361 | symm_mse 0.293 | ppl 2.57 | wps 15717 | ups 1.93 | wpb 8159.9 | bsz 278.2 | num_updates 23727 | lr 4.1059e-05 | gnorm 0.81 | train_wall 327 | wall 0
111
2020-12-02 00:59:02 | INFO | fairseq.trainer | begin training epoch 82
2020-12-02 00:59:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:59:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 00:59:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 00:59:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:04:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 01:04:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:04:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:04:40 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:04:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:04:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:04:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:05:09 | INFO | valid | epoch 082 | valid on 'valid' subset | symm_mse 0 | loss 9.618 | nll_loss 8.654 | ppl 402.86 | bleu 12.1 | wps 2553.5 | wpb 4629 | bsz 118.9 | num_updates 24446 | best_bleu 12.4
2020-12-02 01:05:09 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 01:05:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:05:10 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:05:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:05:13 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:05:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 82 @ 24446 updates, score 12.1) (writing took 4.987535860389471 seconds)
2020-12-02 01:05:14 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2020-12-02 01:05:14 | INFO | train | epoch 082 | loss 3.259 | nll_loss 1.356 | symm_mse 0.291 | ppl 2.56 | wps 15772.6 | ups 1.93 | wpb 8159.9 | bsz 278.2 | num_updates 24446 | lr 4.04507e-05 | gnorm 0.815 | train_wall 327 | wall 0
111
2020-12-02 01:05:14 | INFO | fairseq.trainer | begin training epoch 83
2020-12-02 01:05:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:05:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:10:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 01:10:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:10:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:10:48 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:10:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:10:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:10:51 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
111
2020-12-02 01:11:14 | INFO | valid | epoch 083 | valid on 'valid' subset | symm_mse 0 | loss 9.626 | nll_loss 8.664 | ppl 405.69 | bleu 12.05 | wps 2827.9 | wpb 4629 | bsz 118.9 | num_updates 25165 | best_bleu 12.4
2020-12-02 01:11:14 | INFO | fairseq_cli.train | begin save checkpoint
111
2020-12-02 01:11:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:11:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:11:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:11:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:11:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 83 @ 25165 updates, score 12.05) (writing took 4.701041495427489 seconds)
2020-12-02 01:11:19 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2020-12-02 01:11:19 | INFO | train | epoch 083 | loss 3.252 | nll_loss 1.349 | symm_mse 0.29 | ppl 2.55 | wps 16075.6 | ups 1.97 | wpb 8159.9 | bsz 278.2 | num_updates 25165 | lr 3.98686e-05 | gnorm 0.813 | train_wall 324 | wall 0
111
2020-12-02 01:11:19 | INFO | fairseq.trainer | begin training epoch 84
2020-12-02 01:11:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:11:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:16:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 01:16:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:16:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:16:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:16:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:16:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:16:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:17:22 | INFO | valid | epoch 084 | valid on 'valid' subset | symm_mse 0 | loss 9.644 | nll_loss 8.682 | ppl 410.63 | bleu 11.97 | wps 2578 | wpb 4629 | bsz 118.9 | num_updates 25884 | best_bleu 12.4
2020-12-02 01:17:22 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 01:17:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:17:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:17:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:17:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:17:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 84 @ 25884 updates, score 11.97) (writing took 5.1644494365900755 seconds)
2020-12-02 01:17:28 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2020-12-02 01:17:28 | INFO | train | epoch 084 | loss 3.245 | nll_loss 1.343 | symm_mse 0.288 | ppl 2.54 | wps 15909.4 | ups 1.95 | wpb 8159.9 | bsz 278.2 | num_updates 25884 | lr 3.9311e-05 | gnorm 0.815 | train_wall 324 | wall 0
111
2020-12-02 01:17:28 | INFO | fairseq.trainer | begin training epoch 85
2020-12-02 01:17:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:17:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:23:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 01:23:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:23:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:23:06 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:23:09 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:23:35 | INFO | valid | epoch 085 | valid on 'valid' subset | symm_mse 0 | loss 9.666 | nll_loss 8.707 | ppl 417.95 | bleu 12.08 | wps 2470 | wpb 4629 | bsz 118.9 | num_updates 26603 | best_bleu 12.4
2020-12-02 01:23:35 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 01:23:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:23:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:23:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:23:40 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 85 @ 26603 updates, score 12.08) (writing took 5.368553463369608 seconds)
2020-12-02 01:23:41 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2020-12-02 01:23:41 | INFO | train | epoch 085 | loss 3.24 | nll_loss 1.337 | symm_mse 0.287 | ppl 2.53 | wps 15731.4 | ups 1.93 | wpb 8159.9 | bsz 278.2 | num_updates 26603 | lr 3.87762e-05 | gnorm 0.818 | train_wall 327 | wall 0
111
2020-12-02 01:23:41 | INFO | fairseq.trainer | begin training epoch 86
2020-12-02 01:23:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:23:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:29:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 01:29:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:29:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:29:18 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:29:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:29:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:29:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:29:46 | INFO | valid | epoch 086 | valid on 'valid' subset | symm_mse 0 | loss 9.67 | nll_loss 8.712 | ppl 419.25 | bleu 11.88 | wps 2624.1 | wpb 4629 | bsz 118.9 | num_updates 27322 | best_bleu 12.4
2020-12-02 01:29:46 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 01:29:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:29:47 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:29:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:29:50 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:29:51 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 86 @ 27322 updates, score 11.88) (writing took 4.940591540187597 seconds)
2020-12-02 01:29:51 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2020-12-02 01:29:51 | INFO | train | epoch 086 | loss 3.234 | nll_loss 1.332 | symm_mse 0.285 | ppl 2.52 | wps 15850.1 | ups 1.94 | wpb 8159.9 | bsz 278.2 | num_updates 27322 | lr 3.82625e-05 | gnorm 0.82 | train_wall 326 | wall 0
111
2020-12-02 01:29:51 | INFO | fairseq.trainer | begin training epoch 87
2020-12-02 01:29:52 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:29:55 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:35:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 01:35:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:35:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:35:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:35:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:35:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:35:32 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:35:58 | INFO | valid | epoch 087 | valid on 'valid' subset | symm_mse 0 | loss 9.669 | nll_loss 8.712 | ppl 419.25 | bleu 11.92 | wps 2516 | wpb 4629 | bsz 118.9 | num_updates 28041 | best_bleu 12.4
2020-12-02 01:35:58 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 01:35:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:35:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:36:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:36:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:36:03 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 87 @ 28041 updates, score 11.92) (writing took 4.981700399890542 seconds)
2020-12-02 01:36:03 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2020-12-02 01:36:03 | INFO | train | epoch 087 | loss 3.228 | nll_loss 1.326 | symm_mse 0.284 | ppl 2.51 | wps 15775.1 | ups 1.93 | wpb 8159.9 | bsz 278.2 | num_updates 28041 | lr 3.77688e-05 | gnorm 0.823 | train_wall 327 | wall 0
111
2020-12-02 01:36:03 | INFO | fairseq.trainer | begin training epoch 88
2020-12-02 01:36:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:36:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:41:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 01:41:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:41:41 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:41:42 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:41:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:41:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:41:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:42:10 | INFO | valid | epoch 088 | valid on 'valid' subset | symm_mse 0 | loss 9.646 | nll_loss 8.686 | ppl 411.82 | bleu 11.97 | wps 2560.9 | wpb 4629 | bsz 118.9 | num_updates 28760 | best_bleu 12.4
2020-12-02 01:42:10 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 01:42:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:42:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:42:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:42:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:42:14 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 88 @ 28760 updates, score 11.97) (writing took 4.6320587527006865 seconds)
2020-12-02 01:42:14 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2020-12-02 01:42:14 | INFO | train | epoch 088 | loss 3.222 | nll_loss 1.32 | symm_mse 0.282 | ppl 2.5 | wps 15779.9 | ups 1.93 | wpb 8159.9 | bsz 278.2 | num_updates 28760 | lr 3.72937e-05 | gnorm 0.822 | train_wall 327 | wall 0
111
2020-12-02 01:42:14 | INFO | fairseq.trainer | begin training epoch 89
2020-12-02 01:42:16 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:42:19 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:47:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 01:47:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:47:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:47:51 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:47:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:47:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:47:54 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:48:21 | INFO | valid | epoch 089 | valid on 'valid' subset | symm_mse 0 | loss 9.674 | nll_loss 8.713 | ppl 419.63 | bleu 12 | wps 2466.4 | wpb 4629 | bsz 118.9 | num_updates 29479 | best_bleu 12.4
2020-12-02 01:48:21 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 01:48:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:48:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:48:26 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 89 @ 29479 updates, score 12.0) (writing took 5.359415335580707 seconds)
2020-12-02 01:48:26 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2020-12-02 01:48:26 | INFO | train | epoch 089 | loss 3.217 | nll_loss 1.315 | symm_mse 0.281 | ppl 2.49 | wps 15767.8 | ups 1.93 | wpb 8159.9 | bsz 278.2 | num_updates 29479 | lr 3.68361e-05 | gnorm 0.823 | train_wall 326 | wall 0
111
2020-12-02 01:48:26 | INFO | fairseq.trainer | begin training epoch 90
2020-12-02 01:48:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:48:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:48:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:48:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:54:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 01:54:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:54:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:54:03 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:54:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:54:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:54:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:54:32 | INFO | valid | epoch 090 | valid on 'valid' subset | symm_mse 0 | loss 9.672 | nll_loss 8.713 | ppl 419.78 | bleu 12.02 | wps 2533.1 | wpb 4629 | bsz 118.9 | num_updates 30198 | best_bleu 12.4
2020-12-02 01:54:32 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 01:54:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:54:34 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:54:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:54:37 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 01:54:37 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 90 @ 30198 updates, score 12.02) (writing took 4.994959050789475 seconds)
2020-12-02 01:54:37 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2020-12-02 01:54:37 | INFO | train | epoch 090 | loss 3.212 | nll_loss 1.31 | symm_mse 0.28 | ppl 2.48 | wps 15820.1 | ups 1.94 | wpb 8159.9 | bsz 278.2 | num_updates 30198 | lr 3.63949e-05 | gnorm 0.825 | train_wall 325 | wall 0
111
2020-12-02 01:54:37 | INFO | fairseq.trainer | begin training epoch 91
2020-12-02 01:54:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 01:54:42 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:00:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 02:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:00:14 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:00:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:00:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:00:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:00:17 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:00:43 | INFO | valid | epoch 091 | valid on 'valid' subset | symm_mse 0 | loss 9.68 | nll_loss 8.722 | ppl 422.4 | bleu 11.95 | wps 2568.6 | wpb 4629 | bsz 118.9 | num_updates 30917 | best_bleu 12.4
2020-12-02 02:00:43 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 02:00:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:00:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:00:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:00:48 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:00:48 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 91 @ 30917 updates, score 11.95) (writing took 5.134349558502436 seconds)
2020-12-02 02:00:48 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2020-12-02 02:00:48 | INFO | train | epoch 091 | loss 3.206 | nll_loss 1.305 | symm_mse 0.278 | ppl 2.47 | wps 15830.3 | ups 1.94 | wpb 8159.9 | bsz 278.2 | num_updates 30917 | lr 3.59692e-05 | gnorm 0.825 | train_wall 326 | wall 0
111
2020-12-02 02:00:48 | INFO | fairseq.trainer | begin training epoch 92
2020-12-02 02:00:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:00:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:06:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 02:06:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:06:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:06:24 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:06:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:06:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:06:27 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:06:54 | INFO | valid | epoch 092 | valid on 'valid' subset | symm_mse 0 | loss 9.699 | nll_loss 8.741 | ppl 427.95 | bleu 11.96 | wps 2545.9 | wpb 4629 | bsz 118.9 | num_updates 31636 | best_bleu 12.4
2020-12-02 02:06:54 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 02:06:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:06:56 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:06:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:06:58 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:06:59 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 92 @ 31636 updates, score 11.96) (writing took 4.939073733985424 seconds)
2020-12-02 02:06:59 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2020-12-02 02:06:59 | INFO | train | epoch 092 | loss 3.201 | nll_loss 1.299 | symm_mse 0.277 | ppl 2.46 | wps 15834.5 | ups 1.94 | wpb 8159.9 | bsz 278.2 | num_updates 31636 | lr 3.55582e-05 | gnorm 0.829 | train_wall 326 | wall 0
111
2020-12-02 02:06:59 | INFO | fairseq.trainer | begin training epoch 93
2020-12-02 02:07:00 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:07:03 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:12:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 02:12:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:12:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:12:35 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:12:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:12:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:12:38 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:13:03 | INFO | valid | epoch 093 | valid on 'valid' subset | symm_mse 0 | loss 9.688 | nll_loss 8.73 | ppl 424.49 | bleu 12.06 | wps 2656.8 | wpb 4629 | bsz 118.9 | num_updates 32355 | best_bleu 12.4
2020-12-02 02:13:03 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 02:13:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:13:05 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:13:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:13:08 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:13:09 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 93 @ 32355 updates, score 12.06) (writing took 5.477042630314827 seconds)
2020-12-02 02:13:09 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2020-12-02 02:13:09 | INFO | train | epoch 093 | loss 3.196 | nll_loss 1.294 | symm_mse 0.276 | ppl 2.45 | wps 15850.4 | ups 1.94 | wpb 8159.9 | bsz 278.2 | num_updates 32355 | lr 3.51608e-05 | gnorm 0.831 | train_wall 325 | wall 0
111
2020-12-02 02:13:09 | INFO | fairseq.trainer | begin training epoch 94
2020-12-02 02:13:11 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:13:14 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:18:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 02:18:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:18:44 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:18:45 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:18:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:18:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:18:47 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:19:13 | INFO | valid | epoch 094 | valid on 'valid' subset | symm_mse 0 | loss 9.701 | nll_loss 8.744 | ppl 428.63 | bleu 11.91 | wps 2477.9 | wpb 4629 | bsz 118.9 | num_updates 33074 | best_bleu 12.4
2020-12-02 02:19:13 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 02:19:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:19:15 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:19:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:19:18 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:19:19 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 94 @ 33074 updates, score 11.91) (writing took 5.034824900329113 seconds)
2020-12-02 02:19:19 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2020-12-02 02:19:19 | INFO | train | epoch 094 | loss 3.192 | nll_loss 1.29 | symm_mse 0.275 | ppl 2.45 | wps 15863.7 | ups 1.94 | wpb 8159.9 | bsz 278.2 | num_updates 33074 | lr 3.47766e-05 | gnorm 0.833 | train_wall 324 | wall 0
111
2020-12-02 02:19:19 | INFO | fairseq.trainer | begin training epoch 95
2020-12-02 02:19:20 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:19:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:24:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 02:24:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:24:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:24:53 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:24:56 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:24:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:24:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:25:22 | INFO | valid | epoch 095 | valid on 'valid' subset | symm_mse 0 | loss 9.71 | nll_loss 8.754 | ppl 431.71 | bleu 11.98 | wps 2658.6 | wpb 4629 | bsz 118.9 | num_updates 33793 | best_bleu 12.4
2020-12-02 02:25:22 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 02:25:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:25:23 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:25:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:25:26 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:25:27 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 95 @ 33793 updates, score 11.98) (writing took 5.300781534984708 seconds)
2020-12-02 02:25:27 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2020-12-02 02:25:27 | INFO | train | epoch 095 | loss 3.186 | nll_loss 1.285 | symm_mse 0.274 | ppl 2.44 | wps 15928.9 | ups 1.95 | wpb 8159.9 | bsz 278.2 | num_updates 33793 | lr 3.44046e-05 | gnorm 0.832 | train_wall 324 | wall 0
111
2020-12-02 02:25:27 | INFO | fairseq.trainer | begin training epoch 96
2020-12-02 02:25:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:25:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:31:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 02:31:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:31:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:31:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:31:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:31:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:31:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
111
2020-12-02 02:31:35 | INFO | valid | epoch 096 | valid on 'valid' subset | symm_mse 0 | loss 9.724 | nll_loss 8.768 | ppl 435.95 | bleu 11.92 | wps 2258 | wpb 4629 | bsz 118.9 | num_updates 34512 | best_bleu 12.4
2020-12-02 02:31:35 | INFO | fairseq_cli.train | begin save checkpoint
111
2020-12-02 02:31:36 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:31:37 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:31:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:31:40 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:31:41 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 96 @ 34512 updates, score 11.92) (writing took 5.435264021158218 seconds)
2020-12-02 02:31:41 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2020-12-02 02:31:41 | INFO | train | epoch 096 | loss 3.182 | nll_loss 1.28 | symm_mse 0.273 | ppl 2.43 | wps 15692.4 | ups 1.92 | wpb 8159.9 | bsz 278.2 | num_updates 34512 | lr 3.40443e-05 | gnorm 0.835 | train_wall 326 | wall 0
111
2020-12-02 02:31:41 | INFO | fairseq.trainer | begin training epoch 97
2020-12-02 02:31:43 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:31:45 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:37:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 02:37:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:37:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:37:19 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:37:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:37:22 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:37:23 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
111
2020-12-02 02:37:48 | INFO | valid | epoch 097 | valid on 'valid' subset | symm_mse 0 | loss 9.726 | nll_loss 8.77 | ppl 436.52 | bleu 12.06 | wps 2641.6 | wpb 4629 | bsz 118.9 | num_updates 35231 | best_bleu 12.4
2020-12-02 02:37:48 | INFO | fairseq_cli.train | begin save checkpoint
111
2020-12-02 02:37:49 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:37:50 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:37:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:37:52 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:37:53 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 97 @ 35231 updates, score 12.06) (writing took 5.002335848286748 seconds)
2020-12-02 02:37:53 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2020-12-02 02:37:53 | INFO | train | epoch 097 | loss 3.177 | nll_loss 1.276 | symm_mse 0.272 | ppl 2.42 | wps 15765.6 | ups 1.93 | wpb 8159.9 | bsz 278.2 | num_updates 35231 | lr 3.36952e-05 | gnorm 0.835 | train_wall 327 | wall 0
111
2020-12-02 02:37:53 | INFO | fairseq.trainer | begin training epoch 98
2020-12-02 02:37:55 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:37:57 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:43:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 02:43:28 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:43:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:43:29 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:43:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:43:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:43:31 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:43:57 | INFO | valid | epoch 098 | valid on 'valid' subset | symm_mse 0 | loss 9.698 | nll_loss 8.743 | ppl 428.41 | bleu 11.88 | wps 2560.9 | wpb 4629 | bsz 118.9 | num_updates 35950 | best_bleu 12.4
2020-12-02 02:43:57 | INFO | fairseq_cli.train | begin save checkpoint
111
111
2020-12-02 02:43:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:43:59 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:44:01 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:44:02 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:44:02 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 98 @ 35950 updates, score 11.88) (writing took 5.250260163098574 seconds)
2020-12-02 02:44:02 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2020-12-02 02:44:02 | INFO | train | epoch 098 | loss 3.172 | nll_loss 1.271 | symm_mse 0.271 | ppl 2.41 | wps 15881.8 | ups 1.95 | wpb 8159.9 | bsz 278.2 | num_updates 35950 | lr 3.33565e-05 | gnorm 0.836 | train_wall 325 | wall 0
111
2020-12-02 02:44:02 | INFO | fairseq.trainer | begin training epoch 99
2020-12-02 02:44:04 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:44:07 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:50:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 02:50:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:50:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:50:17 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:50:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:50:20 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:50:21 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
111
2020-12-02 02:50:59 | INFO | valid | epoch 099 | valid on 'valid' subset | symm_mse 0 | loss 9.739 | nll_loss 8.782 | ppl 440.18 | bleu 11.88 | wps 1756.2 | wpb 4629 | bsz 118.9 | num_updates 36669 | best_bleu 12.4
2020-12-02 02:50:59 | INFO | fairseq_cli.train | begin save checkpoint
111
2020-12-02 02:51:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:51:01 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:51:06 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 99 @ 36669 updates, score 11.88) (writing took 6.790299849584699 seconds)
2020-12-02 02:51:06 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2020-12-02 02:51:06 | INFO | train | epoch 099 | loss 3.168 | nll_loss 1.267 | symm_mse 0.27 | ppl 2.41 | wps 13865 | ups 1.7 | wpb 8159.9 | bsz 278.2 | num_updates 36669 | lr 3.30279e-05 | gnorm 0.838 | train_wall 360 | wall 0
111
2020-12-02 02:51:06 | INFO | fairseq.trainer | begin training epoch 100
2020-12-02 02:51:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:51:06 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:51:07 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:51:11 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:57:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-12-02 02:57:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:57:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:57:39 | INFO | transformers.file_utils | PyTorch version 1.6.0 available.
2020-12-02 02:57:43 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:57:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:57:44 | INFO | transformers.file_utils | TensorFlow version 2.2.0 available.
2020-12-02 02:58:21 | INFO | valid | epoch 100 | valid on 'valid' subset | symm_mse 0 | loss 9.734 | nll_loss 8.779 | ppl 439.32 | bleu 11.9 | wps 1705.5 | wpb 4629 | bsz 118.9 | num_updates 37388 | best_bleu 12.4
2020-12-02 02:58:21 | INFO | fairseq_cli.train | begin save checkpoint
2020-12-02 02:58:28 | INFO | fairseq.checkpoint_utils | saved checkpoint ./examples/transformer_enzh/bash/../checkpoints/closer/checkpoint_last.pt (epoch 100 @ 37388 updates, score 11.9) (writing took 6.64262868091464 seconds)
2020-12-02 02:58:28 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2020-12-02 02:58:28 | INFO | train | epoch 100 | loss 3.164 | nll_loss 1.263 | symm_mse 0.269 | ppl 2.4 | wps 13268.4 | ups 1.63 | wpb 8159.9 | bsz 278.2 | num_updates 37388 | lr 3.27087e-05 | gnorm 0.841 | train_wall 378 | wall 0
2020-12-02 02:58:28 | INFO | fairseq_cli.train | done training in 18647.5 seconds
